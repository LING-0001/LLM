# LLMå­¦ä¹  - ä»é›¶å¼€å§‹

## ç¬¬0æ­¥ï¼šå‡†å¤‡å·¥ä½œï¼ˆé¦–æ¬¡é…ç½®ï¼‰

### 0.1 å®‰è£…Homebrewï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
æ‰“å¼€ç»ˆç«¯ï¼ˆApplications â†’ å®ç”¨å·¥å…· â†’ ç»ˆç«¯ï¼‰ï¼Œç²˜è´´è¿è¡Œï¼š
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```
ç­‰å¾…å®‰è£…å®Œæˆï¼ˆ10-20åˆ†é’Ÿï¼‰ã€‚

### 0.2 å®‰è£…Miniconda
```bash
# ä¸‹è½½Miniconda
cd ~/Downloads
curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh

# å®‰è£…
bash Miniconda3-latest-MacOSX-arm64.sh -b -p $HOME/miniconda3

# åˆå§‹åŒ–
~/miniconda3/bin/conda init zsh

# å…³é—­å¹¶é‡æ–°æ‰“å¼€ç»ˆç«¯ï¼Œè®©é…ç½®ç”Ÿæ•ˆ
```

### 0.3 åˆ›å»ºPythonç¯å¢ƒ
```bash
# åˆ›å»ºllm-learningç¯å¢ƒ
conda create -n llm-learning python=3.10 -y

# æ¿€æ´»ç¯å¢ƒï¼ˆæ¯æ¬¡æ‰“å¼€æ–°ç»ˆç«¯éƒ½è¦è¿è¡Œï¼‰
conda activate llm-learning

# éªŒè¯Pythonç‰ˆæœ¬
python --version
# åº”è¯¥æ˜¾ç¤ºï¼šPython 3.10.x
```

### 0.4 å…‹éš†é¡¹ç›®
```bash
# è¿›å…¥å·¥ä½œç›®å½•
cd ~/code  # å¦‚æœæ²¡æœ‰codeæ–‡ä»¶å¤¹ï¼Œå…ˆè¿è¡Œï¼šmkdir -p ~/code

# å…‹éš†æˆ–åˆ›å»ºé¡¹ç›®ï¼ˆä½ åº”è¯¥å·²ç»åœ¨ /Users/a58/code/MyLLM äº†ï¼‰
cd /Users/a58/code/MyLLM

# å®‰è£…åŸºç¡€ä¾èµ–
pip install requests numpy pandas tqdm
```

---

## ğŸ“ ç¬¬1æ­¥ï¼šå®‰è£…llama.cppï¼ˆå½“å‰ä»»åŠ¡ï¼‰

### 1.1 å…‹éš†llama.cppä»“åº“

åœ¨ç»ˆç«¯è¿è¡Œï¼š
```bash
cd ~
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

### 1.2 ç¼–è¯‘ï¼ˆM1ä¼˜åŒ–ï¼Œä½¿ç”¨CMakeï¼‰

```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

ç¼–è¯‘éœ€è¦3-5åˆ†é’Ÿï¼Œå®Œæˆåä¼šåœ¨ `build/bin/` ç›®å½•ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶ã€‚

---

## âœ… ç¬¬1æ­¥ï¼šllama.cppç¼–è¯‘å®Œæˆï¼

---

## ğŸ“ ç¬¬2æ­¥ï¼šä¸‹è½½GGUFæ¨¡å‹ï¼ˆå½“å‰ä»»åŠ¡ï¼‰

æ¨èä½¿ç”¨ Qwen2.5-3Bï¼ˆ3Bæ¨¡å‹ï¼Œç¨³å®šä¸”æ•ˆæœå¥½ï¼‰ï¼š

```bash
cd ~/llama.cpp
mkdir -p models
cd models

# ä¸‹è½½Qwen2.5-3Bçš„Q4é‡åŒ–ç‰ˆæœ¬ï¼ˆçº¦2GBï¼‰
# æ–¹æ¡ˆ1ï¼šä½¿ç”¨curlå¸¦ç»­ä¼ åŠŸèƒ½ï¼ˆæ¨èï¼‰
curl -C - -L -o qwen2.5-3b-instruct-q4_k_m.gguf \
  --connect-timeout 60 --max-time 3600 \
  "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf"

# å¦‚æœä¸­æ–­äº†ï¼Œå†æ¬¡è¿è¡Œä¸Šé¢çš„å‘½ä»¤ä¼šè‡ªåŠ¨ç»­ä¼ 

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨wgetï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
# brew install wget  # å…ˆå®‰è£…wget
# wget -c "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf"

# æ–¹æ¡ˆ3ï¼šä½¿ç”¨hfé•œåƒç«™ï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰
# curl -L -o qwen2.5-3b-instruct-q4_k_m.gguf \
#   "https://hf-mirror.com/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf"
```

ä¸‹è½½éœ€è¦5-15åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰ã€‚

---

## âœ… ç¬¬2æ­¥ï¼šæ¨¡å‹ä¸‹è½½å®Œæˆï¼

---

## ğŸ“ ç¬¬3æ­¥ï¼šæµ‹è¯•æ¨¡å‹ï¼ˆå½“å‰ä»»åŠ¡ï¼‰

### 3.1 éªŒè¯æ¨¡å‹æ–‡ä»¶

é¦–å…ˆç¡®è®¤æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½å®Œæ•´ï¼š

```bash
cd ~/llama.cpp/models
ls -lh qwen2.5-3b-instruct-q4_k_m.gguf
```

åº”è¯¥çœ‹åˆ°æ–‡ä»¶å¤§å°çº¦ 1.9-2.1GBã€‚

### 3.2 è¿è¡Œæ¨¡å‹ï¼ˆå‘½ä»¤è¡Œæµ‹è¯•ï¼‰

**æ³¨æ„ï¼šå¦‚æœé‡åˆ° Metal é”™è¯¯ï¼Œéœ€è¦è°ƒæ•´å‚æ•°**

```bash
cd ~/llama.cpp

# æ–¹æ¡ˆ1ï¼šå‡å°ä¸Šä¸‹æ–‡çª—å£ï¼ˆæ¨èï¼‰
./build/bin/llama-cli \
  -m models/qwen2.5-3b-instruct-q4_k_m.gguf \
  -p "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚" \
  -n 128 \
  -c 2048 \
  --temp 0.7

# æ–¹æ¡ˆ2ï¼šç¦ç”¨ Metalï¼ˆä½¿ç”¨ CPUï¼Œè¾ƒæ…¢ä½†ç¨³å®šï¼‰
./build/bin/llama-cli \
  -m models/qwen2.5-3b-instruct-q4_k_m.gguf \
  -p "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚" \
  -n 128 \
  -ngl 0 \
  --temp 0.7
```

**å‚æ•°è¯´æ˜ï¼š**
- `-m`ï¼šæŒ‡å®šæ¨¡å‹æ–‡ä»¶
- `-p`ï¼šæç¤ºè¯ï¼ˆpromptï¼‰
- `-n 128`ï¼šç”Ÿæˆæœ€å¤š128ä¸ªtoken
- `-c 2048`ï¼šä¸Šä¸‹æ–‡é•¿åº¦è®¾ä¸º2048ï¼ˆé»˜è®¤4096å¯èƒ½å¤ªå¤§ï¼‰
- `-ngl 0`ï¼šä¸ä½¿ç”¨GPUå±‚ï¼Œçº¯CPUè¿è¡Œ
- `--temp 0.7`ï¼šæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ›é€ æ€§ï¼ˆ0-1ï¼‰

ä½ åº”è¯¥èƒ½çœ‹åˆ°æ¨¡å‹ç”Ÿæˆä¸­æ–‡å›å¤ï¼

**å¸¸è§é—®é¢˜ï¼š**
- å¦‚æœè¿˜æ˜¯å´©æºƒï¼Œè¯•è¯•æ–¹æ¡ˆ2ï¼ˆçº¯CPUæ¨¡å¼ï¼‰
- æˆ–è€…è¿›ä¸€æ­¥å‡å°ä¸Šä¸‹æ–‡ï¼š`-c 1024` æˆ– `-c 512`

### 3.3 å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆæ¨èï¼‰

å¯åŠ¨ä¸€ä¸ªæœ¬åœ°APIæœåŠ¡å™¨ï¼Œå¯ä»¥é€šè¿‡HTTPè°ƒç”¨æ¨¡å‹ï¼š

```bash
cd ~/llama.cpp

# ä½¿ç”¨è¾ƒå°çš„ä¸Šä¸‹æ–‡é¿å…å†…å­˜é—®é¢˜
./build/bin/llama-server \
  -m models/qwen2.5-3b-instruct-q4_k_m.gguf \
  --host 127.0.0.1 \
  --port 8080 \
  -c 2048 \
  -ngl 32
```

**å‚æ•°è¯´æ˜ï¼š**
- `--host 127.0.0.1`ï¼šæœ¬åœ°è®¿é—®
- `--port 8080`ï¼šç«¯å£å·
- `-c 2048`ï¼šä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå‡å°ä»¥é¿å… Metal é”™è¯¯ï¼‰
- `-ngl 32`ï¼šGPUåŠ è½½å±‚æ•°ï¼ˆ36å±‚ä¸­çš„32å±‚ï¼Œç•™ç‚¹ç»™CPUï¼‰

å¯åŠ¨åï¼š
- æœåŠ¡å™¨ä¼šæŒç»­è¿è¡Œï¼ˆä¿æŒç»ˆç«¯æ‰“å¼€ï¼‰
- åœ¨æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:8080
- ä½ ä¼šçœ‹åˆ°ä¸€ä¸ªèŠå¤©ç•Œé¢ï¼

**å¦‚æœè¿˜æ˜¯é‡åˆ°é”™è¯¯ï¼Œç”¨çº¯CPUæ¨¡å¼ï¼š**
```bash
./build/bin/llama-server \
  -m models/qwen2.5-3b-instruct-q4_k_m.gguf \
  --host 127.0.0.1 \
  --port 8080 \
  -c 2048 \
  -ngl 0
```

**æµ‹è¯•APIï¼ˆæ–°å¼€ç»ˆç«¯ï¼‰ï¼š**
```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

---

## âœ… ç¬¬3æ­¥ï¼šæ¨¡å‹æµ‹è¯•æˆåŠŸï¼

---

## ğŸ“ ç¬¬4æ­¥ï¼šç”¨Pythonè°ƒç”¨æ¨¡å‹ï¼ˆå½“å‰ä»»åŠ¡ï¼‰

ç°åœ¨æˆ‘ä»¬ç”¨Pythonä»£ç æ¥è°ƒç”¨æœ¬åœ°æ¨¡å‹ï¼Œè¿™æ ·æ›´çµæ´»ï¼Œå¯ä»¥é›†æˆåˆ°ä½ çš„é¡¹ç›®ä¸­ã€‚

### 4.1 å®‰è£…Pythonåº“

```bash
cd ~/code/MyLLM
conda activate llm-learning

# æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆç¡®ä¿æ˜¯3.10ï¼‰
python --version
which python

# å¦‚æœæ˜¾ç¤ºPython 2.7ï¼Œé‡æ–°åˆå§‹åŒ–conda
source ~/miniconda3/bin/activate
conda activate llm-learning

# å†æ¬¡æ£€æŸ¥
python --version  # åº”è¯¥æ˜¾ç¤º Python 3.10.x

# å®‰è£… llama-cpp-pythonï¼ˆPythonç»‘å®šï¼‰
pip install llama-cpp-python
```

**å¦‚æœå®‰è£…è¿‡ç¨‹å¾ˆæ…¢æˆ–å¤±è´¥ï¼Œå¯ä»¥å°è¯•ï¼š**
```bash
# ä½¿ç”¨é¢„ç¼–è¯‘çš„wheelï¼ˆæ›´å¿«ï¼‰
CMAKE_ARGS="-DGGML_METAL=off" pip install llama-cpp-python --no-cache-dir
```

### 4.2 åˆ›å»ºç¬¬ä¸€ä¸ªPythonè„šæœ¬

åˆ›å»ºæ–‡ä»¶ `test_model.py`ï¼š

```python
from llama_cpp import Llama

# åŠ è½½æ¨¡å‹ï¼ˆçº¯CPUæ¨¡å¼ï¼‰
llm = Llama(
    model_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf",
    n_ctx=2048,       # ä¸Šä¸‹æ–‡é•¿åº¦
    n_threads=4,      # CPUçº¿ç¨‹æ•°
    n_gpu_layers=0,   # 0 = çº¯CPUæ¨¡å¼
    verbose=False     # ä¸æ˜¾ç¤ºåŠ è½½ä¿¡æ¯
)

# æµ‹è¯•å¯¹è¯
prompt = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"

print(f"é—®é¢˜: {prompt}")
print("å›ç­”: ", end="", flush=True)

# ç”Ÿæˆå›å¤ï¼ˆæµå¼è¾“å‡ºï¼‰
output = llm(
    prompt,
    max_tokens=128,
    temperature=0.7,
    stop=["</s>", "\n\n"],  # åœæ­¢æ ‡è®°
    echo=False
)

print(output['choices'][0]['text'])
print(f"\nç”Ÿæˆäº† {output['usage']['completion_tokens']} ä¸ªtoken")
```

### 4.3 è¿è¡Œæµ‹è¯•

```bash
cd ~/code/MyLLM
python test_model.py
```

ä½ ä¼šçœ‹åˆ°æ¨¡å‹çš„ä¸­æ–‡å›å¤ï¼

### 4.4 åˆ›å»ºèŠå¤©æœºå™¨äºº

åˆ›å»º `chatbot.py`ï¼Œå®ç°è¿ç»­å¯¹è¯ï¼š

```python
from llama_cpp import Llama
import sys

# åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
llm = Llama(
    model_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    verbose=False
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼è¾“å…¥ 'exit' é€€å‡º\n")

# å¯¹è¯å¾ªç¯
conversation_history = []

while True:
    # è·å–ç”¨æˆ·è¾“å…¥
    user_input = input("ä½ : ")
    
    if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
        print("å†è§ï¼")
        break
    
    # æ„å»ºpromptï¼ˆåŒ…å«å†å²ï¼‰
    conversation_history.append(f"ç”¨æˆ·: {user_input}")
    prompt = "\n".join(conversation_history) + "\nåŠ©æ‰‹: "
    
    # ç”Ÿæˆå›å¤
    output = llm(
        prompt,
        max_tokens=256,
        temperature=0.7,
        stop=["ç”¨æˆ·:", "\n\n"],
        echo=False
    )
    
    response = output['choices'][0]['text'].strip()
    conversation_history.append(f"åŠ©æ‰‹: {response}")
    
    print(f"AI: {response}\n")
    
    # é™åˆ¶å†å²é•¿åº¦ï¼ˆé¿å…è¶…å‡ºä¸Šä¸‹æ–‡ï¼‰
    if len(conversation_history) > 10:
        conversation_history = conversation_history[-10:]
```

### 4.5 è¿è¡ŒèŠå¤©æœºå™¨äºº

```bash
python chatbot.py
```

ç°åœ¨ä½ å¯ä»¥å’Œæ¨¡å‹è¿ç»­å¯¹è¯äº†ï¼

---

## âœ… ç¬¬4æ­¥ï¼šPythonè°ƒç”¨æˆåŠŸï¼

---

## ğŸ“ ç¬¬5æ­¥ï¼šæç¤ºè¯å·¥ç¨‹å’Œå‚æ•°è°ƒä¼˜ï¼ˆå½“å‰ä»»åŠ¡ï¼‰

ç°åœ¨ä½ å·²ç»èƒ½è¿è¡Œæ¨¡å‹äº†ï¼Œæ¥ä¸‹æ¥å­¦ä¹ å¦‚ä½•**è®©æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„å†…å®¹**ã€‚

### 5.1 ç†è§£å…³é”®å‚æ•°

åˆ›å»º `test_parameters.py` æ¥å®éªŒä¸åŒå‚æ•°ï¼š

```python
from llama_cpp import Llama

print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
llm = Llama(
    model_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    verbose=False
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

prompt = "å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„äº”è¨€ç»å¥"

print(f"æç¤ºè¯: {prompt}\n")

# æµ‹è¯•ä¸åŒçš„æ¸©åº¦å€¼
temperatures = [0.1, 0.5, 0.9]

for temp in temperatures:
    print(f"{'='*50}")
    print(f"Temperature = {temp}")
    print(f"{'='*50}")
    
    response = ""
    for output in llm(
        prompt,
        max_tokens=100,
        temperature=temp,
        top_p=0.9,
        repeat_penalty=1.1,
        stream=True
    ):
        text = output['choices'][0]['text']
        print(text, end="", flush=True)
        response += text
    
    print("\n")
```

**å‚æ•°è¯´æ˜ï¼š**
- **temperature** (0-2): åˆ›é€ æ€§
  - 0.1-0.3: ä¿å®ˆã€ç¡®å®šæ€§å¼ºï¼ˆé€‚åˆäº‹å®æ€§é—®é¢˜ï¼‰
  - 0.7-0.9: å¹³è¡¡ï¼ˆæ—¥å¸¸å¯¹è¯ï¼‰
  - 1.0-2.0: åˆ›é€ æ€§å¼ºï¼ˆåˆ›æ„å†™ä½œï¼‰

- **top_p** (0-1): é‡‡æ ·èŒƒå›´
  - 0.9: ä»æ¦‚ç‡æœ€é«˜çš„90%çš„è¯ä¸­é€‰æ‹©
  - 0.95: æ›´å¤šæ ·æ€§
  
- **repeat_penalty** (1.0-1.5): é˜²æ­¢é‡å¤
  - 1.0: ä¸æƒ©ç½š
  - 1.1-1.2: è½»åº¦æƒ©ç½šï¼ˆæ¨èï¼‰
  - 1.5+: å¼ºåŠ›æƒ©ç½š

### 5.2 æç¤ºè¯å·¥ç¨‹æŠ€å·§

åˆ›å»º `prompt_engineering.py`ï¼š

```python
from llama_cpp import Llama

print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
llm = Llama(
    model_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    verbose=False
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

# æŠ€å·§1: è§’è‰²è®¾å®š
print("="*60)
print("æŠ€å·§1: è§’è‰²è®¾å®š")
print("="*60)

prompt1 = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Pythonç¨‹åºå‘˜ï¼Œæ“…é•¿ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µã€‚

è¯·è§£é‡Šä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"""

print(f"æç¤ºè¯:\n{prompt1}\n")
print("å›ç­”: ", end="", flush=True)

for output in llm(prompt1, max_tokens=200, temperature=0.7, stream=True):
    print(output['choices'][0]['text'], end="", flush=True)

print("\n\n")

# æŠ€å·§2: åˆ†æ­¥éª¤æ€è€ƒ
print("="*60)
print("æŠ€å·§2: åˆ†æ­¥éª¤æ€è€ƒï¼ˆChain of Thoughtï¼‰")
print("="*60)

prompt2 = """è¯·ä¸€æ­¥ä¸€æ­¥æ€è€ƒå¹¶è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

é—®é¢˜ï¼šä¸€ä¸ªæ°´æ± æœ‰è¿›æ°´ç®¡å’Œå‡ºæ°´ç®¡ï¼Œè¿›æ°´ç®¡æ¯å°æ—¶æ³¨å…¥10å‡æ°´ï¼Œå‡ºæ°´ç®¡æ¯å°æ—¶æ’å‡º3å‡æ°´ã€‚
å¦‚æœæ°´æ± åˆå§‹æ˜¯ç©ºçš„ï¼Œ5å°æ—¶åæ°´æ± æœ‰å¤šå°‘å‡æ°´ï¼Ÿ

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼š
1. åˆ—å‡ºå·²çŸ¥æ¡ä»¶
2. è®¡ç®—å‡€å¢åŠ é€Ÿåº¦
3. è®¡ç®—æœ€ç»ˆç»“æœ"""

print(f"æç¤ºè¯:\n{prompt2}\n")
print("å›ç­”: ", end="", flush=True)

for output in llm(prompt2, max_tokens=300, temperature=0.3, stream=True):
    print(output['choices'][0]['text'], end="", flush=True)

print("\n\n")

# æŠ€å·§3: Few-shot Learningï¼ˆæä¾›ç¤ºä¾‹ï¼‰
print("="*60)
print("æŠ€å·§3: Few-shot Learningï¼ˆæä¾›ç¤ºä¾‹ï¼‰")
print("="*60)

prompt3 = """è¯·å°†ä»¥ä¸‹å¥å­æ”¹å†™æˆæ›´æ­£å¼çš„è¡¨è¾¾æ–¹å¼ï¼š

ç¤ºä¾‹1:
åŸå¥: è¿™ä¸ªä¸œè¥¿çœŸä¸é”™
æ­£å¼: è¯¥äº§å“è´¨é‡ä¼˜è‰¯

ç¤ºä¾‹2:
åŸå¥: æˆ‘è§‰å¾—å¯ä»¥è¯•è¯•
æ­£å¼: æˆ‘è®¤ä¸ºæ­¤æ–¹æ¡ˆå€¼å¾—å°è¯•

ç¤ºä¾‹3:
åŸå¥: ä»–æŒºå‰å®³çš„
æ­£å¼: ä»–çš„èƒ½åŠ›è¾ƒä¸ºå‡ºä¼—

ç°åœ¨è½®åˆ°ä½ äº†:
åŸå¥: è¿™ä»£ç å†™å¾—å¤ªä¹±äº†
æ­£å¼:"""

print(f"æç¤ºè¯:\n{prompt3}\n")
print("å›ç­”: ", end="", flush=True)

for output in llm(prompt3, max_tokens=50, temperature=0.5, stream=True):
    print(output['choices'][0]['text'], end="", flush=True)

print("\n\n")

# æŠ€å·§4: è®¾ç½®è¾“å‡ºæ ¼å¼
print("="*60)
print("æŠ€å·§4: è®¾ç½®è¾“å‡ºæ ¼å¼")
print("="*60)

prompt4 = """è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä»½äº§å“è¯„æµ‹ï¼Œä½¿ç”¨JSONæ ¼å¼è¾“å‡ºï¼š

äº§å“ï¼šMacBook Pro M1
ä½¿ç”¨ä½“éªŒï¼šæ€§èƒ½å¼ºåŠ²ï¼Œç»­èˆªå‡ºè‰²ï¼Œå±å¹•æ˜¾ç¤ºæ•ˆæœå¥½

è¾“å‡ºæ ¼å¼ï¼š
{
  "product": "äº§å“åç§°",
  "rating": "è¯„åˆ†(1-5)",
  "pros": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
  "cons": ["ç¼ºç‚¹1", "ç¼ºç‚¹2"],
  "summary": "æ€»ç»“"
}"""

print(f"æç¤ºè¯:\n{prompt4}\n")
print("å›ç­”: ", end="", flush=True)

for output in llm(prompt4, max_tokens=300, temperature=0.5, stream=True):
    print(output['choices'][0]['text'], end="", flush=True)

print("\n\n")
```

### 5.3 å®ç”¨åº”ç”¨ç¤ºä¾‹

åˆ›å»º `practical_examples.py`ï¼š

```python
from llama_cpp import Llama

print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
llm = Llama(
    model_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    verbose=False
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

examples = [
    {
        "title": "ä»£ç å®¡æŸ¥åŠ©æ‰‹",
        "prompt": """è¯·å®¡æŸ¥ä»¥ä¸‹Pythonä»£ç ï¼ŒæŒ‡å‡ºæ½œåœ¨é—®é¢˜ï¼š

```python
def calculate_average(numbers):
    sum = 0
    for num in numbers:
        sum = sum + num
    return sum / len(numbers)
```

è¯·æŒ‡å‡ºï¼š
1. ä»£ç é€»è¾‘æ˜¯å¦æ­£ç¡®
2. æ˜¯å¦æœ‰æ½œåœ¨çš„é”™è¯¯
3. å¦‚ä½•æ”¹è¿›""",
        "temp": 0.3
    },
    {
        "title": "æ–‡æ¡ˆç”Ÿæˆ",
        "prompt": """ä¸ºä¸€æ¬¾AIå­¦ä¹ åŠ©æ‰‹Appå†™ä¸€æ®µå¸å¼•äººçš„äº§å“ä»‹ç»ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼š

äº§å“ç‰¹ç‚¹ï¼š
- ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„
- å®æ—¶ç­”ç–‘
- æ™ºèƒ½å¤ä¹ æé†’

è¦æ±‚ï¼šçªå‡ºä¼˜åŠ¿ï¼Œè¯­è¨€ç®€æ´æœ‰åŠ›""",
        "temp": 0.8
    },
    {
        "title": "æ•°æ®åˆ†æ",
        "prompt": """è¯·åˆ†æä»¥ä¸‹é”€å”®æ•°æ®å¹¶ç»™å‡ºå»ºè®®ï¼š

1æœˆé”€å”®é¢: 100ä¸‡
2æœˆé”€å”®é¢: 95ä¸‡
3æœˆé”€å”®é¢: 88ä¸‡

è¯·ï¼š
1. åˆ†æè¶‹åŠ¿
2. æ‰¾å‡ºå¯èƒ½çš„åŸå› 
3. æå‡º3æ¡æ”¹è¿›å»ºè®®""",
        "temp": 0.5
    }
]

for i, example in enumerate(examples, 1):
    print(f"\n{'='*60}")
    print(f"ç¤ºä¾‹{i}: {example['title']}")
    print(f"{'='*60}\n")
    print(f"æç¤ºè¯:\n{example['prompt']}\n")
    print("å›ç­”: ", end="", flush=True)
    
    for output in llm(
        example['prompt'],
        max_tokens=300,
        temperature=example['temp'],
        stream=True
    ):
        print(output['choices'][0]['text'], end="", flush=True)
    
    print("\n")
```

### 5.4 è¿è¡Œå®éªŒ

```bash
cd ~/code/MyLLM

# å®éªŒä¸åŒå‚æ•°
python test_parameters.py

# å­¦ä¹ æç¤ºè¯æŠ€å·§
python prompt_engineering.py

# æŸ¥çœ‹å®ç”¨æ¡ˆä¾‹
python practical_examples.py
```

---

## ğŸ’¡ æ ¸å¿ƒè¦ç‚¹æ€»ç»“

### å‚æ•°è°ƒä¼˜ï¼š
- **äº‹å®é—®ç­”**ï¼štemperature=0.1-0.3
- **æ—¥å¸¸å¯¹è¯**ï¼štemperature=0.7
- **åˆ›æ„å†™ä½œ**ï¼štemperature=0.9-1.2

### æç¤ºè¯æŠ€å·§ï¼š
1. âœ… æ˜ç¡®è§’è‰²å®šä½
2. âœ… æä¾›å…·ä½“ç¤ºä¾‹
3. âœ… åˆ†æ­¥éª¤å¼•å¯¼æ€è€ƒ
4. âœ… æŒ‡å®šè¾“å‡ºæ ¼å¼

---

## â¸ï¸ å®Œæˆç¬¬5æ­¥åå‘Šè¯‰æˆ‘

å°è¯•è¿™äº›æŠ€å·§åï¼Œæˆ‘ç»™ä½ ç¬¬6æ­¥ï¼šæ„å»ºå®ç”¨çš„AIåº”ç”¨ï¼ˆç¿»è¯‘åŠ©æ‰‹ã€ä»£ç åŠ©æ‰‹ç­‰ï¼‰ã€‚

---