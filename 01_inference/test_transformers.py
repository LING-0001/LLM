"""
TransformersåŸç”Ÿæ¨ç†æµ‹è¯•

è¿™ä¸ªè„šæœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨Transformersåº“ç›´æ¥åŠ è½½å’Œè¿è¡Œæ¨¡å‹
ä¼˜åŠ¿ï¼šå®Œå…¨é€æ˜ï¼Œå¯ä»¥çœ‹åˆ°æ¯ä¸€æ­¥çš„ç»†èŠ‚
åŠ£åŠ¿ï¼šé€Ÿåº¦è¾ƒæ…¢ï¼Œå†…å­˜å ç”¨é«˜

é€‚åˆï¼šç†è§£æ¨¡å‹åŠ è½½ã€æ¨ç†çš„åº•å±‚åŸç†
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time


def test_transformers_inference(
    model_name="Qwen/Qwen2-7B-Instruct",
    prompt="ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
    max_length=200,
    use_4bit=True
):
    """
    ä½¿ç”¨Transformersè¿›è¡Œæ¨ç†
    
    Args:
        model_name: HuggingFaceæ¨¡å‹åç§°
        prompt: è¾“å…¥æç¤º
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        use_4bit: æ˜¯å¦ä½¿ç”¨4-bité‡åŒ–ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    """
    
    print(f"ğŸ¤– æ¨¡å‹: {model_name}")
    print(f"ğŸ“ é—®é¢˜: {prompt}")
    print(f"ğŸ’¾ é‡åŒ–: {'4-bit' if use_4bit else 'FP16'}")
    print("-" * 60)
    
    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½tokenizer
    print("\n[1/4] åŠ è½½Tokenizer...")
    start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    print(f"âœ… TokenizeråŠ è½½å®Œæˆ ({time.time()-start:.2f}ç§’)")
    
    # ç¬¬äºŒæ­¥ï¼šåŠ è½½æ¨¡å‹
    print("\n[2/4] åŠ è½½æ¨¡å‹...")
    print("âš ï¸  é¦–æ¬¡è¿è¡Œä¼šä»HuggingFaceä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦10-30åˆ†é’Ÿ")
    print("   æ¨¡å‹ä¼šç¼“å­˜åˆ° ~/.cache/huggingface/")
    
    start = time.time()
    
    if use_4bit:
        # ä½¿ç”¨4-bité‡åŒ–ï¼ˆéœ€è¦bitsandbytesåº“ï¼‰
        try:
            from transformers import BitsAndBytesConfig
            
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True
            )
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (4-bité‡åŒ–) ({time.time()-start:.2f}ç§’)")
            
        except ImportError:
            print("âš ï¸  bitsandbytesæœªå®‰è£…ï¼Œä½¿ç”¨FP16æ¨¡å¼")
            use_4bit = False
    
    if not use_4bit:
        # ä½¿ç”¨FP16ï¼ˆM1ä¸Šçš„é»˜è®¤ç²¾åº¦ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (FP16) ({time.time()-start:.2f}ç§’)")
    
    # æŸ¥çœ‹å†…å­˜å ç”¨
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1e9
        print(f"ğŸ“Š æ˜¾å­˜å ç”¨: {memory_used:.2f} GB")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("ğŸ“Š ä½¿ç”¨Apple Metal GPUåŠ é€Ÿ")
    
    # ç¬¬ä¸‰æ­¥ï¼šç¼–ç è¾“å…¥
    print("\n[3/4] ç¼–ç è¾“å…¥...")
    start = time.time()
    
    # æ„å»ºå¯¹è¯æ ¼å¼ï¼ˆQwen2æ ¼å¼ï¼‰
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ]
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    print(f"âœ… è¾“å…¥ç¼–ç å®Œæˆ ({time.time()-start:.2f}ç§’)")
    print(f"   è¾“å…¥tokenæ•°: {inputs.input_ids.shape[1]}")
    
    # ç¬¬å››æ­¥ï¼šç”Ÿæˆå›å¤
    print("\n[4/4] ç”Ÿæˆå›å¤...")
    start = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    end = time.time()
    
    # è§£ç è¾“å‡º
    generated_text = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    )
    
    print(f"âœ… ç”Ÿæˆå®Œæˆ ({end-start:.2f}ç§’)")
    print(f"   è¾“å‡ºtokenæ•°: {outputs.shape[1] - inputs.input_ids.shape[1]}")
    print(f"   é€Ÿåº¦: {(outputs.shape[1] - inputs.input_ids.shape[1]) / (end-start):.2f} tokens/ç§’")
    
    print("\nğŸ’¬ å›ç­”:")
    print("-" * 60)
    print(generated_text)
    print("-" * 60)


def download_model_guide():
    """ä¸‹è½½æ¨¡å‹æŒ‡å—"""
    print("\n" + "="*60)
    print("ğŸ“¥ æ¨¡å‹ä¸‹è½½æŒ‡å—")
    print("="*60)
    
    print("\næ¨èæ¨¡å‹ï¼ˆ7Bçº§åˆ«ï¼Œé€‚åˆM1 16Gï¼‰ï¼š")
    print("\n1. Qwen2-7B-Instruct (æ¨è)")
    print("   - HuggingFace ID: Qwen/Qwen2-7B-Instruct")
    print("   - ä¸­æ–‡èƒ½åŠ›æœ€å¼º")
    print("   - æ¨¡å‹å¤§å°: ~15GB (åŸå§‹) / ~4GB (4-bité‡åŒ–)")
    
    print("\n2. DeepSeek-Coder-7B-Instruct")
    print("   - HuggingFace ID: deepseek-ai/deepseek-coder-7b-instruct-v1.5")
    print("   - ä»£ç èƒ½åŠ›å¼º")
    print("   - æ¨¡å‹å¤§å°: ~14GB (åŸå§‹) / ~4GB (4-bité‡åŒ–)")
    
    print("\n3. Llama-3-8B-Instruct")
    print("   - HuggingFace ID: meta-llama/Meta-Llama-3-8B-Instruct")
    print("   - å›½é™…ä¸»æµ")
    print("   - éœ€è¦åœ¨HuggingFaceåŒæ„è®¸å¯åè®®")
    
    print("\nä¸‹è½½æ–¹æ³•ï¼š")
    print("\næ–¹æ³•1ï¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆè¿è¡Œè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½ï¼‰")
    print("  python 01_inference/test_transformers.py")
    
    print("\næ–¹æ³•2ï¼šæ‰‹åŠ¨é¢„ä¸‹è½½")
    print("  pip install huggingface-hub")
    print("  huggingface-cli download Qwen/Qwen2-7B-Instruct")
    
    print("\næ–¹æ³•3ï¼šä½¿ç”¨å›½å†…é•œåƒåŠ é€Ÿï¼ˆå¯é€‰ï¼‰")
    print("  export HF_ENDPOINT=https://hf-mirror.com")
    print("  python 01_inference/test_transformers.py")
    
    print("\nâš ï¸  æ³¨æ„äº‹é¡¹ï¼š")
    print("  1. é¦–æ¬¡ä¸‹è½½éœ€è¦10-30åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰")
    print("  2. æ¨¡å‹ä¼šç¼“å­˜åˆ° ~/.cache/huggingface/")
    print("  3. ç¡®ä¿è‡³å°‘æœ‰20GBå¯ç”¨ç£ç›˜ç©ºé—´")
    print("  4. å»ºè®®åœ¨ç¨³å®šç½‘ç»œç¯å¢ƒä¸‹ä¸‹è½½")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "guide":
        download_model_guide()
        exit(0)
    
    print("ğŸš€ Transformersæ¨ç†æµ‹è¯•\n")
    print("âš ï¸  è¿™æ˜¯ä¸€ä¸ªæ•™å­¦è„šæœ¬ï¼Œç”¨äºç†è§£æ¨¡å‹æ¨ç†åŸç†")
    print("   ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨Ollamaæˆ–llama.cppï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰\n")
    
    # æ£€æŸ¥æ˜¯å¦è¦ä¸‹è½½æ¨¡å‹
    response = input("æ˜¯å¦å¼€å§‹æµ‹è¯•ï¼Ÿè¿™å°†è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰[y/N]: ")
    
    if response.lower() != 'y':
        print("\nå–æ¶ˆæµ‹è¯•ã€‚")
        print("å¦‚éœ€æŸ¥çœ‹ä¸‹è½½æŒ‡å—ï¼Œè¿è¡Œï¼špython 01_inference/test_transformers.py guide")
        exit(0)
    
    try:
        test_transformers_inference(
            model_name="Qwen/Qwen2-7B-Instruct",
            prompt="ç”¨ä¸‰å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯RAGæŠ€æœ¯",
            max_length=200,
            use_4bit=True
        )
        
        print("\nâœ… æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nå¯èƒ½çš„åŸå› ï¼š")
        print("  1. ç½‘ç»œè¿æ¥é—®é¢˜ï¼ˆæ— æ³•ä¸‹è½½æ¨¡å‹ï¼‰")
        print("  2. å†…å­˜ä¸è¶³")
        print("  3. ä¾èµ–åº“æœªå®‰è£…")
        print("\nå»ºè®®ï¼šå…ˆä½¿ç”¨Ollamaè¿›è¡Œæµ‹è¯•ï¼ˆæ›´ç®€å•ï¼‰")

