"""
llama.cpp Pythonç»‘å®šæµ‹è¯•è„šæœ¬

ä½¿ç”¨å‰ç¡®ä¿ï¼š
1. å·²å®‰è£…llama-cpp-python: pip install llama-cpp-python
2. å·²ä¸‹è½½GGUFæ¨¡å‹

è¿™ä¸ªè„šæœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨llama-cpp-pythonè¿›è¡Œæ¨ç†
"""

import os
import time
from pathlib import Path


def find_gguf_models():
    """æŸ¥æ‰¾å¯ç”¨çš„GGUFæ¨¡å‹"""
    possible_paths = [
        Path.home() / "llama.cpp" / "models",
        Path.home() / ".ollama" / "models",
        Path("/Users/a58/code/MyLLM/models")
    ]
    
    gguf_models = []
    for path in possible_paths:
        if path.exists():
            for file in path.rglob("*.gguf"):
                gguf_models.append(file)
    
    return gguf_models


def test_llamacpp_basic(model_path, prompt="ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ"):
    """åŸºç¡€æ¨ç†æµ‹è¯•"""
    try:
        from llama_cpp import Llama
    except ImportError:
        print("âŒ llama-cpp-pythonæœªå®‰è£…")
        print("\nå®‰è£…æ–¹æ³•ï¼š")
        print("  pip install llama-cpp-python")
        print("\nM1ç”¨æˆ·ä½¿ç”¨MetalåŠ é€Ÿï¼š")
        print("  CMAKE_ARGS='-DLLAMA_METAL=on' pip install llama-cpp-python --force-reinstall --no-cache-dir")
        return
    
    print(f"ğŸ¤– æ¨¡å‹: {model_path.name}")
    print(f"ğŸ“ é—®é¢˜: {prompt}")
    print("-" * 60)
    
    # åŠ è½½æ¨¡å‹
    print("\n[1/3] åŠ è½½æ¨¡å‹...")
    start = time.time()
    
    llm = Llama(
        model_path=str(model_path),
        n_ctx=2048,          # ä¸Šä¸‹æ–‡çª—å£
        n_threads=6,         # CPUçº¿ç¨‹æ•°ï¼ˆM1å»ºè®®4-8ï¼‰
        n_gpu_layers=1,      # ä½¿ç”¨MetalåŠ é€Ÿ
        verbose=False
    )
    
    load_time = time.time() - start
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}ç§’)")
    
    # ç¼–ç è¾“å…¥
    print("\n[2/3] ç¼–ç è¾“å…¥...")
    # Qwen2èŠå¤©æ ¼å¼
    formatted_prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # ç”Ÿæˆå›å¤
    print("\n[3/3] ç”Ÿæˆå›å¤...")
    start = time.time()
    
    output = llm(
        formatted_prompt,
        max_tokens=200,
        temperature=0.7,
        top_p=0.9,
        repeat_penalty=1.1,
        stop=["<|im_end|>", "<|endoftext|>"],
        echo=False
    )
    
    gen_time = time.time() - start
    
    # æå–ç»“æœ
    response = output['choices'][0]['text']
    tokens_generated = output['usage']['completion_tokens']
    
    print(f"âœ… ç”Ÿæˆå®Œæˆ ({gen_time:.2f}ç§’)")
    print(f"   ç”Ÿæˆtokenæ•°: {tokens_generated}")
    print(f"   é€Ÿåº¦: {tokens_generated/gen_time:.2f} tokens/ç§’")
    
    print("\nğŸ’¬ å›ç­”:")
    print("-" * 60)
    print(response.strip())
    print("-" * 60)


def test_llamacpp_stream(model_path, prompt="å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„å››è¡Œè¯—"):
    """æµå¼è¾“å‡ºæµ‹è¯•"""
    try:
        from llama_cpp import Llama
    except ImportError:
        return
    
    print(f"\n{'='*60}")
    print("ğŸŒŠ æµå¼è¾“å‡ºæµ‹è¯•")
    print(f"ğŸ¤– æ¨¡å‹: {model_path.name}")
    print(f"ğŸ“ é—®é¢˜: {prompt}")
    print("-" * 60)
    
    # åŠ è½½æ¨¡å‹
    llm = Llama(
        model_path=str(model_path),
        n_ctx=2048,
        n_threads=6,
        n_gpu_layers=1,
        verbose=False
    )
    
    # æ ¼å¼åŒ–è¾“å…¥
    formatted_prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    print("\nğŸ’¬ å›ç­”: ", end="", flush=True)
    
    # æµå¼ç”Ÿæˆ
    stream = llm(
        formatted_prompt,
        max_tokens=200,
        temperature=0.7,
        stream=True,
        stop=["<|im_end|>", "<|endoftext|>"]
    )
    
    token_count = 0
    start_time = time.time()
    
    for output in stream:
        text = output['choices'][0]['text']
        print(text, end="", flush=True)
        token_count += 1
    
    elapsed = time.time() - start_time
    
    print(f"\n\nâ±ï¸  è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"ğŸš€ é€Ÿåº¦: {token_count/elapsed:.2f} tokens/ç§’")


def benchmark_quantization(model_paths):
    """å¯¹æ¯”ä¸åŒé‡åŒ–æ–¹å¼çš„æ€§èƒ½"""
    try:
        from llama_cpp import Llama
    except ImportError:
        return
    
    print("\n" + "="*60)
    print("ğŸ“Š é‡åŒ–æ–¹å¼æ€§èƒ½å¯¹æ¯”")
    print("="*60)
    
    test_prompt = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿç”¨ä¸€å¥è¯å›ç­”ã€‚"
    
    results = []
    
    for model_path in model_paths:
        print(f"\næµ‹è¯•: {model_path.name}")
        
        # åŠ è½½æ¨¡å‹
        start = time.time()
        llm = Llama(
            model_path=str(model_path),
            n_ctx=512,
            n_threads=6,
            verbose=False
        )
        load_time = time.time() - start
        
        # æ¨ç†æµ‹è¯•
        formatted_prompt = f"<|im_start|>user\n{test_prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        start = time.time()
        output = llm(formatted_prompt, max_tokens=50, temperature=0.7)
        gen_time = time.time() - start
        
        tokens = output['usage']['completion_tokens']
        speed = tokens / gen_time
        
        # è·å–æ¨¡å‹å¤§å°
        size_gb = model_path.stat().st_size / 1e9
        
        results.append({
            'name': model_path.name,
            'size': size_gb,
            'load_time': load_time,
            'speed': speed
        })
        
        print(f"  å¤§å°: {size_gb:.2f} GB")
        print(f"  åŠ è½½: {load_time:.2f}ç§’")
        print(f"  é€Ÿåº¦: {speed:.2f} tokens/ç§’")
    
    # è¾“å‡ºå¯¹æ¯”è¡¨æ ¼
    print("\n" + "="*60)
    print("å¯¹æ¯”æ€»ç»“:")
    print(f"{'æ¨¡å‹':<40} {'å¤§å°':>8} {'åŠ è½½':>8} {'é€Ÿåº¦':>10}")
    print("-" * 60)
    for r in results:
        print(f"{r['name']:<40} {r['size']:>7.2f}G {r['load_time']:>7.2f}s {r['speed']:>9.2f}t/s")


if __name__ == "__main__":
    print("ğŸš€ llama.cppæ¨ç†æµ‹è¯•\n")
    
    # æŸ¥æ‰¾GGUFæ¨¡å‹
    print("ğŸ” æœç´¢GGUFæ¨¡å‹...")
    models = find_gguf_models()
    
    if not models:
        print("\nâŒ æœªæ‰¾åˆ°GGUFæ¨¡å‹")
        print("\nè¯·å…ˆä¸‹è½½æ¨¡å‹ï¼š")
        print("\næ–¹æ³•1ï¼šä»HuggingFaceä¸‹è½½ï¼ˆæ¨èï¼‰")
        print("  pip install huggingface-hub")
        print("  huggingface-cli download Qwen/Qwen2-7B-Instruct-GGUF \\")
        print("    qwen2-7b-instruct-q4_k_m.gguf \\")
        print("    --local-dir ~/llama.cpp/models")
        
        print("\næ–¹æ³•2ï¼šå¦‚æœå·²å®‰è£…Ollamaï¼Œæ¨¡å‹åœ¨ï¼š")
        print("  ~/.ollama/models/blobs/")
        print("  ï¼ˆéœ€è¦æ‰¾åˆ°.ggufæ–‡ä»¶ï¼‰")
        
        print("\næ–¹æ³•3ï¼šæ‰‹åŠ¨ä¸‹è½½")
        print("  è®¿é—®ï¼šhttps://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF")
        print("  ä¸‹è½½ï¼šqwen2-7b-instruct-q4_k_m.gguf")
        print("  ä¿å­˜åˆ°ï¼š~/llama.cpp/models/")
        
        exit(1)
    
    print(f"\nâœ… æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹:")
    for i, model in enumerate(models, 1):
        size_gb = model.stat().st_size / 1e9
        print(f"  {i}. {model.name} ({size_gb:.2f} GB)")
        print(f"     ä½ç½®: {model.parent}")
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•
    selected_model = models[0]
    
    print(f"\nä½¿ç”¨æ¨¡å‹: {selected_model.name}\n")
    
    # æµ‹è¯•1ï¼šåŸºç¡€æ¨ç†
    test_llamacpp_basic(
        selected_model,
        prompt="ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿç”¨ç®€å•çš„è¯è§£é‡Šã€‚"
    )
    
    # æµ‹è¯•2ï¼šæµå¼è¾“å‡º
    test_llamacpp_stream(
        selected_model,
        prompt="åˆ—ä¸¾3ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨åœºæ™¯"
    )
    
    # å¦‚æœæ‰¾åˆ°å¤šä¸ªæ¨¡å‹ï¼Œå¯¹æ¯”æ€§èƒ½
    if len(models) > 1:
        print("\n" + "="*60)
        response = input("æ˜¯å¦å¯¹æ¯”ä¸åŒé‡åŒ–æ–¹å¼çš„æ€§èƒ½ï¼Ÿ[y/N]: ")
        if response.lower() == 'y':
            benchmark_quantization(models[:3])  # æœ€å¤šå¯¹æ¯”3ä¸ª
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ æç¤ºï¼š")
    print("  - llama.cppæ¯”Transformerså¿«5-10å€")
    print("  - Q4_K_Mé‡åŒ–æ˜¯æœ€ä½³å¹³è¡¡é€‰æ‹©")
    print("  - M1èŠ¯ç‰‡MetalåŠ é€Ÿæ•ˆæœæ˜æ˜¾")
    print("  - å¯ä»¥é€šè¿‡n_threadså‚æ•°è°ƒæ•´CPUçº¿ç¨‹æ•°")

