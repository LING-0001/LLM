"""
Ollamaæ¨ç†æµ‹è¯•è„šæœ¬

ä½¿ç”¨å‰ç¡®ä¿ï¼š
1. å·²å®‰è£…Ollama
2. å·²ä¸‹è½½æ¨¡å‹ï¼šollama pull qwen2:7b
3. OllamaæœåŠ¡å·²å¯åŠ¨ï¼šollama serveï¼ˆé€šå¸¸è‡ªåŠ¨å¯åŠ¨ï¼‰
"""

import requests
import json
import time


def test_ollama_api(model="qwen2:7b", prompt="ç”¨ä¸€å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹"):
    """æµ‹è¯•Ollama API"""
    url = "http://localhost:11434/api/generate"
    
    print(f"ğŸ¤– æ¨¡å‹: {model}")
    print(f"ğŸ“ é—®é¢˜: {prompt}")
    print("-" * 50)
    
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 200
        }
    }
    
    try:
        start_time = time.time()
        response = requests.post(url, json=data, timeout=60)
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… å›ç­”: {result['response']}")
            print(f"\nâ±ï¸  è€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“Š ç”Ÿæˆtokenæ•°: {result.get('eval_count', 'N/A')}")
            if 'eval_count' in result and 'eval_duration' in result:
                tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)
                print(f"ğŸš€ é€Ÿåº¦: {tokens_per_sec:.2f} tokens/ç§’")
        else:
            print(f"âŒ é”™è¯¯: HTTP {response.status_code}")
            print(response.text)
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡")
        print("\nè¯·æ£€æŸ¥ï¼š")
        print("1. Ollamaæ˜¯å¦å·²å®‰è£…ï¼Ÿè¿è¡Œ: ollama --version")
        print("2. OllamaæœåŠ¡æ˜¯å¦å¯åŠ¨ï¼Ÿè¿è¡Œ: ollama serve")
        print("3. æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½ï¼Ÿè¿è¡Œ: ollama list")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


def test_ollama_stream(model="qwen2:7b", prompt="å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„å››è¡Œè¯—"):
    """æµ‹è¯•æµå¼è¾“å‡º"""
    url = "http://localhost:11434/api/generate"
    
    print(f"\n{'='*50}")
    print("ğŸŒŠ æµå¼è¾“å‡ºæµ‹è¯•")
    print(f"ğŸ¤– æ¨¡å‹: {model}")
    print(f"ğŸ“ é—®é¢˜: {prompt}")
    print("-" * 50)
    
    data = {
        "model": model,
        "prompt": prompt,
        "stream": True
    }
    
    try:
        response = requests.post(url, json=data, stream=True, timeout=60)
        
        print("ğŸ’¬ å›ç­”: ", end="", flush=True)
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if 'response' in chunk:
                    print(chunk['response'], end="", flush=True)
        print("\n")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


def check_available_models():
    """æ£€æŸ¥å¯ç”¨æ¨¡å‹"""
    url = "http://localhost:11434/api/tags"
    
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print("\nğŸ“¦ å·²ä¸‹è½½çš„æ¨¡å‹ï¼š")
            for model in models:
                print(f"  - {model['name']}")
                print(f"    å¤§å°: {model['size'] / 1e9:.2f} GB")
                print(f"    ä¿®æ”¹æ—¶é—´: {model['modified_at']}")
            return [m['name'] for m in models]
        else:
            print("âŒ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨")
            return []
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return []


if __name__ == "__main__":
    print("ğŸš€ Ollamaæ¨ç†æµ‹è¯•\n")
    
    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    available_models = check_available_models()
    
    if not available_models:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ°å·²ä¸‹è½½çš„æ¨¡å‹")
        print("\nè¯·å…ˆä¸‹è½½æ¨¡å‹ï¼š")
        print("  ollama pull qwen2:7b        # æ¨èï¼šç»¼åˆèƒ½åŠ›å¼º")
        print("  ollama pull deepseek-coder:7b  # ä»£ç èƒ½åŠ›å¼º")
        exit(1)
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹è¿›è¡Œæµ‹è¯•
    test_model = available_models[0]
    
    # æµ‹è¯•1ï¼šæ™®é€šé—®ç­”
    test_ollama_api(
        model=test_model,
        prompt="ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿç”¨ç®€å•çš„è¯­è¨€è§£é‡Šã€‚"
    )
    
    # æµ‹è¯•2ï¼šæµå¼è¾“å‡º
    test_ollama_stream(
        model=test_model,
        prompt="åˆ—ä¸¾3ä¸ªä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨åœºæ™¯"
    )
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ æç¤ºï¼š")
    print("  - ä¿®æ”¹promptå‚æ•°å¯ä»¥æµ‹è¯•ä¸åŒçš„é—®é¢˜")
    print("  - ä¿®æ”¹temperatureå‚æ•°å¯ä»¥è°ƒæ•´è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0.0-1.0ï¼‰")
    print("  - ä½¿ç”¨stream=Trueå¯ä»¥å®ç°æ‰“å­—æœºæ•ˆæœ")

