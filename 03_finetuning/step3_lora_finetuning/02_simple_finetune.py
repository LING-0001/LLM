#!/usr/bin/env python3
"""
ç®€å•å¾®è°ƒè„šæœ¬

ä½¿ç”¨LoRAå¾®è°ƒQwenæ¨¡å‹ï¼Œå®ç°äº¤é€šæ³•é—®ç­”
"""

import os
import sys
import json
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    print("æ£€æŸ¥ä¾èµ–åº“...")
    
    required = {
        'transformers': 'transformers',
        'peft': 'peft',
        'torch': 'torch',
        'datasets': 'datasets',
        'trl': 'trl'
    }
    
    missing = []
    for name, import_name in required.items():
        try:
            __import__(import_name)
            print(f"  âœ… {name}")
        except ImportError:
            print(f"  âŒ {name}")
            missing.append(name)
    
    if missing:
        print(f"\nç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        print("è¯·è¿è¡Œ: pip install " + " ".join(missing))
        return False
    
    return True


def load_dataset(train_file, eval_file):
    """åŠ è½½æ•°æ®é›†"""
    from datasets import Dataset
    
    print(f"\nğŸ“š åŠ è½½æ•°æ®é›†...")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_data = []
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            train_data.append(json.loads(line))
    
    # åŠ è½½éªŒè¯æ•°æ®
    eval_data = []
    with open(eval_file, 'r', encoding='utf-8') as f:
        for line in f:
            eval_data.append(json.loads(line))
    
    print(f"  è®­ç»ƒé›†: {len(train_data)} æ¡")
    print(f"  éªŒè¯é›†: {len(eval_data)} æ¡")
    
    # è½¬æ¢ä¸ºHuggingFace Dataset
    train_dataset = Dataset.from_list(train_data)
    eval_dataset = Dataset.from_list(eval_data)
    
    return train_dataset, eval_dataset


def setup_model_and_tokenizer(model_path):
    """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    print(f"\nğŸ¤– åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½åˆ†è¯å™¨
    print("  åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side='right'
    )
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    print("  åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype="auto",
        device_map=None  # CPUæ¨¡å¼
    )
    
    print(f"  âœ… æ¨¡å‹å‚æ•°: {model.num_parameters() / 1e9:.2f}B")
    
    return model, tokenizer


def setup_lora(model):
    """é…ç½®LoRA"""
    from peft import LoraConfig, get_peft_model, TaskType
    
    print("\nâš™ï¸  é…ç½®LoRA...")
    
    # LoRAé…ç½®
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,                    # rank
        lora_alpha=16,          # alpha
        lora_dropout=0.05,      # dropout
        target_modules=["q_proj", "v_proj"],  # ç›®æ ‡æ¨¡å—
        bias="none"
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"  æ€»å‚æ•°: {total_params / 1e6:.2f}M")
    print(f"  å¯è®­ç»ƒ: {trainable_params / 1e6:.2f}M ({trainable_params / total_params * 100:.2f}%)")
    
    return model


def preprocess_function(examples, tokenizer):
    """æ•°æ®é¢„å¤„ç†"""
    # æ ¼å¼åŒ–ä¸ºèŠå¤©æ¨¡æ¿
    texts = []
    for messages in examples["messages"]:
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        texts.append(text)
    
    # åˆ†è¯
    model_inputs = tokenizer(
        texts,
        max_length=512,
        truncation=True,
        padding=False,
    )
    
    # å¤åˆ¶input_idsä½œä¸ºlabels
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    
    return model_inputs


def train_model(model, tokenizer, train_dataset, eval_dataset, output_dir):
    """è®­ç»ƒæ¨¡å‹"""
    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # é¢„å¤„ç†æ•°æ®
    print("  é¢„å¤„ç†æ•°æ®...")
    train_dataset = train_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        remove_columns=train_dataset.column_names
    )
    
    eval_dataset = eval_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        remove_columns=eval_dataset.column_names
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=10,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        learning_rate=2e-4,
        logging_steps=5,
        save_steps=50,
        eval_strategy="steps",
        eval_steps=50,
        save_total_limit=2,
        remove_unused_columns=False,
        report_to="none",
        use_cpu=True,  # å¼ºåˆ¶ä½¿ç”¨CPU
    )
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 60)
    print("è®­ç»ƒä¸­...".center(60))
    print("=" * 60)
    print("\nè¿™å¯èƒ½éœ€è¦30-60åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("ä½ å¯ä»¥çœ‹åˆ°lossé€æ¸ä¸‹é™\n")
    
    trainer.train()
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    
    return trainer


def save_model(model, tokenizer, output_dir):
    """ä¿å­˜æ¨¡å‹"""
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
    
    # ä¿å­˜LoRAé€‚é…å™¨
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print("  âœ… æ¨¡å‹å·²ä¿å­˜")


def show_summary(output_dir):
    """æ˜¾ç¤ºæ€»ç»“"""
    print("\n" + "=" * 60)
    print("ğŸ‰ å¾®è°ƒå®Œæˆï¼".center(60))
    print("=" * 60)
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("\nåŒ…å«æ–‡ä»¶:")
    print("  - adapter_config.json  # LoRAé…ç½®")
    print("  - adapter_model.bin    # LoRAæƒé‡ (~15MB)")
    print("  - tokenizeré…ç½®")
    
    print("\nğŸš€ ä¸‹ä¸€æ­¥:")
    print("  python 03_test_finetuned.py  # æµ‹è¯•å¾®è°ƒæ•ˆæœ")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("LoRA å¾®è°ƒå®æˆ˜".center(60))
    print("=" * 60)
    
    # 1. æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # 2. é…ç½®è·¯å¾„
    # æ•°æ®è·¯å¾„
    data_dir = Path(__file__).parent.parent / "step2_data_preparation" / "data"
    train_file = data_dir / "train.jsonl"
    eval_file = data_dir / "eval.jsonl"
    
    # å¦‚æœæ•°æ®åœ¨æ ¹ç›®å½•çš„dataæ–‡ä»¶å¤¹
    if not train_file.exists():
        data_dir = Path(__file__).parent.parent.parent / "data"
        train_file = data_dir / "train.jsonl"
        eval_file = data_dir / "eval.jsonl"
    
    if not train_file.exists():
        print(f"\nâŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®: {train_file}")
        print("è¯·å…ˆè¿è¡Œ: python ../step2_data_preparation/02_create_dataset.py")
        return
    
    # æ¨¡å‹è·¯å¾„
    model_path = os.path.expanduser("~/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf")
    
    # æ£€æŸ¥æ¨¡å‹
    if not Path(model_path).exists():
        print(f"\nâŒ æœªæ‰¾åˆ°æ¨¡å‹: {model_path}")
        print("\nè¯·æä¾›æ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return
    
    # æ³¨æ„ï¼šGGUFæ ¼å¼ä¸èƒ½ç›´æ¥ç”¨äºå¾®è°ƒ
    # æˆ‘ä»¬éœ€è¦ä½¿ç”¨HuggingFaceæ ¼å¼çš„æ¨¡å‹
    print("\nâš ï¸  æ³¨æ„: GGUFæ ¼å¼ä¸æ”¯æŒå¾®è°ƒ")
    print("æˆ‘ä»¬éœ€è¦HuggingFaceæ ¼å¼çš„æ¨¡å‹")
    print("\nå»ºè®®:")
    print("  1. ä½¿ç”¨åœ¨çº¿Colab/Kaggle (å…è´¹GPU)")
    print("  2. ä¸‹è½½HFæ ¼å¼æ¨¡å‹è¿›è¡Œæœ¬åœ°å¾®è°ƒ")
    print("  3. å…ˆç†è§£æµç¨‹ï¼Œå®é™…å¾®è°ƒå¯ä»¥äº‘ç«¯è¿›è¡Œ")
    print("\næœ¬è„šæœ¬å±•ç¤ºå®Œæ•´çš„å¾®è°ƒæµç¨‹ä»£ç ")
    
    # è¾“å‡ºç›®å½•
    output_dir = "./output/lora-traffic-law"
    
    print(f"\nğŸ“ é…ç½®:")
    print(f"  æ•°æ®: {data_dir}")
    print(f"  æ¨¡å‹: {model_path}")
    print(f"  è¾“å‡º: {output_dir}")
    
    # ç”±äºGGUFæ ¼å¼é—®é¢˜ï¼Œè¿™é‡Œåªå±•ç¤ºæµç¨‹
    print("\n" + "=" * 60)
    print("ğŸ’¡ å¾®è°ƒæµç¨‹è¯´æ˜".center(60))
    print("=" * 60)
    
    print("\nå®Œæ•´çš„å¾®è°ƒæ­¥éª¤:")
    print("  1. âœ… å‡†å¤‡æ•°æ® (å·²å®Œæˆ)")
    print("  2. â¸ï¸  åŠ è½½æ¨¡å‹ (éœ€è¦HFæ ¼å¼)")
    print("  3. â¸ï¸  é…ç½®LoRA")
    print("  4. â¸ï¸  è®­ç»ƒæ¨¡å‹")
    print("  5. â¸ï¸  ä¿å­˜é€‚é…å™¨")
    
    print("\nç”±äºæ¨¡å‹æ ¼å¼é™åˆ¶ï¼Œå»ºè®®ä½¿ç”¨Google Colabè¿›è¡Œå®é™…å¾®è°ƒ")
    print("æˆ‘å·²ç»å‡†å¤‡å¥½äº†å®Œæ•´çš„ä»£ç æ¡†æ¶")


if __name__ == "__main__":
    main()

