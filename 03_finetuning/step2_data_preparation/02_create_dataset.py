#!/usr/bin/env python3
"""
åˆ›å»ºè®­ç»ƒæ•°æ®é›†

åŸºäºäº¤é€šæ³•æ–‡æ¡£ï¼Œç”Ÿæˆé—®ç­”å¯¹è®­ç»ƒæ•°æ®
"""

import json
import os
import random
from pathlib import Path


def load_traffic_law():
    """åŠ è½½äº¤é€šæ³•æ–‡æ¡£"""
    # ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„äº¤é€šæ³•æ–‡æ¡£
    doc_path = Path(__file__).parent.parent.parent / "traffic_law_document.md"
    
    if not doc_path.exists():
        print(f"âš ï¸  æœªæ‰¾åˆ°æ–‡æ¡£: {doc_path}")
        print("ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®...")
        return get_sample_document()
    
    with open(doc_path, 'r', encoding='utf-8') as f:
        return f.read()


def get_sample_document():
    """è·å–ç¤ºä¾‹æ–‡æ¡£ï¼ˆå¦‚æœä¸»æ–‡æ¡£ä¸å­˜åœ¨ï¼‰"""
    return """
# é“è·¯äº¤é€šå®‰å…¨æ³•

## ç¬¬ä¸€ç«  æ€»åˆ™

æœºåŠ¨è½¦é©¾é©¶äººåº”å½“éµå®ˆé“è·¯äº¤é€šå®‰å…¨æ³•å¾‹æ³•è§„ï¼ŒæŒ‰ç…§æ“ä½œè§„èŒƒå®‰å…¨é©¾é©¶ã€æ–‡æ˜é©¾é©¶ã€‚
é¥®é…’ã€æœç”¨å›½å®¶ç®¡åˆ¶çš„ç²¾ç¥è¯å“æˆ–è€…éº»é†‰è¯å“ï¼Œä¸å¾—é©¾é©¶æœºåŠ¨è½¦ã€‚

## ç¬¬äºŒç«  è¿æ³•å¤„ç½š

### é†‰é…’é©¾é©¶
é†‰é…’é©¾é©¶æœºåŠ¨è½¦çš„ï¼Œç”±å…¬å®‰æœºå…³äº¤é€šç®¡ç†éƒ¨é—¨çº¦æŸè‡³é…’é†’ï¼ŒåŠé”€æœºåŠ¨è½¦é©¾é©¶è¯ï¼Œä¾æ³•è¿½ç©¶åˆ‘äº‹è´£ä»»ï¼›äº”å¹´å†…ä¸å¾—é‡æ–°å–å¾—æœºåŠ¨è½¦é©¾é©¶è¯ã€‚

### é¥®é…’é©¾é©¶
é¥®é…’åé©¾é©¶æœºåŠ¨è½¦çš„ï¼Œå¤„æš‚æ‰£å…­ä¸ªæœˆæœºåŠ¨è½¦é©¾é©¶è¯ï¼Œå¹¶å¤„ä¸€åƒå…ƒä»¥ä¸ŠäºŒåƒå…ƒä»¥ä¸‹ç½šæ¬¾ã€‚
é¥®é…’åé©¾é©¶è¥è¿æœºåŠ¨è½¦çš„ï¼Œå¤„åäº”æ—¥æ‹˜ç•™ï¼Œå¹¶å¤„äº”åƒå…ƒç½šæ¬¾ï¼ŒåŠé”€æœºåŠ¨è½¦é©¾é©¶è¯ï¼Œäº”å¹´å†…ä¸å¾—é‡æ–°å–å¾—æœºåŠ¨è½¦é©¾é©¶è¯ã€‚

### è¶…é€Ÿé©¾é©¶
è¶…é€Ÿé©¾é©¶æŒ‰ç…§è¶…é€Ÿæ¯”ä¾‹è¿›è¡Œå¤„ç½šã€‚è¶…é€Ÿ50%ä»¥ä¸Šçš„ï¼Œå¤„ä»¥ç½šæ¬¾å¹¶æ‰£12åˆ†ã€‚

### é—¯çº¢ç¯
é—¯çº¢ç¯çš„ï¼Œä¸€æ¬¡è®°6åˆ†ï¼Œç½šæ¬¾200å…ƒã€‚

## ç¬¬ä¸‰ç«  ç‰¹æ®Šè§„å®š

### ç–²åŠ³é©¾é©¶
è¿ç»­é©¾é©¶è¶…è¿‡4å°æ—¶æœªåœè½¦ä¼‘æ¯çš„ï¼Œå¤„ä»¥è­¦å‘Šæˆ–200å…ƒç½šæ¬¾ã€‚

### ä¸ç³»å®‰å…¨å¸¦
åœ¨é«˜é€Ÿå…¬è·¯æˆ–åŸå¸‚å¿«é€Ÿè·¯é©¾é©¶ä¸ç³»å®‰å…¨å¸¦çš„ï¼Œç½šæ¬¾200å…ƒï¼Œæ‰£2åˆ†ã€‚
"""


def generate_qa_pairs():
    """ç”Ÿæˆé—®ç­”å¯¹"""
    print("=" * 60)
    print("ç”Ÿæˆè®­ç»ƒæ•°æ®".center(60))
    print("=" * 60)
    
    # å®šä¹‰QAæ¨¡æ¿
    qa_templates = [
        # é†‰é©¾ç›¸å…³
        {
            "questions": [
                "é†‰é©¾ä¼šå—åˆ°ä»€ä¹ˆå¤„ç½šï¼Ÿ",
                "é†‰é…’é©¾é©¶æœ‰ä»€ä¹ˆåæœï¼Ÿ",
                "é†‰é…’åå¼€è½¦ä¼šæ€æ ·ï¼Ÿ",
                "å–é†‰äº†å¼€è½¦ä¼šè¢«æ€ä¹ˆå¤„ç½šï¼Ÿ",
                "é†‰é©¾çš„æ³•å¾‹åæœæ˜¯ä»€ä¹ˆï¼Ÿ"
            ],
            "answer": "é†‰é…’é©¾é©¶æœºåŠ¨è½¦çš„ï¼Œç”±å…¬å®‰æœºå…³äº¤é€šç®¡ç†éƒ¨é—¨çº¦æŸè‡³é…’é†’ï¼ŒåŠé”€æœºåŠ¨è½¦é©¾é©¶è¯ï¼Œä¾æ³•è¿½ç©¶åˆ‘äº‹è´£ä»»ï¼›äº”å¹´å†…ä¸å¾—é‡æ–°å–å¾—æœºåŠ¨è½¦é©¾é©¶è¯ã€‚"
        },
        # é¥®é…’é©¾é©¶
        {
            "questions": [
                "å–é…’åå¼€è½¦ä¼šè¢«æ€ä¹ˆå¤„ç½šï¼Ÿ",
                "é¥®é…’é©¾é©¶çš„å¤„ç½šæ˜¯ä»€ä¹ˆï¼Ÿ",
                "é…’åé©¾è½¦ä¼šæ‰£åˆ†å—ï¼Ÿ",
                "å–äº†é…’å¯ä»¥å¼€è½¦å—ï¼Ÿ",
                "é…’é©¾çš„å¤„ç½šæ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
            ],
            "answer": "é¥®é…’åé©¾é©¶æœºåŠ¨è½¦çš„ï¼Œå¤„æš‚æ‰£å…­ä¸ªæœˆæœºåŠ¨è½¦é©¾é©¶è¯ï¼Œå¹¶å¤„ä¸€åƒå…ƒä»¥ä¸ŠäºŒåƒå…ƒä»¥ä¸‹ç½šæ¬¾ã€‚å¦‚æœæ˜¯è¥è¿æœºåŠ¨è½¦ï¼Œå¤„åäº”æ—¥æ‹˜ç•™ï¼Œå¹¶å¤„äº”åƒå…ƒç½šæ¬¾ï¼ŒåŠé”€æœºåŠ¨è½¦é©¾é©¶è¯ï¼Œäº”å¹´å†…ä¸å¾—é‡æ–°å–å¾—ã€‚"
        },
        # é—¯çº¢ç¯
        {
            "questions": [
                "é—¯çº¢ç¯ä¼šæ‰£å¤šå°‘åˆ†ï¼Ÿ",
                "é—¯çº¢ç¯çš„å¤„ç½šæ˜¯ä»€ä¹ˆï¼Ÿ",
                "çº¢ç¯æ²¡åœä¼šæ€æ ·ï¼Ÿ",
                "è¯¯é—¯çº¢ç¯ä¼šè¢«ç½šæ¬¾å—ï¼Ÿ",
                "é—¯çº¢ç¯ç½šå¤šå°‘é’±ï¼Ÿ"
            ],
            "answer": "é—¯çº¢ç¯çš„ï¼Œä¸€æ¬¡è®°6åˆ†ï¼Œç½šæ¬¾200å…ƒã€‚"
        },
        # è¶…é€Ÿ
        {
            "questions": [
                "è¶…é€Ÿä¼šè¢«æ‰£åˆ†å—ï¼Ÿ",
                "è¶…é€Ÿé©¾é©¶æ€ä¹ˆå¤„ç½šï¼Ÿ",
                "å¼€è½¦è¶…é€Ÿ50%ä»¥ä¸Šä¼šæ€æ ·ï¼Ÿ",
                "è¶…é€Ÿçš„å¤„ç½šæ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
                "è¶…é€Ÿè¡Œé©¶çš„åæœï¼Ÿ"
            ],
            "answer": "è¶…é€Ÿé©¾é©¶æŒ‰ç…§è¶…é€Ÿæ¯”ä¾‹è¿›è¡Œå¤„ç½šã€‚è¶…é€Ÿ50%ä»¥ä¸Šçš„ï¼Œå¤„ä»¥ç½šæ¬¾å¹¶æ‰£12åˆ†ã€‚"
        },
        # ä¸ç³»å®‰å…¨å¸¦
        {
            "questions": [
                "ä¸ç³»å®‰å…¨å¸¦ä¼šè¢«ç½šæ¬¾å—ï¼Ÿ",
                "é«˜é€Ÿä¸Šä¸ç³»å®‰å…¨å¸¦æ€ä¹ˆå¤„ç½šï¼Ÿ",
                "å¼€è½¦ä¸ç³»å®‰å…¨å¸¦æ‰£åˆ†å—ï¼Ÿ",
                "å®‰å…¨å¸¦å¿…é¡»ç³»å—ï¼Ÿ",
                "ä¸ç³»å®‰å…¨å¸¦çš„å¤„ç½šï¼Ÿ"
            ],
            "answer": "åœ¨é«˜é€Ÿå…¬è·¯æˆ–åŸå¸‚å¿«é€Ÿè·¯é©¾é©¶ä¸ç³»å®‰å…¨å¸¦çš„ï¼Œç½šæ¬¾200å…ƒï¼Œæ‰£2åˆ†ã€‚"
        },
        # ç–²åŠ³é©¾é©¶
        {
            "questions": [
                "ç–²åŠ³é©¾é©¶ä¼šè¢«å¤„ç½šå—ï¼Ÿ",
                "è¿ç»­å¼€è½¦å¤šä¹…è¦ä¼‘æ¯ï¼Ÿ",
                "ç–²åŠ³é©¾é©¶çš„å¤„ç½šæ ‡å‡†ï¼Ÿ",
                "å¼€è½¦ç´¯äº†å¯ä»¥ç»§ç»­å—ï¼Ÿ",
                "é•¿æ—¶é—´é©¾é©¶è¿æ³•å—ï¼Ÿ"
            ],
            "answer": "è¿ç»­é©¾é©¶è¶…è¿‡4å°æ—¶æœªåœè½¦ä¼‘æ¯çš„ï¼Œå¤„ä»¥è­¦å‘Šæˆ–200å…ƒç½šæ¬¾ã€‚"
        },
    ]
    
    # ç”Ÿæˆæ•°æ®é›†
    dataset = []
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº¤é€šæ³•å¾‹é¡¾é—®ï¼Œå›ç­”è¦å‡†ç¡®ã€ä¸“ä¸šã€ç®€æ´ã€‚"
    
    print(f"\nğŸ“ ç”Ÿæˆé—®ç­”å¯¹...")
    for template in qa_templates:
        for question in template['questions']:
            data_point = {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question},
                    {"role": "assistant", "content": template['answer']}
                ]
            }
            dataset.append(data_point)
    
    print(f"âœ… ç”Ÿæˆäº† {len(dataset)} æ¡æ•°æ®")
    
    # æ·»åŠ ä¸€äº›å˜ä½“å’Œç»„åˆé—®é¢˜
    print(f"\nğŸ“ ç”Ÿæˆå˜ä½“å’Œç»„åˆé—®é¢˜...")
    
    combo_questions = [
        {
            "question": "é†‰é©¾å’Œé…’é©¾æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "answer": "é†‰é©¾å’Œé…’é©¾çš„å¤„ç½šç¨‹åº¦ä¸åŒã€‚é¥®é…’é©¾é©¶ï¼ˆé…’é©¾ï¼‰å¤„æš‚æ‰£å…­ä¸ªæœˆé©¾é©¶è¯ï¼Œç½šæ¬¾1000-2000å…ƒï¼›é†‰é…’é©¾é©¶ï¼ˆé†‰é©¾ï¼‰åˆ™ä¼šåŠé”€é©¾é©¶è¯ï¼Œè¿½ç©¶åˆ‘äº‹è´£ä»»ï¼Œäº”å¹´å†…ä¸å¾—é‡æ–°å–å¾—é©¾é©¶è¯ã€‚é†‰é©¾çš„å¤„ç½šæ›´ä¸¥å‰ã€‚"
        },
        {
            "question": "å“ªäº›äº¤é€šè¿æ³•ä¼šè¢«æ‰£12åˆ†ï¼Ÿ",
            "answer": "æ ¹æ®äº¤é€šæ³•è§„ï¼Œè¶…é€Ÿ50%ä»¥ä¸Šä¼šè¢«æ‰£12åˆ†ã€‚è¿™æ˜¯æœ€ä¸¥é‡çš„äº¤é€šè¿æ³•è¡Œä¸ºä¹‹ä¸€ã€‚"
        },
        {
            "question": "äº¤é€šè¿æ³•æ‰£åˆ†å¯ä»¥ç´¯ç§¯å—ï¼Ÿ",
            "answer": "æ˜¯çš„ï¼Œäº¤é€šè¿æ³•æ‰£åˆ†æ˜¯ç´¯ç§¯çš„ã€‚ä¸€ä¸ªè®°åˆ†å‘¨æœŸï¼ˆ12ä¸ªæœˆï¼‰å†…ç´¯ç§¯æ‰£æ»¡12åˆ†ï¼Œéœ€è¦é‡æ–°å­¦ä¹ å¹¶è€ƒè¯•ã€‚"
        },
        {
            "question": "é«˜é€Ÿå…¬è·¯ä¸Šæœ‰å“ªäº›ç‰¹æ®Šè§„å®šï¼Ÿ",
            "answer": "é«˜é€Ÿå…¬è·¯ä¸Šé©¾é©¶æ—¶ï¼Œå¿…é¡»ç³»å®‰å…¨å¸¦ï¼Œå¦åˆ™ç½šæ¬¾200å…ƒå¹¶æ‰£2åˆ†ã€‚åŒæ—¶è¦æ³¨æ„ä¸è¦ç–²åŠ³é©¾é©¶ï¼Œè¿ç»­é©¾é©¶è¶…è¿‡4å°æ—¶å¿…é¡»åœè½¦ä¼‘æ¯ã€‚"
        },
    ]
    
    for item in combo_questions:
        data_point = {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": item['question']},
                {"role": "assistant", "content": item['answer']}
            ]
        }
        dataset.append(data_point)
    
    print(f"âœ… æ€»å…±ç”Ÿæˆäº† {len(dataset)} æ¡æ•°æ®")
    
    return dataset


def split_dataset(dataset, train_ratio=0.9):
    """åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    print(f"\nğŸ“Š åˆ’åˆ†æ•°æ®é›† (è®­ç»ƒ:{train_ratio*100:.0f}% / éªŒè¯:{(1-train_ratio)*100:.0f}%)")
    
    # æ‰“ä¹±æ•°æ®
    random.shuffle(dataset)
    
    # åˆ’åˆ†
    split_idx = int(len(dataset) * train_ratio)
    train_data = dataset[:split_idx]
    eval_data = dataset[split_idx:]
    
    print(f"  è®­ç»ƒé›†: {len(train_data)} æ¡")
    print(f"  éªŒè¯é›†: {len(eval_data)} æ¡")
    
    return train_data, eval_data


def save_dataset(train_data, eval_data, output_dir="./data"):
    """ä¿å­˜æ•°æ®é›†"""
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ° {output_dir}/")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé›†
    train_file = os.path.join(output_dir, "train.jsonl")
    with open(train_file, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"  âœ… {train_file}")
    
    # ä¿å­˜éªŒè¯é›†
    eval_file = os.path.join(output_dir, "eval.jsonl")
    with open(eval_file, 'w', encoding='utf-8') as f:
        for item in eval_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"  âœ… {eval_file}")
    
    return train_file, eval_file


def show_samples(train_file, eval_file):
    """å±•ç¤ºæ•°æ®æ ·æœ¬"""
    print("\n" + "=" * 60)
    print("æ•°æ®æ ·æœ¬é¢„è§ˆ".center(60))
    print("=" * 60)
    
    print("\nã€è®­ç»ƒé›†æ ·æœ¬ã€‘")
    print("-" * 60)
    with open(train_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 2:  # åªæ˜¾ç¤ºå‰2æ¡
                break
            data = json.loads(line)
            print(f"\næ ·æœ¬ {i+1}:")
            print(json.dumps(data, ensure_ascii=False, indent=2))
    
    print("\n" + "-" * 60)
    print(f"... (å…± {sum(1 for _ in open(train_file))} æ¡)")


def show_summary():
    """æ˜¾ç¤ºæ€»ç»“"""
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼".center(60))
    print("=" * 60)
    
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ./data/train.jsonl  # è®­ç»ƒæ•°æ®")
    print("  ./data/eval.jsonl   # éªŒè¯æ•°æ®")
    
    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print("  æ ¼å¼: å¯¹è¯æ ¼å¼ (Chat Format)")
    print("  System: äº¤é€šæ³•å¾‹é¡¾é—®")
    print("  è¦†ç›–: é†‰é©¾ã€é…’é©¾ã€é—¯çº¢ç¯ã€è¶…é€Ÿç­‰")
    
    print("\nğŸš€ ä¸‹ä¸€æ­¥:")
    print("  1. æ£€æŸ¥æ•°æ®è´¨é‡: python 03_data_quality.py")
    print("  2. å¼€å§‹å¾®è°ƒ: cd ../step3_lora_finetuning")


def main():
    """ä¸»å‡½æ•°"""
    # 1. åŠ è½½æ–‡æ¡£
    print("ğŸ“„ åŠ è½½äº¤é€šæ³•æ–‡æ¡£...")
    doc = load_traffic_law()
    print(f"âœ… æ–‡æ¡£é•¿åº¦: {len(doc)} å­—ç¬¦\n")
    
    # 2. ç”ŸæˆQAå¯¹
    dataset = generate_qa_pairs()
    
    # 3. åˆ’åˆ†æ•°æ®é›†
    train_data, eval_data = split_dataset(dataset)
    
    # 4. ä¿å­˜æ•°æ®é›†
    train_file, eval_file = save_dataset(train_data, eval_data)
    
    # 5. å±•ç¤ºæ ·æœ¬
    show_samples(train_file, eval_file)
    
    # 6. æ€»ç»“
    show_summary()


if __name__ == "__main__":
    main()

