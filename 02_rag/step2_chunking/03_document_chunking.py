"""
ç»ƒä¹ 3ï¼šå¤„ç†çœŸå®æ–‡æ¡£
è¯»å–TXT/Markdownæ–‡ä»¶ï¼Œæ™ºèƒ½åˆ‡å—å¹¶ä¿å­˜ç»“æœ
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter
import os
import json

print("="*70)
print(" "*20 + "çœŸå®æ–‡æ¡£åˆ‡å—")
print("="*70)
print()

# åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
sample_doc = """# MyLLM é¡¹ç›®å­¦ä¹ æŒ‡å—

## é¡¹ç›®ç®€ä»‹

MyLLMæ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹å­¦ä¹ å¤§è¯­è¨€æ¨¡å‹çš„é¡¹ç›®ï¼ŒåŒ…å«RAGå’ŒFine-tuningä¸¤å¤§æ ¸å¿ƒå†…å®¹ã€‚

## å­¦ä¹ è·¯çº¿

### é˜¶æ®µ1ï¼šRAGå­¦ä¹ 

RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§è®©LLMèƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†çš„æŠ€æœ¯ã€‚

#### Step 1: ç†è§£RAGåŸç†

å­¦ä¹ RAGçš„åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œæµç¨‹ã€‚é€šè¿‡å¯¹æ¯”å®éªŒï¼Œç›´è§‚æ„Ÿå—RAGçš„ä»·å€¼ã€‚

#### Step 2: æ–‡æœ¬åˆ‡å—

å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆåˆé€‚çš„å°å—ã€‚å­¦ä¹ ä¸‰ç§åˆ‡å—æ–¹æ³•ï¼šå›ºå®šé•¿åº¦ã€æŒ‰åˆ†éš”ç¬¦ã€æ™ºèƒ½è¯­ä¹‰åˆ‡å—ã€‚

#### Step 3: å‘é‡åŒ–

å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚å­¦ä¹ Embeddingæ¨¡å‹çš„ä½¿ç”¨ï¼Œè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ã€‚

#### Step 4: å‘é‡æ•°æ®åº“

ä½¿ç”¨ChromaDBå­˜å‚¨å’Œæ£€ç´¢å‘é‡ã€‚å­¦ä¹ å¦‚ä½•é«˜æ•ˆç®¡ç†å¤§é‡æ–‡æ¡£ã€‚

#### Step 5: æ£€ç´¢ä¸ç”Ÿæˆ

æ•´åˆæ£€ç´¢å’Œç”Ÿæˆæµç¨‹ï¼Œæ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿã€‚

### é˜¶æ®µ2ï¼šFine-tuningå­¦ä¹ 

Fine-tuningæ˜¯ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ã€‚

#### Step 1: ç†è§£å¾®è°ƒåŸç†

å­¦ä¹ ä»€ä¹ˆæ˜¯å¾®è°ƒï¼ŒLoRAå’ŒQLoRAçš„åŒºåˆ«ã€‚

#### Step 2: æ•°æ®å‡†å¤‡

æ„é€ é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚

#### Step 3: è®­ç»ƒè¿‡ç¨‹

ä½¿ç”¨Unslothæˆ–LLaMA-Factoryè¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚

## æœ€ä½³å®è·µ

### RAGæœ€ä½³å®è·µ

1. é€‰æ‹©åˆé€‚çš„chunk_sizeï¼ˆ300-500å­—ï¼‰
2. è®¾ç½®15-20%çš„overlap
3. ä½¿ç”¨é«˜è´¨é‡çš„Embeddingæ¨¡å‹
4. ä¼˜åŒ–æ£€ç´¢ç­–ç•¥

### Fine-tuningæœ€ä½³å®è·µ

1. å‡†å¤‡é«˜è´¨é‡æ•°æ®ï¼ˆè‡³å°‘1000æ¡ï¼‰
2. ä½¿ç”¨LoRAé™ä½è®­ç»ƒæˆæœ¬
3. ä»”ç»†é€‰æ‹©è®­ç»ƒå‚æ•°
4. è¯„ä¼°æ¨¡å‹æ•ˆæœ

## æ€»ç»“

é€šè¿‡ç³»ç»Ÿå­¦ä¹ RAGå’ŒFine-tuningï¼Œä½ å°†æŒæ¡å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåº”ç”¨æŠ€æœ¯ã€‚
"""

# ä¿å­˜ç¤ºä¾‹æ–‡æ¡£
doc_path = "sample_document.md"
with open(doc_path, 'w', encoding='utf-8') as f:
    f.write(sample_doc)

print(f"ğŸ“„ å·²åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ï¼š{doc_path}")
print(f"   æ–‡ä»¶å¤§å°ï¼š{len(sample_doc)} å­—ç¬¦")
print()

# è¯»å–æ–‡æ¡£
print("="*70)
print("æ­¥éª¤1ï¼šè¯»å–æ–‡æ¡£")
print("="*70)
print()

with open(doc_path, 'r', encoding='utf-8') as f:
    content = f.read()

print(f"âœ… æˆåŠŸè¯»å–æ–‡æ¡£")
print(f"   æ€»é•¿åº¦ï¼š{len(content)} å­—ç¬¦")
print(f"   è¡Œæ•°ï¼š{content.count(chr(10)) + 1} è¡Œ")
print()

# æ™ºèƒ½åˆ‡å—
print("="*70)
print("æ­¥éª¤2ï¼šæ™ºèƒ½åˆ‡å—")
print("="*70)
print()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
    separators=["\n## ", "\n### ", "\n#### ", "\n\n", "\n", "ã€‚", " ", ""]
)

chunks = text_splitter.split_text(content)

print(f"âœ… åˆ‡å—å®Œæˆ")
print(f"   å—æ•°ï¼š{len(chunks)} å—")
print(f"   å¹³å‡å¤§å°ï¼š{sum(len(c) for c in chunks) / len(chunks):.0f} å­—ç¬¦")
print()

# å±•ç¤ºåˆ‡å—ç»“æœ
print("="*70)
print("æ­¥éª¤3ï¼šæŸ¥çœ‹åˆ‡å—ç»“æœ")
print("="*70)
print()

for i, chunk in enumerate(chunks[:5], 1):  # åªæ˜¾ç¤ºå‰5å—
    print(f"ã€å— {i}ã€‘({len(chunk)} å­—ç¬¦)")
    print(chunk[:150] + ("..." if len(chunk) > 150 else ""))
    print(f"{'â”€'*70}")
    print()

if len(chunks) > 5:
    print(f"... è¿˜æœ‰ {len(chunks) - 5} å—\n")

# æ·»åŠ å…ƒæ•°æ®
print("="*70)
print("æ­¥éª¤4ï¼šæ·»åŠ å…ƒæ•°æ®")
print("="*70)
print()

chunks_with_metadata = []
for i, chunk in enumerate(chunks):
    chunk_data = {
        "chunk_id": i,
        "content": chunk,
        "length": len(chunk),
        "source": doc_path,
        "metadata": {
            "has_heading": chunk.strip().startswith("#"),
            "is_code": "```" in chunk,
            "has_list": any(chunk.strip().startswith(marker) for marker in ["- ", "1. ", "* "]),
            "char_start": sum(len(chunks[j]) for j in range(i)),
            "char_end": sum(len(chunks[j]) for j in range(i+1))
        }
    }
    chunks_with_metadata.append(chunk_data)

print("âœ… å·²ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®")
print()
print("ç¤ºä¾‹ï¼ˆå—1çš„å…ƒæ•°æ®ï¼‰ï¼š")
print(json.dumps(chunks_with_metadata[0]["metadata"], indent=2, ensure_ascii=False))
print()

# ä¿å­˜ç»“æœ
print("="*70)
print("æ­¥éª¤5ï¼šä¿å­˜åˆ‡å—ç»“æœ")
print("="*70)
print()

output_file = "chunks_output.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(chunks_with_metadata, f, ensure_ascii=False, indent=2)

print(f"âœ… åˆ‡å—ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_file}")
print()

# ç»Ÿè®¡åˆ†æ
print("="*70)
print("ğŸ“Š åˆ‡å—ç»Ÿè®¡åˆ†æ")
print("="*70)
print()

# å—å¤§å°åˆ†å¸ƒ
sizes = [len(c) for c in chunks]
print("å—å¤§å°åˆ†å¸ƒï¼š")
print(f"   æœ€å°ï¼š{min(sizes)} å­—ç¬¦")
print(f"   æœ€å¤§ï¼š{max(sizes)} å­—ç¬¦")
print(f"   å¹³å‡ï¼š{sum(sizes)/len(sizes):.1f} å­—ç¬¦")
print(f"   ä¸­ä½æ•°ï¼š{sorted(sizes)[len(sizes)//2]} å­—ç¬¦")
print()

# å†…å®¹ç±»å‹ç»Ÿè®¡
has_heading = sum(1 for c in chunks_with_metadata if c["metadata"]["has_heading"])
has_code = sum(1 for c in chunks_with_metadata if c["metadata"]["is_code"])
has_list = sum(1 for c in chunks_with_metadata if c["metadata"]["has_list"])

print("å†…å®¹ç±»å‹ç»Ÿè®¡ï¼š")
print(f"   åŒ…å«æ ‡é¢˜çš„å—ï¼š{has_heading} ({has_heading/len(chunks)*100:.1f}%)")
print(f"   åŒ…å«ä»£ç çš„å—ï¼š{has_code} ({has_code/len(chunks)*100:.1f}%)")
print(f"   åŒ…å«åˆ—è¡¨çš„å—ï¼š{has_list} ({has_list/len(chunks)*100:.1f}%)")
print()

# å¯è§†åŒ–å—å¤§å°
print("å—å¤§å°å¯è§†åŒ–ï¼š")
for i, size in enumerate(sizes, 1):
    bar = "â–ˆ" * (size // 15)
    print(f"   å—{i:2d}: {bar} ({size})")
print()

# å®ç”¨å‡½æ•°ï¼šæœç´¢å—
print("="*70)
print("å®ç”¨åŠŸèƒ½ï¼šæœç´¢ç‰¹å®šå†…å®¹çš„å—")
print("="*70)
print()

def search_chunks(chunks_data, keyword):
    """æœç´¢åŒ…å«å…³é”®è¯çš„å—"""
    results = []
    for chunk_data in chunks_data:
        if keyword.lower() in chunk_data["content"].lower():
            results.append({
                "chunk_id": chunk_data["chunk_id"],
                "preview": chunk_data["content"][:100] + "...",
                "length": chunk_data["length"]
            })
    return results

# æœç´¢ç¤ºä¾‹
keywords = ["RAG", "Fine-tuning", "æœ€ä½³å®è·µ"]
for keyword in keywords:
    results = search_chunks(chunks_with_metadata, keyword)
    print(f"ğŸ” æœç´¢ '{keyword}'ï¼šæ‰¾åˆ° {len(results)} ä¸ªå—")
    if results:
        print(f"   ç¤ºä¾‹ï¼šå—{results[0]['chunk_id']} - {results[0]['preview']}")
    print()

# è´¨é‡æ£€æŸ¥
print("="*70)
print("ğŸ” åˆ‡å—è´¨é‡æ£€æŸ¥")
print("="*70)
print()

def check_chunk_quality(chunks):
    """æ£€æŸ¥åˆ‡å—è´¨é‡"""
    issues = []
    
    for i, chunk in enumerate(chunks):
        # æ£€æŸ¥æ˜¯å¦å¤ªçŸ­
        if len(chunk) < 50:
            issues.append(f"å—{i}: å¤ªçŸ­ï¼ˆ{len(chunk)}å­—ç¬¦ï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦å¤ªé•¿
        if len(chunk) > 600:
            issues.append(f"å—{i}: å¤ªé•¿ï¼ˆ{len(chunk)}å­—ç¬¦ï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦ä»¥ä¸å®Œæ•´å¥å­ç»“å°¾
        if not chunk.rstrip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '\n', '#')):
            if len(chunk) > 100:  # åªæ£€æŸ¥è¾ƒé•¿çš„å—
                issues.append(f"å—{i}: å¯èƒ½åœ¨å¥å­ä¸­é—´åˆ‡æ–­")
    
    return issues

issues = check_chunk_quality(chunks)

if issues:
    print("âš ï¸  å‘ç°ä»¥ä¸‹é—®é¢˜ï¼š")
    for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
        print(f"   {issue}")
    if len(issues) > 5:
        print(f"   ... è¿˜æœ‰ {len(issues)-5} ä¸ªé—®é¢˜")
else:
    print("âœ… åˆ‡å—è´¨é‡è‰¯å¥½ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")

print()

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
cleanup = input("æ˜¯å¦åˆ é™¤ç¤ºä¾‹æ–‡ä»¶ï¼Ÿ(y/n): ").strip().lower()
if cleanup == 'y':
    os.remove(doc_path)
    os.remove(output_file)
    print(f"âœ… å·²åˆ é™¤ {doc_path} å’Œ {output_file}")
else:
    print(f"ğŸ“ æ–‡ä»¶ä¿ç•™åœ¨å½“å‰ç›®å½•")

print()
print("="*70)
print("âœ… ç»ƒä¹ 3å®Œæˆï¼")
print()
print("ğŸ’¡ å…³é”®æ”¶è·ï¼š")
print("   â€¢ å­¦ä¼šè¯»å–å’Œå¤„ç†çœŸå®æ–‡æ¡£")
print("   â€¢ ä¸ºåˆ‡å—æ·»åŠ æœ‰ç”¨çš„å…ƒæ•°æ®")
print("   â€¢ æŒæ¡åˆ‡å—è´¨é‡åˆ†ææ–¹æ³•")
print("   â€¢ èƒ½å¤Ÿæœç´¢å’Œç®¡ç†åˆ‡å—ç»“æœ")
print()
print("ğŸ“ ä¸‹ä¸€æ­¥ï¼špython 04_chunk_optimization.py")
print("   å­¦ä¹ å¦‚ä½•ä¼˜åŒ–åˆ‡å—å‚æ•°ï¼")
print("="*70)

