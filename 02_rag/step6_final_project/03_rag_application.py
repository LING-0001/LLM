#!/usr/bin/env python3
"""
RAGæœ€ç»ˆé¡¹ç›® - å®Œæ•´RAGåº”ç”¨

è¿™æ˜¯ä¸€ä¸ªç”Ÿäº§çº§çš„RAGé—®ç­”ç³»ç»Ÿï¼Œæ•´åˆæ‰€æœ‰å­¦åˆ°çš„çŸ¥è¯†ï¼š
- æ–‡æ¡£ç®¡ç†
- é«˜çº§æ£€ç´¢
- Promptä¼˜åŒ–
- æµå¼è¾“å‡º
- æ€§èƒ½ç›‘æ§
- ç”¨æˆ·ä½“éªŒä¼˜åŒ–
"""

import os
import sys
import time
from llama_cpp import Llama
from typing import List, Dict, Any, Optional

# å¯¼å…¥å‰é¢å¼€å‘çš„æ¨¡å—
from pathlib import Path
import importlib.util

# åŠ¨æ€å¯¼å…¥åŒç›®å½•çš„æ¨¡å—
current_dir = Path(__file__).parent
retrieval_module_path = current_dir / "02_advanced_retrieval.py"
spec = importlib.util.spec_from_file_location("advanced_retrieval", retrieval_module_path)
advanced_retrieval = importlib.util.module_from_spec(spec)
spec.loader.exec_module(advanced_retrieval)
AdvancedRetriever = advanced_retrieval.AdvancedRetriever


class ProductionRAG:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""
    
    def __init__(self, 
                 model_path: str,
                 chroma_path: str = "./data/document_store",
                 collection_name: str = "documents"):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            model_path: LLMæ¨¡å‹è·¯å¾„
            chroma_path: ChromaDBè·¯å¾„
            collection_name: é›†åˆåç§°
        """
        print("=" * 60)
        print("ğŸš€ åˆå§‹åŒ–ç”Ÿäº§çº§RAGç³»ç»Ÿ")
        print("=" * 60)
        
        # 1. åŠ è½½LLM
        print("\nğŸ“¦ åŠ è½½è¯­è¨€æ¨¡å‹...")
        print(f"   æ¨¡å‹: {os.path.basename(model_path)}")
        self.llm = Llama(
            model_path=model_path,
            n_ctx=4096,
            n_gpu_layers=0,
            verbose=False
        )
        print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 2. åˆå§‹åŒ–æ£€ç´¢å™¨
        print("\nğŸ” åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ...")
        self.retriever = AdvancedRetriever(
            chroma_path=chroma_path,
            collection_name=collection_name
        )
        
        # 3. ç³»ç»Ÿé…ç½®
        self.config = {
            'retrieval_method': 'hybrid',  # vector, keyword, hybrid
            'n_results': 5,
            'use_rerank': True,
            'use_context_window': False,  # æš‚æ—¶å…³é—­ä¸Šä¸‹æ–‡çª—å£ï¼Œç”¨æ··åˆæ£€ç´¢
            'context_window_size': 1,
            'similarity_threshold': 0.3,  # é™ä½é˜ˆå€¼ï¼Œå› ä¸ºå‘é‡è·ç¦»å¯èƒ½æ˜¯è´Ÿæ•°
            'max_context_length': 2000,
            'llm_temperature': 0.3,
            'llm_max_tokens': 512
        }
        
        print("\nâš™ï¸  ç³»ç»Ÿé…ç½®:")
        for key, value in self.config.items():
            print(f"   {key}: {value}")
        
        print("\n" + "=" * 60)
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print("=" * 60 + "\n")
    
    def retrieve(self, query: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ£€ç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æ£€ç´¢ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        start_time = time.time()
        
        # 1. æ ¹æ®é…ç½®é€‰æ‹©æ£€ç´¢æ–¹æ³•
        method = self.config['retrieval_method']
        n_results = self.config['n_results']
        
        if self.config['use_context_window']:
            # å¸¦ä¸Šä¸‹æ–‡çª—å£çš„æ£€ç´¢
            results, _ = self.retriever.search_with_context(
                query,
                n_results=n_results,
                context_window=self.config['context_window_size']
            )
        elif method == 'vector':
            results, _ = self.retriever.vector_search(query, n_results=n_results * 2)
        elif method == 'keyword':
            results, _ = self.retriever.keyword_search(query, n_results=n_results * 2)
        else:  # hybrid
            results, _ = self.retriever.hybrid_search(query, n_results=n_results * 2)
        
        # 2. é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config['use_rerank'] and not self.config['use_context_window']:
            results, _ = self.retriever.rerank_results(query, results, top_k=n_results)
        
        # 3. è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        threshold = self.config['similarity_threshold']
        filtered_results = [r for r in results if r.get('similarity', 0) >= threshold]
        
        retrieval_time = time.time() - start_time
        
        return {
            'results': filtered_results,
            'total_found': len(filtered_results),
            'retrieval_time': retrieval_time,
            'method': method + ('+rerank' if self.config['use_rerank'] else '')
        }
    
    def build_prompt(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºä¼˜åŒ–çš„Prompt
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            å®Œæ•´çš„prompt
        """
        if not contexts:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·ç›´æ¥æ ¹æ®ä½ çš„çŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚

é—®é¢˜ï¼š{query}

è¯·ç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ï¼š"""
            return prompt
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = ""
        max_length = self.config['max_context_length']
        current_length = 0
        
        for i, ctx in enumerate(contexts, 1):
            # ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'full_context' in ctx:
                text = ctx['full_context']
            else:
                text = ctx['document']
            
            # æ§åˆ¶æ€»é•¿åº¦
            if current_length + len(text) > max_length:
                remaining = max_length - current_length
                if remaining > 100:  # è‡³å°‘ä¿ç•™100å­—ç¬¦
                    text = text[:remaining] + "..."
                else:
                    break
            
            doc_name = ctx['metadata'].get('doc_name', 'æœªçŸ¥')
            similarity = ctx.get('similarity', 0)
            
            context_text += f"\nå‚è€ƒèµ„æ–™ {i} (æ¥æº:{doc_name}, ç›¸å…³åº¦:{similarity:.0%}):\n{text}\n"
            current_length += len(text)
        
        # æ„å»ºå®Œæ•´prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context_text}

é—®é¢˜ï¼š{query}

å›ç­”è¦æ±‚ï¼š
1. ä¼˜å…ˆä½¿ç”¨å‚è€ƒèµ„æ–™ä¸­çš„ä¿¡æ¯
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸å¤Ÿå……åˆ†ï¼Œå¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†è¡¥å……
3. å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸šã€ç®€æ´
4. å¦‚æœå‚è€ƒèµ„æ–™ä¸é—®é¢˜å®Œå…¨æ— å…³ï¼Œè¯´æ˜æƒ…å†µåå†ç”¨ä½ çš„çŸ¥è¯†å›ç­”

è¯·å›ç­”ï¼š"""
        
        return prompt
    
    def generate(self, prompt: str, stream: bool = True):
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            prompt: å®Œæ•´çš„prompt
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Yields/Returns:
            æµå¼è¾“å‡ºæ–‡æœ¬å—æˆ–å®Œæ•´æ–‡æœ¬
        """
        start_time = time.time()
        
        response = self.llm(
            prompt,
            max_tokens=self.config['llm_max_tokens'],
            temperature=self.config['llm_temperature'],
            stop=["é—®é¢˜ï¼š", "\n\n\n"],
            stream=stream
        )
        
        if stream:
            # æµå¼è¾“å‡º
            full_text = ""
            for chunk in response:
                text = chunk['choices'][0]['text']
                full_text += text
                yield {
                    'text': text,
                    'full_text': full_text,
                    'done': False
                }
            
            generation_time = time.time() - start_time
            yield {
                'text': '',
                'full_text': full_text,
                'done': True,
                'generation_time': generation_time
            }
        else:
            # éæµå¼è¾“å‡º
            full_text = response['choices'][0]['text']
            generation_time = time.time() - start_time
            return {
                'text': full_text,
                'generation_time': generation_time
            }
    
    def answer(self, query: str, stream: bool = True, verbose: bool = True):
        """
        å›ç­”é—®é¢˜ï¼ˆå®Œæ•´æµç¨‹ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            stream: æ˜¯å¦æµå¼è¾“å‡º
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Yields/Returns:
            å›ç­”ç»“æœ
        """
        total_start = time.time()
        
        # 1. æ£€ç´¢
        if verbose:
            print(f"ğŸ” æ£€ç´¢ä¸­...", end='', flush=True)
        
        retrieval_result = self.retrieve(query)
        
        if verbose:
            print(f" å®Œæˆ ({retrieval_result['retrieval_time']*1000:.0f}ms)")
            print(f"   æ–¹æ³•: {retrieval_result['method']}")
            print(f"   æ‰¾åˆ°: {retrieval_result['total_found']} æ¡ç›¸å…³å†…å®¹")
            
            if retrieval_result['total_found'] > 0:
                print("\nğŸ“š æ£€ç´¢ç»“æœ:")
                for i, result in enumerate(retrieval_result['results'][:3], 1):
                    doc_name = result['metadata'].get('doc_name', 'æœªçŸ¥')
                    similarity = result.get('similarity', 0)
                    content = result['document'][:80].replace('\n', ' ')
                    print(f"   {i}. [{doc_name}] ({similarity:.0%}) {content}...")
            else:
                print("   âš ï¸  æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œå°†ä½¿ç”¨LLMç›´æ¥å›ç­”")
        
        # 2. æ„å»ºPrompt
        prompt = self.build_prompt(query, retrieval_result['results'])
        
        if verbose:
            print(f"\nğŸ“ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # 3. ç”Ÿæˆå›ç­”
        if verbose:
            print(f"\nğŸ’¬ AIå›ç­”:")
            print("-" * 60)
        
        if stream:
            # æµå¼è¾“å‡º
            for chunk in self.generate(prompt, stream=True):
                if not chunk['done']:
                    if verbose:
                        print(chunk['text'], end='', flush=True)
                    yield chunk
                else:
                    if verbose:
                        print()
                        print("-" * 60)
                    
                    total_time = time.time() - total_start
                    
                    result = {
                        'query': query,
                        'answer': chunk['full_text'],
                        'retrieval_time': retrieval_result['retrieval_time'],
                        'generation_time': chunk['generation_time'],
                        'total_time': total_time,
                        'num_contexts': retrieval_result['total_found'],
                        'method': retrieval_result['method']
                    }
                    
                    if verbose:
                        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
                        print(f"   æ£€ç´¢: {result['retrieval_time']*1000:.0f}ms")
                        print(f"   ç”Ÿæˆ: {result['generation_time']*1000:.0f}ms")
                        print(f"   æ€»è®¡: {result['total_time']*1000:.0f}ms")
                    
                    yield result
        else:
            # éæµå¼è¾“å‡º
            generation_result = self.generate(prompt, stream=False)
            total_time = time.time() - total_start
            
            result = {
                'query': query,
                'answer': generation_result['text'],
                'retrieval_time': retrieval_result['retrieval_time'],
                'generation_time': generation_result['generation_time'],
                'total_time': total_time,
                'num_contexts': retrieval_result['total_found'],
                'method': retrieval_result['method']
            }
            
            if verbose:
                print(result['answer'])
                print("-" * 60)
                print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
                print(f"   æ£€ç´¢: {result['retrieval_time']*1000:.0f}ms")
                print(f"   ç”Ÿæˆ: {result['generation_time']*1000:.0f}ms")
                print(f"   æ€»è®¡: {result['total_time']*1000:.0f}ms")
            
            return result
    
    def interactive_mode(self):
        """äº¤äº’å¼é—®ç­”æ¨¡å¼"""
        print("\n" + "=" * 60)
        print("ğŸ’¬ è¿›å…¥äº¤äº’å¼é—®ç­”æ¨¡å¼")
        print("=" * 60)
        print("\nå‘½ä»¤:")
        print("  - è¾“å…¥é—®é¢˜è¿›è¡Œæé—®")
        print("  - è¾“å…¥ 'config' æŸ¥çœ‹/ä¿®æ”¹é…ç½®")
        print("  - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("\n" + "=" * 60 + "\n")
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                query = input("ğŸ‘¤ ä½ : ").strip()
                
                if not query:
                    continue
                
                # å¤„ç†å‘½ä»¤
                if query.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ å†è§ï¼")
                    break
                
                if query.lower() == 'config':
                    self._show_config_menu()
                    continue
                
                # å›ç­”é—®é¢˜
                print()
                final_result = None
                for result in self.answer(query, stream=True, verbose=True):
                    if isinstance(result, dict) and 'total_time' in result:
                        final_result = result
                
                print()
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
    
    def _show_config_menu(self):
        """æ˜¾ç¤ºé…ç½®èœå•"""
        print("\nâš™ï¸  å½“å‰é…ç½®:")
        for i, (key, value) in enumerate(self.config.items(), 1):
            print(f"   {i}. {key}: {value}")
        print("\næç¤º: é…ç½®ä¿®æ”¹åŠŸèƒ½å¯ä»¥è¿›ä¸€æ­¥å¼€å‘")


def demo():
    """æ¼”ç¤ºå®Œæ•´RAGåº”ç”¨"""
    print("=" * 60)
    print("RAGæœ€ç»ˆé¡¹ç›® - å®Œæ•´RAGåº”ç”¨æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = os.path.expanduser("~/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf")
    if not os.path.exists(model_path):
        print(f"\nâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½")
        return
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = ProductionRAG(model_path=model_path)
    
    # æµ‹è¯•å‡ ä¸ªé—®é¢˜
    test_queries = [
        "é†‰é©¾ä¼šå—åˆ°ä»€ä¹ˆå¤„ç½šï¼Ÿ",
        "åŠ³åŠ¨è€…çš„å·¥ä½œæ—¶é—´æœ‰ä»€ä¹ˆè§„å®šï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"  # æ–‡æ¡£ä¸­æ²¡æœ‰çš„é—®é¢˜
    ]
    
    print("\nğŸ“‹ æ‰¹é‡æµ‹è¯•æ¨¡å¼")
    print("=" * 60)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i}/{len(test_queries)}")
        print(f"{'='*60}")
        print(f"\né—®é¢˜: {query}\n")
        
        # å›ç­”é—®é¢˜
        for result in rag.answer(query, stream=True, verbose=True):
            if isinstance(result, dict) and 'total_time' in result:
                pass  # æœ€ç»ˆç»“æœå·²ç»åœ¨answerä¸­æ‰“å°
        
        if i < len(test_queries):
            print("\n" + "="*60)
    
    # è¿›å…¥äº¤äº’æ¨¡å¼
    print("\n" + "=" * 60)
    choice = input("\næ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()
    if choice == 'y':
        rag.interactive_mode()
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ‰ æ­å–œï¼ä½ å·²ç»å®Œæˆäº†RAGå®Œæ•´å­¦ä¹ è·¯å¾„ï¼")
    print("\nğŸ’¡ ä½ ç°åœ¨æŒæ¡äº†:")
    print("   âœ… RAGå®Œæ•´åŸç†å’Œå®ç°")
    print("   âœ… æ–‡æ¡£ç®¡ç†å’Œå‘é‡å­˜å‚¨")
    print("   âœ… å¤šç§æ£€ç´¢ç­–ç•¥å’Œä¼˜åŒ–")
    print("   âœ… Promptå·¥ç¨‹å’Œç”Ÿæˆä¼˜åŒ–")
    print("   âœ… ç”Ÿäº§çº§ç³»ç»Ÿè®¾è®¡")
    print("\nğŸš€ ä¸‹ä¸€æ­¥:")
    print("   - Phase 3: Fine-tuning (æ¨¡å‹å¾®è°ƒ)")
    print("   - å°†RAGåº”ç”¨åˆ°å®é™…é¡¹ç›®")
    print("   - æ¢ç´¢æ›´å¤šä¼˜åŒ–æŠ€æœ¯")


if __name__ == "__main__":
    demo()

