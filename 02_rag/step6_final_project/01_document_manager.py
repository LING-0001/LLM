#!/usr/bin/env python3
"""
RAGæœ€ç»ˆé¡¹ç›® - æ–‡æ¡£ç®¡ç†ç³»ç»Ÿ

åŠŸèƒ½ï¼š
1. å¤šæ–‡æ¡£å¯¼å…¥å’Œç®¡ç†
2. å…ƒæ•°æ®ç®¡ç†ï¼ˆæ–‡æ¡£åã€ç±»å‹ã€æ—¥æœŸç­‰ï¼‰
3. æ–‡æ¡£æ›´æ–°å’Œåˆ é™¤
4. å‘é‡åº“ç»´æŠ¤

è¿™æ˜¯ç”Ÿäº§çº§RAGç³»ç»Ÿçš„åŸºç¡€ç»„ä»¶
"""

import os
import json
import chromadb
from datetime import datetime
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any


class DocumentManager:
    """æ–‡æ¡£ç®¡ç†å™¨ï¼šç®¡ç†å¤šä¸ªæ–‡æ¡£çš„å¯¼å…¥ã€å­˜å‚¨å’Œç»´æŠ¤"""
    
    def __init__(self, 
                 chroma_path: str = "./data/document_store",
                 collection_name: str = "documents"):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç®¡ç†å™¨
        
        Args:
            chroma_path: ChromaDBå­˜å‚¨è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        # åˆå§‹åŒ–å‘é‡æ¨¡å‹
        print("ğŸ“¦ åŠ è½½å‘é‡æ¨¡å‹...")
        self.embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')
        # é‡è¦ï¼šè¾“å‡ºå½’ä¸€åŒ–çš„å‘é‡
        self.embedding_model.encode_kwargs = {'normalize_embeddings': True}
        
        # åˆå§‹åŒ–ChromaDB
        print(f"ğŸ’¾ åˆå§‹åŒ–æ–‡æ¡£åº“: {chroma_path}")
        self.client = chromadb.PersistentClient(path=chroma_path)
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "å¤šæ–‡æ¡£RAGç³»ç»Ÿ"}
        )
        
        print(f"âœ… æ–‡æ¡£ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼å½“å‰æ–‡æ¡£æ•°ï¼š{self.collection.count()}\n")
    
    def add_document(self, 
                     content: str, 
                     doc_name: str,
                     doc_type: str = "text",
                     metadata: Dict[str, Any] = None,
                     chunk_size: int = 200,
                     chunk_overlap: int = 50) -> Dict[str, Any]:
        """
        æ·»åŠ æ–°æ–‡æ¡£åˆ°ç³»ç»Ÿ
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            doc_name: æ–‡æ¡£åç§°
            doc_type: æ–‡æ¡£ç±»å‹ï¼ˆtext, pdf, urlç­‰ï¼‰
            metadata: é¢å¤–çš„å…ƒæ•°æ®
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å 
            
        Returns:
            æ·»åŠ ç»“æœç»Ÿè®¡
        """
        print(f"\nğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {doc_name}")
        print(f"   æ–‡æ¡£ç±»å‹: {doc_type}")
        print(f"   æ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # 1. æ™ºèƒ½åˆ†å—
        chunks = self._smart_chunk(content, chunk_size, chunk_overlap)
        print(f"   âœ‚ï¸  åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
        
        # 2. ç”Ÿæˆå‘é‡ï¼ˆå½’ä¸€åŒ–ï¼‰
        print("   ğŸ”„ ç”Ÿæˆå‘é‡...")
        embeddings = self.embedding_model.encode(
            chunks,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        # 3. å‡†å¤‡å…ƒæ•°æ®
        timestamp = datetime.now().isoformat()
        base_metadata = {
            "doc_name": doc_name,
            "doc_type": doc_type,
            "import_time": timestamp,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„å…ƒæ•°æ®
        if metadata:
            base_metadata.update(metadata)
        
        # 4. ä¸ºæ¯ä¸ªå—å‡†å¤‡æ•°æ®
        ids = []
        metadatas = []
        for i in range(len(chunks)):
            chunk_id = f"{doc_name}_{timestamp}_{i}"
            ids.append(chunk_id)
            
            chunk_metadata = base_metadata.copy()
            chunk_metadata.update({
                "chunk_index": i,
                "chunk_total": len(chunks)
            })
            metadatas.append(chunk_metadata)
        
        # 5. æ·»åŠ åˆ°å‘é‡åº“
        self.collection.add(
            ids=ids,
            documents=chunks,
            embeddings=embeddings.tolist(),
            metadatas=metadatas
        )
        
        result = {
            "doc_name": doc_name,
            "chunks": len(chunks),
            "timestamp": timestamp,
            "total_docs": self.collection.count()
        }
        
        print(f"   âœ… æ–‡æ¡£å·²æ·»åŠ ï¼æ€»æ–‡æ¡£å—æ•°ï¼š{result['total_docs']}")
        return result
    
    def _smart_chunk(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """
        æ™ºèƒ½åˆ†å—ï¼šæŒ‰å¥å­è¾¹ç•Œåˆ†å—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            chunk_size: ç›®æ ‡å—å¤§å°
            overlap: é‡å å¤§å°
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        # æŒ‰å¥å­åˆ†å‰²
        sentences = []
        for sep in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n']:
            if sep in text:
                text = text.replace(sep, sep + '|||')
        
        raw_sentences = text.split('|||')
        sentences = [s.strip() for s in raw_sentences if s.strip()]
        
        # ç»„åˆæˆå—
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > chunk_size and current_chunk:
                # ä¿å­˜å½“å‰å—
                chunks.append(''.join(current_chunk))
                
                # è®¡ç®—é‡å éƒ¨åˆ†
                overlap_text = []
                overlap_length = 0
                for s in reversed(current_chunk):
                    if overlap_length + len(s) <= overlap:
                        overlap_text.insert(0, s)
                        overlap_length += len(s)
                    else:
                        break
                
                current_chunk = overlap_text
                current_length = overlap_length
            
            current_chunk.append(sentence)
            current_length += sentence_length
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(''.join(current_chunk))
        
        return chunks
    
    def list_documents(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
        
        Returns:
            æ–‡æ¡£åˆ—è¡¨ï¼ˆå»é‡åçš„æ–‡æ¡£å…ƒæ•°æ®ï¼‰
        """
        # è·å–æ‰€æœ‰æ•°æ®
        results = self.collection.get()
        
        if not results['metadatas']:
            return []
        
        # æŒ‰æ–‡æ¡£ååˆ†ç»„
        docs_dict = {}
        for metadata in results['metadatas']:
            doc_name = metadata.get('doc_name', 'unknown')
            if doc_name not in docs_dict:
                docs_dict[doc_name] = {
                    'doc_name': doc_name,
                    'doc_type': metadata.get('doc_type', 'unknown'),
                    'import_time': metadata.get('import_time', 'unknown'),
                    'chunks': 0
                }
            docs_dict[doc_name]['chunks'] += 1
        
        return list(docs_dict.values())
    
    def delete_document(self, doc_name: str) -> Dict[str, Any]:
        """
        åˆ é™¤æŒ‡å®šæ–‡æ¡£
        
        Args:
            doc_name: æ–‡æ¡£åç§°
            
        Returns:
            åˆ é™¤ç»“æœ
        """
        print(f"\nğŸ—‘ï¸  åˆ é™¤æ–‡æ¡£: {doc_name}")
        
        # æŸ¥è¯¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰å—
        results = self.collection.get(
            where={"doc_name": doc_name}
        )
        
        if not results['ids']:
            print(f"   âš ï¸  æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")
            return {"success": False, "message": "æ–‡æ¡£ä¸å­˜åœ¨"}
        
        # åˆ é™¤æ‰€æœ‰å—
        self.collection.delete(ids=results['ids'])
        
        print(f"   âœ… å·²åˆ é™¤ {len(results['ids'])} ä¸ªæ–‡æ¡£å—")
        return {
            "success": True,
            "doc_name": doc_name,
            "deleted_chunks": len(results['ids']),
            "remaining_total": self.collection.count()
        }
    
    def search_documents(self, 
                        query: str, 
                        n_results: int = 5,
                        doc_name: str = None) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°
            doc_name: é™å®šæ–‡æ¡£åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode(
            query,
            convert_to_numpy=True
        )
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where = {"doc_name": doc_name} if doc_name else None
        
        # æ‰§è¡Œæœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=n_results,
            where=where
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'id': results['ids'][0][i],
                'document': results['documents'][0][i],
                'distance': results['distances'][0][i],
                'metadata': results['metadatas'][0][i]
            })
        
        return formatted_results
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        docs = self.list_documents()
        
        return {
            "total_chunks": self.collection.count(),
            "total_documents": len(docs),
            "documents": docs
        }


def demo():
    """æ¼”ç¤ºæ–‡æ¡£ç®¡ç†ç³»ç»Ÿçš„åŠŸèƒ½"""
    print("=" * 60)
    print("RAGæœ€ç»ˆé¡¹ç›® - æ–‡æ¡£ç®¡ç†ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–ç®¡ç†å™¨
    manager = DocumentManager()
    
    # 2. æ·»åŠ ç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼ˆäº¤é€šæ³•ï¼‰
    traffic_law = """
    é“è·¯äº¤é€šå®‰å…¨æ³•è§„å®šï¼š
    
    ç¬¬ä¸€ç«  æ€»åˆ™
    æœºåŠ¨è½¦é©¾é©¶äººåº”å½“éµå®ˆé“è·¯äº¤é€šå®‰å…¨æ³•å¾‹æ³•è§„ï¼ŒæŒ‰ç…§æ“ä½œè§„èŒƒå®‰å…¨é©¾é©¶ã€æ–‡æ˜é©¾é©¶ã€‚
    é¥®é…’ã€æœç”¨å›½å®¶ç®¡åˆ¶çš„ç²¾ç¥è¯å“æˆ–è€…éº»é†‰è¯å“ï¼Œä¸å¾—é©¾é©¶æœºåŠ¨è½¦ã€‚
    
    ç¬¬äºŒç«  è¿æ³•å¤„ç½š
    é†‰é…’é©¾é©¶æœºåŠ¨è½¦çš„ï¼Œç”±å…¬å®‰æœºå…³äº¤é€šç®¡ç†éƒ¨é—¨çº¦æŸè‡³é…’é†’ï¼Œ
    åŠé”€æœºåŠ¨è½¦é©¾é©¶è¯ï¼Œä¾æ³•è¿½ç©¶åˆ‘äº‹è´£ä»»ï¼›äº”å¹´å†…ä¸å¾—é‡æ–°å–å¾—æœºåŠ¨è½¦é©¾é©¶è¯ã€‚
    é¥®é…’åé©¾é©¶è¥è¿æœºåŠ¨è½¦çš„ï¼Œå¤„åäº”æ—¥æ‹˜ç•™ï¼Œå¹¶å¤„äº”åƒå…ƒç½šæ¬¾ï¼Œ
    åŠé”€æœºåŠ¨è½¦é©¾é©¶è¯ï¼Œäº”å¹´å†…ä¸å¾—é‡æ–°å–å¾—æœºåŠ¨è½¦é©¾é©¶è¯ã€‚
    
    ç¬¬ä¸‰ç«  ç‰¹æ®Šè§„å®š
    è¶…é€Ÿé©¾é©¶æŒ‰ç…§è¶…é€Ÿæ¯”ä¾‹è¿›è¡Œå¤„ç½šã€‚è¶…é€Ÿ50%ä»¥ä¸Šçš„ï¼Œå¤„ä»¥ç½šæ¬¾å¹¶æ‰£12åˆ†ã€‚
    é—¯çº¢ç¯çš„ï¼Œä¸€æ¬¡è®°6åˆ†ï¼Œç½šæ¬¾200å…ƒã€‚
    """
    
    manager.add_document(
        content=traffic_law,
        doc_name="äº¤é€šæ³•",
        doc_type="æ³•å¾‹æ–‡æœ¬",
        metadata={"category": "æ³•å¾‹", "version": "2023"}
    )
    
    # 3. æ·»åŠ ç¬¬äºŒä¸ªæ–‡æ¡£ï¼ˆåŠ³åŠ¨æ³•ï¼‰
    labor_law = """
    åŠ³åŠ¨åˆåŒæ³•è§„å®šï¼š
    
    ç¬¬ä¸€ç«  åŠ³åŠ¨åˆåŒè®¢ç«‹
    å»ºç«‹åŠ³åŠ¨å…³ç³»ï¼Œåº”å½“è®¢ç«‹ä¹¦é¢åŠ³åŠ¨åˆåŒã€‚å·²å»ºç«‹åŠ³åŠ¨å…³ç³»ï¼ŒæœªåŒæ—¶è®¢ç«‹ä¹¦é¢åŠ³åŠ¨åˆåŒçš„ï¼Œ
    åº”å½“è‡ªç”¨å·¥ä¹‹æ—¥èµ·ä¸€ä¸ªæœˆå†…è®¢ç«‹ä¹¦é¢åŠ³åŠ¨åˆåŒã€‚
    
    ç¬¬äºŒç«  å·¥ä½œæ—¶é—´ä¸ä¼‘æ¯ä¼‘å‡
    å›½å®¶å®è¡ŒåŠ³åŠ¨è€…æ¯æ—¥å·¥ä½œæ—¶é—´ä¸è¶…è¿‡å…«å°æ—¶ã€å¹³å‡æ¯å‘¨å·¥ä½œæ—¶é—´ä¸è¶…è¿‡å››åå››å°æ—¶çš„å·¥æ—¶åˆ¶åº¦ã€‚
    ç”¨äººå•ä½åº”å½“ä¿è¯åŠ³åŠ¨è€…æ¯å‘¨è‡³å°‘ä¼‘æ¯ä¸€æ—¥ã€‚
    åŠ³åŠ¨è€…è¿ç»­å·¥ä½œä¸€å¹´ä»¥ä¸Šçš„ï¼Œäº«å—å¸¦è–ªå¹´ä¼‘å‡ã€‚
    
    ç¬¬ä¸‰ç«  å·¥èµ„æ”¯ä»˜
    å·¥èµ„åº”å½“ä»¥è´§å¸å½¢å¼æŒ‰æœˆæ”¯ä»˜ç»™åŠ³åŠ¨è€…æœ¬äººã€‚ä¸å¾—å…‹æ‰£æˆ–è€…æ— æ•…æ‹–æ¬ åŠ³åŠ¨è€…çš„å·¥èµ„ã€‚
    ç”¨äººå•ä½å®‰æ’åŠ ç­çš„ï¼Œåº”å½“æŒ‰ç…§è§„å®šæ”¯ä»˜åŠ ç­è´¹ã€‚
    """
    
    manager.add_document(
        content=labor_law,
        doc_name="åŠ³åŠ¨æ³•",
        doc_type="æ³•å¾‹æ–‡æœ¬",
        metadata={"category": "æ³•å¾‹", "version": "2023"}
    )
    
    # 4. åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
    print("\n" + "=" * 60)
    print("ğŸ“š æ–‡æ¡£åº“æ¸…å•")
    print("=" * 60)
    docs = manager.list_documents()
    for doc in docs:
        print(f"\nğŸ“„ {doc['doc_name']}")
        print(f"   ç±»å‹: {doc['doc_type']}")
        print(f"   å¯¼å…¥æ—¶é—´: {doc['import_time']}")
        print(f"   æ–‡æ¡£å—æ•°: {doc['chunks']}")
    
    # 5. è·¨æ–‡æ¡£æœç´¢
    print("\n" + "=" * 60)
    print("ğŸ” è·¨æ–‡æ¡£æœç´¢æµ‹è¯•")
    print("=" * 60)
    
    queries = [
        "å·¥ä½œæ—¶é—´æœ‰ä»€ä¹ˆè§„å®š",
        "é†‰é©¾çš„å¤„ç½š",
        "å¹´å‡æ€ä¹ˆç®—"
    ]
    
    for query in queries:
        print(f"\né—®é¢˜: {query}")
        print("-" * 50)
        results = manager.search_documents(query, n_results=3)
        for i, result in enumerate(results, 1):
            similarity = 1 - result['distance']
            print(f"\nç»“æœ {i} (ç›¸ä¼¼åº¦: {similarity:.1%})")
            print(f"æ¥æº: {result['metadata']['doc_name']}")
            print(f"å†…å®¹: {result['document'][:100]}...")
    
    # 6. å•æ–‡æ¡£æœç´¢
    print("\n" + "=" * 60)
    print("ğŸ¯ å•æ–‡æ¡£æœç´¢æµ‹è¯•")
    print("=" * 60)
    
    query = "å¤„ç½š"
    print(f"\nåœ¨ã€Œäº¤é€šæ³•ã€ä¸­æœç´¢: {query}")
    print("-" * 50)
    results = manager.search_documents(query, n_results=3, doc_name="äº¤é€šæ³•")
    for i, result in enumerate(results, 1):
        similarity = 1 - result['distance']
        print(f"\nç»“æœ {i} (ç›¸ä¼¼åº¦: {similarity:.1%})")
        print(f"å†…å®¹: {result['document'][:150]}...")
    
    # 7. ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“Š ç³»ç»Ÿç»Ÿè®¡")
    print("=" * 60)
    stats = manager.get_stats()
    print(f"\næ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"æ€»æ–‡æ¡£å—æ•°: {stats['total_chunks']}")
    print(f"å¹³å‡æ¯æ–‡æ¡£å—æ•°: {stats['total_chunks'] / stats['total_documents']:.1f}")
    
    # 8. åˆ é™¤æ–‡æ¡£æµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ—‘ï¸  åˆ é™¤æ–‡æ¡£æµ‹è¯•")
    print("=" * 60)
    
    # åˆ é™¤åŠ³åŠ¨æ³•
    result = manager.delete_document("åŠ³åŠ¨æ³•")
    print(f"\nå‰©ä½™æ€»å—æ•°: {result['remaining_total']}")
    
    # å†æ¬¡åˆ—å‡ºæ–‡æ¡£
    print("\nå½“å‰æ–‡æ¡£:")
    docs = manager.list_documents()
    for doc in docs:
        print(f"  - {doc['doc_name']} ({doc['chunks']} å—)")
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ’¡ å­¦åˆ°çš„çŸ¥è¯†:")
    print("   1. å¤šæ–‡æ¡£ç®¡ç†ï¼šå¯ä»¥å¯¼å…¥å¤šä¸ªæ–‡æ¡£å¹¶ç‹¬ç«‹ç®¡ç†")
    print("   2. å…ƒæ•°æ®ç®¡ç†ï¼šæ¯ä¸ªæ–‡æ¡£éƒ½æœ‰ä¸°å¯Œçš„å…ƒæ•°æ®")
    print("   3. è·¨æ–‡æ¡£æœç´¢ï¼šå¯ä»¥åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æœç´¢")
    print("   4. å•æ–‡æ¡£æœç´¢ï¼šå¯ä»¥é™å®šåœ¨ç‰¹å®šæ–‡æ¡£ä¸­æœç´¢")
    print("   5. æ–‡æ¡£ç»´æŠ¤ï¼šæ”¯æŒåˆ é™¤å’Œæ›´æ–°")
    print("\nä¸‹ä¸€æ­¥ï¼šå­¦ä¹ é«˜çº§æ£€ç´¢ç­–ç•¥ (02_advanced_retrieval.py)")


if __name__ == "__main__":
    demo()

