# Step 3: 向量化（Embedding）

> 把文本转换成数字向量，让计算机能"理解"语义相似度

---

## 🎯 本节目标

- 理解什么是向量（Vector）和向量化（Embedding）
- 学会计算文本相似度
- 掌握中文Embedding模型的使用
- 为向量数据库做好准备

---

## 🤔 什么是向量化？

### 一个类比

想象你要向盲人描述颜色：

**方法1：用词语**
- "红色" - 但盲人不知道红色是什么

**方法2：用数字**
- 红色 = [255, 0, 0]（RGB）
- 蓝色 = [0, 0, 255]
- 紫色 = [128, 0, 128]

现在可以计算：紫色 = (红色 + 蓝色) / 2 ✅

**向量化就是把文本转成数字，让计算机能"计算"语义！**

---

## 📊 文本向量化示例

```
"Python很好用"     →  [0.2, 0.8, 0.1, 0.9, ...]  (768维向量)
"Python非常实用"   →  [0.3, 0.7, 0.2, 0.8, ...]
"Java很强大"       →  [0.1, 0.3, 0.8, 0.2, ...]
```

**观察：**
- 前两句意思相近，向量也相近
- 第三句意思不同，向量差别大

**计算相似度：**
```
相似度("Python很好用", "Python非常实用") = 0.92  ← 很相似！
相似度("Python很好用", "Java很强大")     = 0.45  ← 不太相似
```

---

## 🔍 向量相似度的计算

### 余弦相似度（最常用）

```python
向量A = [0.2, 0.8, 0.1]
向量B = [0.3, 0.7, 0.2]

余弦相似度 = (A·B) / (|A| × |B|)
         = 0.92  ← 范围：-1到1，越接近1越相似
```

**几何意义：**
```
      向量A
        ↗
       /  θ (角度小 → 相似度高)
      /
     /
    └─────→ 向量B
```

---

## 🧠 Embedding模型

### 什么是Embedding模型？

一个**神经网络**，专门把文本转成向量：

```
输入："Python是一种编程语言"
  ↓
[Embedding模型]
  ↓
输出：[0.23, 0.15, -0.08, 0.42, ..., 0.19]  (768维)
```

### 常用的中文Embedding模型

| 模型名称 | 维度 | 大小 | 特点 |
|---------|------|------|------|
| text2vec-base-chinese | 768 | 400MB | 通用中文，平衡 |
| bge-small-zh | 512 | 95MB | 小巧快速 |
| bge-large-zh | 1024 | 1.3GB | 效果最好 |
| m3e-base | 768 | 400MB | 中英双语 |

**本教程使用：`text2vec-base-chinese`**（平衡性能和效果）

---

## 💡 为什么需要向量化？

### 场景对比

**问题：** 用户问"如何学Python？"

**文档块：**
1. "Python学习指南"
2. "Java开发教程"
3. "Python入门方法"

---

**方法1：关键词匹配**
```python
if "Python" in 块1: 匹配！
if "Python" in 块2: 不匹配
if "Python" in 块3: 匹配！
```
❌ 问题：只看关键词，不理解语义

---

**方法2：向量相似度**
```python
问题向量 = [0.2, 0.8, 0.1, ...]
块1向量  = [0.3, 0.7, 0.2, ...]  相似度=0.95 ✅
块2向量  = [0.1, 0.3, 0.8, ...]  相似度=0.32
块3向量  = [0.25, 0.75, 0.15, ...] 相似度=0.93 ✅
```
✅ 理解语义，找到真正相关的内容！

---

## 🛠️ 实践练习

### 练习1：理解向量基础
```bash
python 01_vector_basics.py
```

**内容：**
- 什么是向量
- 向量的加减乘除
- 计算余弦相似度
- 可视化向量关系

---

### 练习2：使用Embedding模型
```bash
python 02_embedding_model.py
```

**内容：**
- 加载中文Embedding模型
- 文本 → 向量转换
- 查看向量的维度和值
- 理解向量的含义

---

### 练习3：计算文本相似度
```bash
python 03_text_similarity.py
```

**内容：**
- 计算两段文本的相似度
- 找到最相似的文本
- 实战：问答匹配
- 观察语义理解能力

---

### 练习4：批量向量化
```bash
python 04_batch_embedding.py
```

**内容：**
- 批量处理多个文本
- 优化向量化速度
- 保存和加载向量
- 为向量数据库准备数据

---

## 📐 向量的关键概念

### 1. 维度（Dimension）

```
2维向量：[0.5, 0.8]           ← 可以画在平面上
3维向量：[0.5, 0.8, 0.3]      ← 可以画在空间中
768维向量：[0.5, 0.8, ..., 0.3] ← 无法直观可视化，但数学上一样！
```

**为什么用768维？**
- 维度越高，能表达的语义越丰富
- 但维度太高，计算成本也高
- 768是个平衡点（BERT的标准维度）

---

### 2. 向量的单位化（Normalization）

```python
原向量：[3, 4, 0]
长度：√(3² + 4²) = 5

单位化后：[3/5, 4/5, 0] = [0.6, 0.8, 0]
长度：√(0.6² + 0.8²) = 1  ← 长度变为1
```

**为什么要单位化？**
- 方便计算相似度
- 消除文本长度的影响
- 大多数Embedding模型会自动单位化

---

### 3. 相似度的范围

**余弦相似度：**
```
 1.0  ← 完全相同
 0.9  ← 非常相似
 0.7  ← 相关
 0.5  ← 有些关系
 0.3  ← 关系不大
 0.0  ← 完全无关
-1.0  ← 完全相反（很少见）
```

**RAG中的阈值：**
- 通常取相似度 > 0.5 或 0.6 的块
- Top-K：取相似度最高的3-5个块

---

## 🎨 向量的神奇性质

### 语义运算

```python
向量("国王") - 向量("男人") + 向量("女人") ≈ 向量("女王")

向量("北京") - 向量("中国") + 向量("日本") ≈ 向量("东京")
```

这就是为什么向量能"理解"语义！

---

### 多语言对齐

好的Embedding模型能做到：

```python
相似度("Hello", "你好") = 0.85  ← 不同语言，但意思相同
相似度("Dog", "狗")    = 0.88
```

---

## 💾 向量的存储

### 原始文本 vs 向量

```
原始：
"Python是一种编程语言，广泛应用于数据科学。"  (24字)

向量：
[0.231, 0.152, -0.083, ..., 0.194]  (768个浮点数)
= 768 × 4字节 = 3KB

100万个文档块 = 100万 × 3KB = 3GB
```

**向量数据库专门优化了向量的存储和检索！**

---

## 🔧 Embedding模型的选择

### 根据需求选择

**速度优先（实时查询）：**
- `bge-small-zh` (512维, 95MB)
- 查询速度快，适合在线服务

**效果优先（离线处理）：**
- `bge-large-zh` (1024维, 1.3GB)
- 检索准确率更高

**平衡选择（推荐）：**
- `text2vec-base-chinese` (768维, 400MB)
- 性能和效果的平衡点

---

## ⚡ 优化技巧

### 1. 批量处理

```python
# ❌ 慢：一次一个
for text in texts:
    vector = model.encode(text)

# ✅ 快：批量处理
vectors = model.encode(texts, batch_size=32)
```

**提速：10-20倍！**

---

### 2. 缓存向量

```python
# 第一次：计算并保存
vectors = model.encode(texts)
np.save('vectors.npy', vectors)

# 之后：直接加载
vectors = np.load('vectors.npy')
```

**避免重复计算！**

---

### 3. GPU加速

```python
# CPU模式（慢）
model = SentenceTransformer('model_name')

# GPU模式（快5-10倍）
model = SentenceTransformer('model_name', device='cuda')
```

---

## 🎓 核心要点总结

### Embedding的作用

1. **把文本转成数字** - 让计算机能处理
2. **保留语义信息** - 意思相近的文本，向量也相近
3. **支持相似度计算** - 找到最相关的文档块

---

### RAG中的Embedding流程

```
准备阶段（一次性）：
文档块 → Embedding模型 → 向量 → 存入向量数据库

查询阶段（每次）：
用户问题 → Embedding模型 → 向量 → 在数据库中搜索 → 找到相似块
```

---

### 关键参数

- **模型选择：** text2vec-base-chinese（推荐）
- **维度：** 768
- **相似度阈值：** 0.5-0.6
- **Top-K：** 3-5个最相关块

---

## ✅ 完成标志

掌握了以下内容，即可进入Step 4：

- [ ] 理解什么是向量和向量化
- [ ] 会使用Embedding模型
- [ ] 能计算文本相似度
- [ ] 理解向量在RAG中的作用
- [ ] 运行了所有4个练习

---

## ❓ 思考题

1. **为什么向量能表示语义？**
   - 提示：神经网络训练过程

2. **768维向量如何压缩信息？**
   - 提示：降维与信息损失

3. **如何评估Embedding模型的好坏？**
   - 提示：检索准确率

---

## 📍 下一步

**Step 4: 向量数据库（Vector Store）**

```bash
cd ../step4_vectorstore
cat README.md
```

学习如何高效存储和检索百万级别的向量。

---

**开始实践吧！** 🚀

```bash
cd ~/code/MyLLM/02_rag/step3_embedding
python 01_vector_basics.py
```

