"""
ç»ƒä¹ 4ï¼šæ‰¹é‡å‘é‡åŒ–å’Œä¼˜åŒ–
å­¦ä¹ å¦‚ä½•é«˜æ•ˆå¤„ç†å¤§é‡æ–‡æœ¬ï¼Œä¸ºå‘é‡æ•°æ®åº“åšå‡†å¤‡
"""

from sentence_transformers import SentenceTransformer
import numpy as np
import time
import json

print("="*70)
print(" "*20 + "æ‰¹é‡å‘é‡åŒ–å’Œä¼˜åŒ–")
print("="*70)
print()

# åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = SentenceTransformer('shibing624/text2vec-base-chinese')
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

# 1. ç”Ÿæˆæµ‹è¯•æ•°æ®
print("="*70)
print("1. ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®")
print("="*70)
print()

# æ¨¡æ‹Ÿä»Step 2åˆ‡å—åå¾—åˆ°çš„æ–‡æ¡£å—
document_chunks = []
for i in range(100):
    chunk = {
        "id": f"chunk_{i:03d}",
        "content": f"è¿™æ˜¯ç¬¬{i}ä¸ªæ–‡æ¡£å—ï¼ŒåŒ…å«å…³äºPythonã€æœºå™¨å­¦ä¹ ã€æ•°æ®ç§‘å­¦çš„å†…å®¹ã€‚" * 3,
        "source": f"document_{i//10}.txt",
        "chunk_index": i % 10
    }
    document_chunks.append(chunk)

print(f"âœ… ç”Ÿæˆäº† {len(document_chunks)} ä¸ªæ–‡æ¡£å—")
print()
print("ç¤ºä¾‹å—ï¼š")
print(json.dumps(document_chunks[0], ensure_ascii=False, indent=2))
print()

# 2. å•ä¸ª vs æ‰¹é‡ç¼–ç 
print("="*70)
print("2. æ€§èƒ½å¯¹æ¯”ï¼šå•ä¸ª vs æ‰¹é‡ç¼–ç ")
print("="*70)
print()

# å‡†å¤‡æ•°æ®
test_texts = [chunk["content"] for chunk in document_chunks[:50]]

# æ–¹æ³•1ï¼šé€ä¸ªç¼–ç 
print("æ–¹æ³•1ï¼šé€ä¸ªç¼–ç ï¼ˆä¸æ¨èï¼‰")
start = time.time()
vectors_individual = []
for text in test_texts:
    vec = model.encode(text)
    vectors_individual.append(vec)
time_individual = time.time() - start
print(f"  è€—æ—¶ï¼š{time_individual:.2f}ç§’")
print()

# æ–¹æ³•2ï¼šæ‰¹é‡ç¼–ç 
print("æ–¹æ³•2ï¼šæ‰¹é‡ç¼–ç ï¼ˆæ¨èï¼‰")
start = time.time()
vectors_batch = model.encode(test_texts, batch_size=32)
time_batch = time.time() - start
print(f"  è€—æ—¶ï¼š{time_batch:.2f}ç§’")
print()

print(f"ğŸ’¡ æ‰¹é‡ç¼–ç å¿« {time_individual/time_batch:.1f}å€ï¼")
print()

# 3. æµ‹è¯•ä¸åŒçš„batch_size
print("="*70)
print("3. ä¼˜åŒ–batch_size")
print("="*70)
print()

batch_sizes = [8, 16, 32, 64]
print("æµ‹è¯•ä¸åŒçš„batch_sizeï¼š")
print()

for bs in batch_sizes:
    start = time.time()
    vectors = model.encode(test_texts, batch_size=bs)
    elapsed = time.time() - start
    
    throughput = len(test_texts) / elapsed
    print(f"  batch_size={bs:2d}: {elapsed:.2f}ç§’, {throughput:.1f} texts/sec")

print()
print("ğŸ’¡ å»ºè®®ï¼š")
print("   â€¢ CPUï¼šbatch_size=16-32")
print("   â€¢ GPUï¼šbatch_size=64-128")
print("   â€¢ å†…å­˜å……è¶³å¯ä»¥æ›´å¤§")
print()

# 4. æ˜¾ç¤ºè¿›åº¦æ¡
print("="*70)
print("4. å¤„ç†å¤§é‡æ•°æ®æ—¶æ˜¾ç¤ºè¿›åº¦")
print("="*70)
print()

from tqdm import tqdm

all_texts = [chunk["content"] for chunk in document_chunks]

print("ç¼–ç æ‰€æœ‰æ–‡æ¡£å—ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰ï¼š")
vectors = model.encode(
    all_texts, 
    batch_size=32,
    show_progress_bar=True
)

print(f"âœ… å®Œæˆï¼ç”Ÿæˆäº† {len(vectors)} ä¸ªå‘é‡")
print(f"   å‘é‡å½¢çŠ¶ï¼š{vectors.shape}")
print()

# 5. ä¿å­˜å‘é‡å’Œå…ƒæ•°æ®
print("="*70)
print("5. ä¿å­˜å‘é‡å’Œå…ƒæ•°æ®")
print("="*70)
print()

# æ–¹å¼1ï¼šåªä¿å­˜å‘é‡
vectors_file = "document_vectors.npy"
np.save(vectors_file, vectors)
print(f"âœ… å‘é‡å·²ä¿å­˜ï¼š{vectors_file}")

# æ–¹å¼2ï¼šä¿å­˜å‘é‡+å…ƒæ•°æ®
data_package = {
    "vectors": vectors.tolist(),  # è½¬æˆlistæ‰èƒ½JSONåºåˆ—åŒ–
    "metadata": document_chunks,
    "model_name": "shibing624/text2vec-base-chinese",
    "vector_dim": vectors.shape[1],
    "count": len(vectors)
}

package_file = "vectors_with_metadata.json"
with open(package_file, 'w', encoding='utf-8') as f:
    json.dump(data_package, f, ensure_ascii=False, indent=2)
print(f"âœ… å‘é‡+å…ƒæ•°æ®å·²ä¿å­˜ï¼š{package_file}")
print()

# 6. åŠ è½½å’ŒéªŒè¯
print("="*70)
print("6. åŠ è½½å’ŒéªŒè¯")
print("="*70)
print()

# åŠ è½½å‘é‡
loaded_vectors = np.load(vectors_file)
print(f"âœ… ä» {vectors_file} åŠ è½½äº† {len(loaded_vectors)} ä¸ªå‘é‡")

# åŠ è½½å®Œæ•´æ•°æ®
with open(package_file, 'r', encoding='utf-8') as f:
    loaded_package = json.load(f)

print(f"âœ… ä» {package_file} åŠ è½½äº†å®Œæ•´æ•°æ®")
print(f"   å‘é‡æ•°é‡ï¼š{loaded_package['count']}")
print(f"   å‘é‡ç»´åº¦ï¼š{loaded_package['vector_dim']}")
print(f"   ä½¿ç”¨æ¨¡å‹ï¼š{loaded_package['model_name']}")
print()

# éªŒè¯
reconstructed_vectors = np.array(loaded_package["vectors"])
print("éªŒè¯å‘é‡ä¸€è‡´æ€§...")
if np.allclose(vectors, reconstructed_vectors):
    print("  âœ… å‘é‡å®Œå…¨ä¸€è‡´")
else:
    print("  âŒ å‘é‡ä¸ä¸€è‡´")
print()

# 7. å¢é‡æ›´æ–°
print("="*70)
print("7. å¢é‡æ›´æ–°å‘é‡åº“")
print("="*70)
print()

print("åœºæ™¯ï¼šæ–°å¢3ä¸ªæ–‡æ¡£å—")

# æ–°æ–‡æ¡£
new_chunks = [
    {"id": "chunk_100", "content": "æ–°å¢çš„ç¬¬1ä¸ªæ–‡æ¡£å—"},
    {"id": "chunk_101", "content": "æ–°å¢çš„ç¬¬2ä¸ªæ–‡æ¡£å—"},
    {"id": "chunk_102", "content": "æ–°å¢çš„ç¬¬3ä¸ªæ–‡æ¡£å—"},
]

# ç”Ÿæˆæ–°å‘é‡
new_texts = [c["content"] for c in new_chunks]
new_vectors = model.encode(new_texts)

# åˆå¹¶
all_vectors = np.vstack([vectors, new_vectors])
all_chunks = document_chunks + new_chunks

print(f"âœ… æ›´æ–°å®Œæˆ")
print(f"   åŸå‘é‡æ•°ï¼š{len(vectors)}")
print(f"   æ–°å‘é‡æ•°ï¼š{len(new_vectors)}")
print(f"   æ€»å‘é‡æ•°ï¼š{len(all_vectors)}")
print()

# 8. å†…å­˜å’Œå­˜å‚¨åˆ†æ
print("="*70)
print("8. å†…å­˜å’Œå­˜å‚¨åˆ†æ")
print("="*70)
print()

import os

# è®¡ç®—å†…å­˜å ç”¨
vector_memory = all_vectors.nbytes / 1024 / 1024  # MB

print(f"å‘é‡å†…å­˜å ç”¨ï¼š")
print(f"  â€¢ å‘é‡æ•°é‡ï¼š{len(all_vectors)}")
print(f"  â€¢ æ¯ä¸ªå‘é‡ï¼š{all_vectors.shape[1]} ç»´")
print(f"  â€¢ æ•°æ®ç±»å‹ï¼š{all_vectors.dtype}")
print(f"  â€¢ æ€»å†…å­˜ï¼š{vector_memory:.2f} MB")
print()

# æ–‡ä»¶å¤§å°
file_size = os.path.getsize(vectors_file) / 1024 / 1024
json_size = os.path.getsize(package_file) / 1024 / 1024

print(f"æ–‡ä»¶å¤§å°ï¼š")
print(f"  â€¢ {vectors_file}: {file_size:.2f} MB")
print(f"  â€¢ {package_file}: {json_size:.2f} MB")
print()

# é¢„ä¼°
print("ğŸ’¡ é¢„ä¼°ï¼ˆ100ä¸‡æ–‡æ¡£å—ï¼‰ï¼š")
scale_factor = 1000000 / len(all_vectors)
estimated_memory = vector_memory * scale_factor
print(f"  â€¢ å†…å­˜éœ€æ±‚ï¼šçº¦ {estimated_memory:.0f} MB ({estimated_memory/1024:.1f} GB)")
print(f"  â€¢ æ¨èä½¿ç”¨å‘é‡æ•°æ®åº“ï¼ˆChromaDBï¼‰ä¼˜åŒ–å­˜å‚¨å’Œæ£€ç´¢")
print()

# 9. å®æˆ˜ï¼šç®€å•çš„å‘é‡æ£€ç´¢
print("="*70)
print("9. å®æˆ˜ï¼šå‘é‡æ£€ç´¢")
print("="*70)
print()

from sklearn.metrics.pairwise import cosine_similarity

def vector_search(query, vectors, chunks, top_k=3):
    """ç®€å•çš„å‘é‡æ£€ç´¢"""
    # é—®é¢˜å‘é‡åŒ–
    query_vec = model.encode(query)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = cosine_similarity([query_vec], vectors)[0]
    
    # æ’åº
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            "chunk_id": chunks[idx]["id"],
            "content": chunks[idx]["content"][:50] + "...",
            "score": similarities[idx],
            "source": chunks[idx]["source"]
        })
    return results

# æµ‹è¯•æ£€ç´¢
query = "Pythonå’Œæœºå™¨å­¦ä¹ "
print(f"æŸ¥è¯¢ï¼š{query}")
print()

results = vector_search(query, all_vectors, all_chunks, top_k=5)

print("æ£€ç´¢ç»“æœï¼š")
for i, res in enumerate(results, 1):
    print(f"{i}. [{res['score']:.3f}] {res['chunk_id']}")
    print(f"   {res['content']}")
    print(f"   æ¥æºï¼š{res['source']}")
    print()

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
print("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
os.remove(vectors_file)
os.remove(package_file)
print("âœ… æ¸…ç†å®Œæˆ")
print()

print("="*70)
print("âœ… ç»ƒä¹ 4å®Œæˆï¼")
print()
print("ğŸ’¡ å…³é”®æ”¶è·ï¼š")
print("   â€¢ æ‰¹é‡ç¼–ç æ¯”å•ä¸ªå¿«10-20å€")
print("   â€¢ batch_sizeè¦æ ¹æ®ç¡¬ä»¶è°ƒæ•´")
print("   â€¢ å‘é‡å¯ä»¥ä¿å­˜å’Œå¢é‡æ›´æ–°")
print("   â€¢ 100ä¸‡å‘é‡çº¦éœ€è¦3GBå†…å­˜")
print("   â€¢ ç®€å•çš„numpyæ•°ç»„å°±èƒ½å®ç°åŸºæœ¬æ£€ç´¢")
print()
print("ğŸ‰ Step 3 å…¨éƒ¨å®Œæˆï¼")
print()
print("ğŸ“Š Step 3 æ€»ç»“ï¼š")
print("   âœ… ç†è§£äº†å‘é‡å’Œå‘é‡åŒ–çš„åŸç†")
print("   âœ… å­¦ä¼šä½¿ç”¨Embeddingæ¨¡å‹")
print("   âœ… æŒæ¡äº†ç›¸ä¼¼åº¦è®¡ç®—å’Œåº”ç”¨")
print("   âœ… èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤§é‡æ–‡æœ¬")
print()
print("ğŸ“ ä¸‹ä¸€æ­¥ï¼šStep 4 - å‘é‡æ•°æ®åº“ï¼ˆChromaDBï¼‰")
print("   å‘½ä»¤ï¼šcd ../step4_vectorstore && cat README.md")
print()
print("ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦å‘é‡æ•°æ®åº“ï¼Ÿ")
print("   â€¢ numpyæ•°ç»„ï¼šç™¾ä¸‡å‘é‡æ£€ç´¢æ…¢")
print("   â€¢ å‘é‡æ•°æ®åº“ï¼šä¼˜åŒ–äº†æ£€ç´¢é€Ÿåº¦ï¼ˆANNç®—æ³•ï¼‰")
print("   â€¢ ChromaDBï¼šè½»é‡çº§ï¼Œæ˜“ç”¨ï¼Œå®Œç¾é€‚åˆå­¦ä¹ ")
print()
print("="*70)

