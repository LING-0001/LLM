"""
å®éªŒ1ï¼šæ²¡æœ‰RAGçš„æ™®é€šLLM
æ¼”ç¤ºï¼šLLMåªèƒ½å›ç­”è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†ï¼Œæ— æ³•è®¿é—®ä½ çš„ç§æœ‰æ–‡æ¡£
"""

from llama_cpp import Llama

print("="*60)
print("å®éªŒ1ï¼šæ™®é€šLLMï¼ˆæ²¡æœ‰RAGï¼‰")
print("="*60)
print()

# åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
llm = Llama(
    model_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    verbose=False
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

# æµ‹è¯•é—®é¢˜ï¼ˆå…³äºè¿™ä¸ªé¡¹ç›®çš„ç§æœ‰çŸ¥è¯†ï¼‰
questions = [
    "MyLLMé¡¹ç›®çš„å­¦ä¹ è·¯çº¿æ˜¯ä»€ä¹ˆï¼Ÿ",
    "è¿™ä¸ªé¡¹ç›®æœ‰å“ªå‡ ä¸ªå­¦ä¹ é˜¶æ®µï¼Ÿ",
    "RAGå­¦ä¹ éƒ¨åˆ†åŒ…å«å“ªäº›æ­¥éª¤ï¼Ÿ"
]

print("ğŸ“Œ æµ‹è¯•åœºæ™¯ï¼šè¯¢é—®æœ¬é¡¹ç›®çš„ç§æœ‰ä¿¡æ¯")
print("   ï¼ˆLLMçš„è®­ç»ƒæ•°æ®ä¸­ä¸åŒ…å«è¿™äº›ä¿¡æ¯ï¼‰\n")

for i, question in enumerate(questions, 1):
    print(f"{'â”€'*60}")
    print(f"é—®é¢˜ {i}: {question}")
    print(f"{'â”€'*60}")
    print("å›ç­”: ", end="", flush=True)
    
    # ç›´æ¥é—®LLMï¼Œä¸æä¾›ä»»ä½•æ–‡æ¡£
    for output in llm(
        question,
        max_tokens=200,
        temperature=0.7,
        stream=True
    ):
        print(output['choices'][0]['text'], end="", flush=True)
    
    print("\n")

print()
print("="*60)
print("ğŸ“Š è§‚å¯Ÿç»“æœ")
print("="*60)
print()
print("âŒ é—®é¢˜ï¼š")
print("   - LLMæ— æ³•å‡†ç¡®å›ç­”å…³äºæœ¬é¡¹ç›®çš„é—®é¢˜")
print("   - å›ç­”å¯èƒ½å«ç³Šä¸æ¸…æˆ–å®Œå…¨é”™è¯¯")
print("   - å¯èƒ½ä¼šç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼ˆå¹»è§‰ï¼‰")
print()
print("ğŸ’¡ åŸå› ï¼š")
print("   - LLMåªçŸ¥é“è®­ç»ƒæ—¶è§è¿‡çš„æ•°æ®")
print("   - æˆ‘ä»¬çš„é¡¹ç›®æ–‡æ¡£ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­")
print("   - æ²¡æœ‰ä»»ä½•å¤–éƒ¨çŸ¥è¯†æ¥æº")
print()
print("âœ… è§£å†³æ–¹æ¡ˆï¼š")
print("   - ä½¿ç”¨RAGï¼è®©LLMèƒ½å¤Ÿæ£€ç´¢æˆ‘ä»¬çš„é¡¹ç›®æ–‡æ¡£")
print("   - è¿è¡Œ 02_with_rag.py æŸ¥çœ‹æ”¹è¿›æ•ˆæœ")
print()
print("="*60)

