# Step 1: 理解RAG原理

> 用最简单的例子理解RAG是什么，为什么需要它

---

## 🎯 本节目标

- 理解RAG的核心概念
- 通过对比实验看清RAG的价值
- 为后续学习打下基础

---

## 📖 什么是RAG？

### 一个生活场景

想象你在图书馆工作，有人问你：
> "《红楼梦》第42回讲了什么？"

**两种回答方式：**

1. **凭记忆回答**（普通LLM）
   - 如果你读过，可能记得大概
   - 如果没读过，可能会瞎说
   - 细节容易出错

2. **查书后回答**（RAG）
   - 去书架找到《红楼梦》
   - 翻到第42回
   - 读完后准确回答
   - 还能引用原文

**RAG = Retrieval Augmented Generation**
- Retrieval = 检索（找到相关资料）
- Augmented = 增强
- Generation = 生成（回答问题）

---

## 🔍 RAG vs 普通LLM

### 场景1：询问训练数据中的知识

**问题：** "太阳系有几颗行星？"

| 方式 | 普通LLM | RAG |
|------|---------|-----|
| **回答** | 8颗 | 8颗 |
| **结果** | ✅ 正确 | ✅ 正确 |
| **原因** | 训练数据中有 | 检索到的资料中有 |

**结论：** 对于通用知识，两者都能回答

---

### 场景2：询问私有/最新知识

**问题：** "我们公司2024年Q4的销售额是多少？"

| 方式 | 普通LLM | RAG |
|------|---------|-----|
| **回答** | "抱歉，我不知道" 或 编造一个数字 | "根据财报显示，Q4销售额为500万" |
| **结果** | ❌ 无法回答或错误 | ✅ 准确 |
| **原因** | 训练数据中没有 | 从你的文档中检索到 |

**结论：** 对于私有/最新知识，RAG才能准确回答

---

## 💡 RAG的应用场景

### 1. 企业知识库问答
- **场景：** 公司有大量内部文档（规章制度、技术文档、FAQ）
- **问题：** 员工需要快速找到信息
- **RAG方案：** 构建智能问答系统，自动从文档中找答案

### 2. 学术论文助手
- **场景：** 研究人员需要查阅大量论文
- **问题：** 人工阅读效率低
- **RAG方案：** 上传论文，AI帮你总结和回答问题

### 3. 法律咨询助手
- **场景：** 律师需要快速查找相关法律条文
- **问题：** 法律文件海量
- **RAG方案：** 检索相关法条，给出专业建议

### 4. 客服机器人
- **场景：** 企业有完善的产品手册和FAQ
- **问题：** 客服人员培训成本高
- **RAG方案：** AI自动从知识库中找答案回复客户

---

## 🔬 实验：对比RAG和普通LLM

运行示例代码，直观感受差异：

### 实验1：普通LLM（没有RAG）
```bash
python 01_without_rag.py
```

**测试问题：**
"MyLLM项目的学习路线是什么？"

**预期结果：**
LLM会根据它的训练数据胡乱猜测，或者说"我不知道"

---

### 实验2：使用RAG
```bash
python 02_with_rag.py
```

**测试问题：**
"MyLLM项目的学习路线是什么？"

**预期结果：**
RAG会从项目的README文档中检索，给出准确的学习路线

---

## 📝 关键概念总结

### 1. 为什么需要RAG？

**LLM的局限：**
- ❌ 只知道训练时的数据
- ❌ 不知道你的私有数据
- ❌ 容易产生幻觉（编造内容）
- ❌ 无法引用来源

**RAG的优势：**
- ✅ 可以访问最新/私有数据
- ✅ 基于真实文档回答
- ✅ 可以标注信息来源
- ✅ 无需重新训练模型

---

### 2. RAG的核心组件

```
┌───────────────┐
│   知识库      │ 你的文档（PDF/TXT/网页等）
└───────┬───────┘
        │
        ▼
┌───────────────┐
│   向量数据库  │ 把文档转换为向量存储
└───────┬───────┘
        │
        ▼
┌───────────────┐
│   检索器      │ 根据问题找到相关文档
└───────┬───────┘
        │
        ▼
┌───────────────┐
│   LLM生成器   │ 根据检索到的文档生成答案
└───────────────┘
```

---

### 3. RAG的工作流程（简化版）

**准备阶段（一次性）：**
1. 把所有文档切成小块（Chunk）
2. 把每块文本转换为向量（Embedding）
3. 把向量存入数据库（Vector Store）

**查询阶段（每次提问）：**
1. 把用户问题转换为向量
2. 在数据库中找到最相似的文档块
3. 把问题和相关文档一起给LLM
4. LLM基于文档生成答案

---

## 🚀 动手实践

### 运行示例代码

```bash
# 确保在正确的目录
cd ~/code/MyLLM/02_rag/step1_understanding

# 实验1：没有RAG
python 01_without_rag.py

# 实验2：使用RAG
python 02_with_rag.py

# 实验3：对比效果
python 03_comparison.py
```

---

## ❓ 思考题

运行完示例后，思考以下问题：

1. **RAG适合哪些场景？不适合哪些场景？**
   - 提示：想想你日常工作中的需求

2. **RAG有什么缺点？**
   - 提示：考虑性能、成本、准确性

3. **如果文档很长（1000页），RAG如何高效检索？**
   - 提示：这就是后续要学习的内容

---

## ✅ 完成标志

理解了以下概念，就可以进入下一步：

- [ ] 我知道RAG是什么
- [ ] 我知道为什么需要RAG
- [ ] 我运行了对比实验，看到了差异
- [ ] 我理解了RAG的基本工作流程

---

## 📍 下一步

**Step 2: 文本切块（Chunking）**

```bash
cd ../step2_chunking
cat README.md
```

学习如何把长文档切成合适的小块，为检索做准备。

---

**继续加油！** 🚀

