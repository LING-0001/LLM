#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 5.2: ä¼˜åŒ–æ£€ç´¢æ•ˆæœ
å­¦ä¹ ç›®æ ‡ï¼šé€šè¿‡è°ƒæ•´æ£€ç´¢ç­–ç•¥æå‡RAGè´¨é‡
"""

import chromadb
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
import os

print("=" * 60)
print("ğŸ” ä¼˜åŒ–RAGæ£€ç´¢æ•ˆæœ")
print("=" * 60)

# ============================================================
# åŠ è½½ç»„ä»¶
# ============================================================

print("\nğŸ“¦ åŠ è½½ç³»ç»Ÿç»„ä»¶...")

# å‘é‡æ•°æ®åº“
db_path = "../step4_vectorstore/data/chroma_traffic_law"
client = chromadb.PersistentClient(path=db_path)
collection = client.get_collection(name="traffic_law")

# Embeddingæ¨¡å‹
embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')

# LLM
llm = Llama(
    model_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    verbose=False
)

print("âœ… æ‰€æœ‰ç»„ä»¶åŠ è½½å®Œæˆ\n")

# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šTop-Kå¯¹æ¯”æµ‹è¯•
# ============================================================

print("ã€ç¬¬ä¸€éƒ¨åˆ†ï¼šTop-Kå¯¹æ¯”æµ‹è¯•ã€‘")
print("=" * 60)

def retrieve_with_topk(question, top_k):
    """æ£€ç´¢æ–‡æ¡£"""
    q_vec = embedding_model.encode([question], show_progress_bar=False)
    results = collection.query(
        query_embeddings=q_vec.tolist(),
        n_results=top_k,
        include=["documents", "distances"]
    )
    
    docs = []
    for i in range(len(results['ids'][0])):
        docs.append({
            'content': results['documents'][0][i],
            'similarity': 1 - results['distances'][0][i]
        })
    return docs

question = "é…’é©¾çš„å¤„ç½šæ˜¯ä»€ä¹ˆï¼Ÿ"
print(f"\nâ“ æµ‹è¯•é—®é¢˜: {question}\n")

# æµ‹è¯•ä¸åŒçš„Top-K
for k in [1, 3, 5]:
    print(f"\n{'='*60}")
    print(f"Top-{k} æ£€ç´¢ç»“æœ")
    print(f"{'='*60}")
    
    docs = retrieve_with_topk(question, k)
    
    for i, doc in enumerate(docs, 1):
        print(f"\n[{i}] ç›¸ä¼¼åº¦: {doc['similarity']*100:.1f}%")
        preview = doc['content'][:80] + "..." if len(doc['content']) > 80 else doc['content']
        print(f"    {preview}")

print(f"\nğŸ’¡ è§‚å¯Ÿ:")
print("   â€¢ Top-1: é€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½ä¿¡æ¯ä¸å…¨")
print("   â€¢ Top-3: å¹³è¡¡ï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µ â­")
print("   â€¢ Top-5: ä¿¡æ¯å…¨ï¼Œä½†å¯èƒ½æœ‰å™ªéŸ³")

# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
# ============================================================

print("\n\nã€ç¬¬äºŒéƒ¨åˆ†ï¼šç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ã€‘")
print("=" * 60)

def retrieve_with_threshold(question, top_k=10, threshold=0.7):
    """
    æ£€ç´¢å¹¶è¿‡æ»¤ä½ç›¸ä¼¼åº¦æ–‡æ¡£
    
    Args:
        question: é—®é¢˜
        top_k: åˆå§‹æ£€ç´¢æ•°é‡
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼(0-1)
    
    Returns:
        è¿‡æ»¤åçš„æ–‡æ¡£åˆ—è¡¨
    """
    # å…ˆæ£€ç´¢Top-K
    docs = retrieve_with_topk(question, top_k)
    
    # è¿‡æ»¤ä½ç›¸ä¼¼åº¦
    filtered_docs = [
        doc for doc in docs
        if doc['similarity'] >= threshold
    ]
    
    return filtered_docs, docs

test_cases = [
    ("é…’é©¾å¤„ç½šæ ‡å‡†", 0.75),  # ç²¾ç¡®é—®é¢˜
    ("è·¯ä¸Šå¼€è½¦è¦æ³¨æ„ä»€ä¹ˆ", 0.60),  # æ¨¡ç³Šé—®é¢˜
]

for question, threshold in test_cases:
    print(f"\n{'='*60}")
    print(f"é—®é¢˜: {question}")
    print(f"é˜ˆå€¼: {threshold}")
    print(f"{'='*60}")
    
    filtered, all_docs = retrieve_with_threshold(question, top_k=5, threshold=threshold)
    
    print(f"\nğŸ“Š æ£€ç´¢ç»“æœ:")
    print(f"   â€¢ åˆå§‹æ£€ç´¢: {len(all_docs)} ä¸ªæ–‡æ¡£")
    print(f"   â€¢ è¿‡æ»¤å: {len(filtered)} ä¸ªæ–‡æ¡£")
    
    if filtered:
        print(f"\nâœ… é«˜è´¨é‡æ–‡æ¡£:")
        for i, doc in enumerate(filtered, 1):
            print(f"   [{i}] ç›¸ä¼¼åº¦: {doc['similarity']*100:.1f}%")
    else:
        print(f"\nâš ï¸ æ²¡æœ‰æ–‡æ¡£è¶…è¿‡é˜ˆå€¼ï¼Œå»ºè®®ï¼š")
        print(f"   1. é™ä½é˜ˆå€¼")
        print(f"   2. æ”¹å†™é—®é¢˜")
        print(f"   3. å›ç­”ã€Œæ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€")

print(f"\nğŸ’¡ é˜ˆå€¼é€‰æ‹©å»ºè®®:")
print("   â€¢ é«˜é˜ˆå€¼(0.8+): ä¸¥æ ¼ï¼Œé€‚åˆä¸“ä¸šåœºæ™¯")
print("   â€¢ ä¸­é˜ˆå€¼(0.7): å¹³è¡¡ï¼Œæ¨è â­")
print("   â€¢ ä½é˜ˆå€¼(0.6): å®½æ¾ï¼Œé€‚åˆæ¨¡ç³Šé—®é¢˜")

# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå…ƒæ•°æ®è¿‡æ»¤
# ============================================================

print("\n\nã€ç¬¬ä¸‰éƒ¨åˆ†ï¼šå…ƒæ•°æ®è¿‡æ»¤ã€‘")
print("=" * 60)

def retrieve_with_metadata(question, chapter=None, min_length=None, top_k=5):
    """
    ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢
    
    Args:
        question: é—®é¢˜
        chapter: æŒ‡å®šç« èŠ‚
        min_length: æœ€å°æ–‡æ¡£é•¿åº¦
        top_k: è¿”å›æ•°é‡
    """
    q_vec = embedding_model.encode([question], show_progress_bar=False)
    
    # æ„å»ºwhereæ¡ä»¶
    where_clause = None
    if chapter and min_length:
        where_clause = {
            "$and": [
                {"chapter": chapter},
                {"length": {"$gte": min_length}}
            ]
        }
    elif chapter:
        where_clause = {"chapter": chapter}
    elif min_length:
        where_clause = {"length": {"$gte": min_length}}
    
    # æ£€ç´¢
    results = collection.query(
        query_embeddings=q_vec.tolist(),
        n_results=top_k,
        where=where_clause,
        include=["documents", "metadatas", "distances"]
    )
    
    docs = []
    for i in range(len(results['ids'][0])):
        docs.append({
            'content': results['documents'][0][i],
            'chapter': results['metadatas'][0][i]['chapter'],
            'length': results['metadatas'][0][i]['length'],
            'similarity': 1 - results['distances'][0][i]
        })
    
    return docs

# æµ‹è¯•åœºæ™¯
print(f"\nåœºæ™¯1: åªåœ¨ã€Œç¬¬ä¸‰ç« ã€ä¸­æœç´¢é©¾é©¶è¯ç›¸å…³é—®é¢˜")
q1 = "é©¾é©¶è¯æ‰£åˆ†è§„å®š"
docs1 = retrieve_with_metadata(q1, chapter="ç¬¬ä¸‰ç« ï¼šæœºåŠ¨è½¦é©¾é©¶è¯ç®¡ç†", top_k=2)

print(f"\nâ“ é—®é¢˜: {q1}")
print(f"ğŸ”§ è¿‡æ»¤: chapter='ç¬¬ä¸‰ç« ï¼šæœºåŠ¨è½¦é©¾é©¶è¯ç®¡ç†'")
print(f"\nç»“æœ:")
for i, doc in enumerate(docs1, 1):
    print(f"\n[{i}] {doc['chapter']} | é•¿åº¦:{doc['length']} | ç›¸ä¼¼åº¦:{doc['similarity']:.2%}")
    preview = doc['content'][:60] + "..." if len(doc['content']) > 60 else doc['content']
    print(f"    {preview}")

print(f"\n{'='*60}")
print(f"åœºæ™¯2: åªæœç´¢è¯¦ç»†æ–‡æ¡£ï¼ˆé•¿åº¦>250ï¼‰")
q2 = "äº¤é€šè¿æ³•å¤„ç½š"
docs2 = retrieve_with_metadata(q2, min_length=250, top_k=3)

print(f"\nâ“ é—®é¢˜: {q2}")
print(f"ğŸ”§ è¿‡æ»¤: length >= 250")
print(f"\nç»“æœ:")
for i, doc in enumerate(docs2, 1):
    print(f"\n[{i}] é•¿åº¦:{doc['length']} | ç›¸ä¼¼åº¦:{doc['similarity']:.2%}")
    print(f"    {doc['chapter']}")

print(f"\nğŸ’¡ å…ƒæ•°æ®è¿‡æ»¤çš„ä¼˜åŠ¿:")
print("   â€¢ ç¼©å°æœç´¢èŒƒå›´ï¼Œæé«˜å‡†ç¡®æ€§")
print("   â€¢ é¿å…æ£€ç´¢åˆ°ä¸ç›¸å…³ç« èŠ‚")
print("   â€¢ å¯ä»¥è¿‡æ»¤å¤ªçŸ­çš„ç¢ç‰‡æ–‡æ¡£")

# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šç»“æœå»é‡
# ============================================================

print("\n\nã€ç¬¬å››éƒ¨åˆ†ï¼šç»“æœå»é‡ã€‘")
print("=" * 60)

def retrieve_and_deduplicate(questions, top_k=3):
    """
    å¤šä¸ªç›¸ä¼¼é—®é¢˜æ£€ç´¢å¹¶å»é‡
    
    Args:
        questions: é—®é¢˜åˆ—è¡¨
        top_k: æ¯ä¸ªé—®é¢˜æ£€ç´¢æ•°é‡
    
    Returns:
        å»é‡åçš„æ–‡æ¡£
    """
    all_docs = {}  # ç”¨dictè‡ªåŠ¨å»é‡
    
    for question in questions:
        q_vec = embedding_model.encode([question], show_progress_bar=False)
        results = collection.query(
            query_embeddings=q_vec.tolist(),
            n_results=top_k,
            include=["documents", "distances"]
        )
        
        for i in range(len(results['ids'][0])):
            doc_id = results['ids'][0][i]
            if doc_id not in all_docs:
                all_docs[doc_id] = {
                    'content': results['documents'][0][i],
                    'similarity': 1 - results['distances'][0][i],
                    'from_question': question
                }
    
    return list(all_docs.values())

# æµ‹è¯•
similar_questions = [
    "é…’åé©¾é©¶çš„å¤„ç½š",
    "é†‰é©¾ä¼šå—åˆ°ä»€ä¹ˆæƒ©ç½š",
    "å–é…’å¼€è½¦æ€ä¹ˆå¤„ç†"
]

print(f"\nğŸ”„ æµ‹è¯•ï¼šå¤šä¸ªç›¸ä¼¼é—®é¢˜")
for i, q in enumerate(similar_questions, 1):
    print(f"   {i}. {q}")

# ä¸å»é‡
total_without_dedup = 0
for q in similar_questions:
    docs = retrieve_with_topk(q, 3)
    total_without_dedup += len(docs)

# å»é‡
deduplicated_docs = retrieve_and_deduplicate(similar_questions, top_k=3)

print(f"\nğŸ“Š ç»Ÿè®¡:")
print(f"   â€¢ ä¸å»é‡: {total_without_dedup} ä¸ªæ–‡æ¡£")
print(f"   â€¢ å»é‡å: {len(deduplicated_docs)} ä¸ªæ–‡æ¡£")
print(f"   â€¢ å‡å°‘: {total_without_dedup - len(deduplicated_docs)} ä¸ªé‡å¤")

print(f"\nå»é‡åçš„æ–‡æ¡£:")
for i, doc in enumerate(deduplicated_docs[:3], 1):
    print(f"\n[{i}] ç›¸ä¼¼åº¦: {doc['similarity']:.2%}")
    print(f"    æ¥è‡ªé—®é¢˜: {doc['from_question']}")
    preview = doc['content'][:60] + "..." if len(doc['content']) > 60 else doc['content']
    print(f"    {preview}")

print(f"\nğŸ’¡ å»é‡çš„å¥½å¤„:")
print("   â€¢ é¿å…ç»™LLMé‡å¤ä¿¡æ¯")
print("   â€¢ èŠ‚çœtokenå’Œæ—¶é—´")
print("   â€¢ æå‡ç­”æ¡ˆè´¨é‡")

# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šç»„åˆä¼˜åŒ–ç­–ç•¥
# ============================================================

print("\n\nã€ç¬¬äº”éƒ¨åˆ†ï¼šç»„åˆä¼˜åŒ–ç­–ç•¥ã€‘")
print("=" * 60)

def optimized_retrieve(question, top_k=10, threshold=0.7, max_results=3):
    """
    ç»„åˆå¤šç§ä¼˜åŒ–ç­–ç•¥çš„æ£€ç´¢å‡½æ•°
    
    ç­–ç•¥ï¼š
    1. å…ˆæ£€ç´¢è¾ƒå¤šæ–‡æ¡£(top_k=10)
    2. ç›¸ä¼¼åº¦è¿‡æ»¤(threshold=0.7)
    3. é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡(max_results=3)
    """
    # æ£€ç´¢
    q_vec = embedding_model.encode([question], show_progress_bar=False)
    results = collection.query(
        query_embeddings=q_vec.tolist(),
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )
    
    # å¤„ç†å’Œè¿‡æ»¤
    filtered_docs = []
    for i in range(len(results['ids'][0])):
        similarity = 1 - results['distances'][0][i]
        
        if similarity >= threshold:
            filtered_docs.append({
                'content': results['documents'][0][i],
                'chapter': results['metadatas'][0][i]['chapter'],
                'similarity': similarity
            })
    
    # è¿”å›Top-N
    return filtered_docs[:max_results]

# æµ‹è¯•
test_q = "äº¤é€šäº‹æ•…é€ƒé€¸çš„åæœ"

print(f"\nâ“ é—®é¢˜: {test_q}")
print(f"ğŸ”§ ä¼˜åŒ–ç­–ç•¥:")
print(f"   1. åˆå§‹æ£€ç´¢ Top-10")
print(f"   2. è¿‡æ»¤ç›¸ä¼¼åº¦ < 0.7")
print(f"   3. è¿”å›æœ€ç»ˆ Top-3")

optimized_docs = optimized_retrieve(test_q, top_k=10, threshold=0.7, max_results=3)

print(f"\nâœ… æœ€ç»ˆç»“æœ ({len(optimized_docs)}ä¸ª):")
for i, doc in enumerate(optimized_docs, 1):
    print(f"\n[{i}] ç›¸ä¼¼åº¦: {doc['similarity']:.2%} | {doc['chapter']}")
    preview = doc['content'][:70] + "..." if len(doc['content']) > 70 else doc['content']
    print(f"    {preview}")

if len(optimized_docs) == 0:
    print("\nâš ï¸  æ²¡æœ‰é«˜è´¨é‡æ–‡æ¡£ï¼Œå»ºè®®å›ç­”ï¼š")
    print("    ã€ŒæŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚ã€")

# ============================================================
# æ€»ç»“
# ============================================================

print("\n\n" + "=" * 60)
print("ğŸ‰ æ£€ç´¢ä¼˜åŒ–å­¦ä¹ å®Œæˆï¼")
print("=" * 60)

print("\nâœ… æŒæ¡çš„ä¼˜åŒ–æŠ€å·§:")
print("   1. Top-Kè°ƒæ•´ï¼ˆ1 vs 3 vs 5ï¼‰")
print("   2. ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼ˆå»é™¤ä½è´¨é‡ï¼‰")
print("   3. å…ƒæ•°æ®è¿‡æ»¤ï¼ˆç¼©å°èŒƒå›´ï¼‰")
print("   4. ç»“æœå»é‡ï¼ˆé¿å…é‡å¤ï¼‰")
print("   5. ç»„åˆç­–ç•¥ï¼ˆå¤šé‡ä¼˜åŒ–ï¼‰")

print("\nğŸ’¡ æœ€ä½³å®è·µ:")
print("   â€¢ å…ˆæ£€ç´¢Top-10ï¼Œå†è¿‡æ»¤åˆ°Top-3")
print("   â€¢ è®¾ç½®é˜ˆå€¼0.7ï¼Œè¿‡æ»¤ä½è´¨é‡")
print("   â€¢ ä½¿ç”¨å…ƒæ•°æ®ç¼©å°èŒƒå›´")
print("   â€¢ å¤šé—®é¢˜æ£€ç´¢è¦å»é‡")

print("\nğŸ¯ ä¸‹ä¸€æ­¥:")
print("   è¿è¡Œ: python 03_improve_prompt.py")
print("   å­¦ä¹ å¦‚ä½•ä¼˜åŒ–Promptæå‡ç”Ÿæˆè´¨é‡")

print("\n" + "=" * 60)

