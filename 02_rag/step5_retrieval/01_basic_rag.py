#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 5.1: åŸºç¡€RAGé—®ç­”ç³»ç»Ÿ
å­¦ä¹ ç›®æ ‡ï¼šæ•´åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œæ„å»ºå®Œæ•´çš„RAGæµç¨‹
"""

import chromadb
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
import os

print("=" * 60)
print("ğŸ¤– åŸºç¡€RAGé—®ç­”ç³»ç»Ÿ")
print("=" * 60)

# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŠ è½½ç»„ä»¶
# ============================================================

print("\nã€ç¬¬ä¸€éƒ¨åˆ†ï¼šåŠ è½½ç³»ç»Ÿç»„ä»¶ã€‘")
print("-" * 60)

# 1. åŠ è½½å‘é‡æ•°æ®åº“
print("\n1ï¸âƒ£ åŠ è½½å‘é‡æ•°æ®åº“...")
db_path = "../step4_vectorstore/data/chroma_traffic_law"

if not os.path.exists(db_path):
    print("âŒ é”™è¯¯ï¼šå‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼")
    print(f"   è¯·å…ˆè¿è¡Œ: cd ../step4_vectorstore && python 02_import_traffic_law.py")
    exit(1)

client = chromadb.PersistentClient(path=db_path)
collection = client.get_collection(name="traffic_law")
print(f"   âœ… æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {collection.count()} ä¸ªæ–‡æ¡£")

# 2. åŠ è½½Embeddingæ¨¡å‹
print("\n2ï¸âƒ£ åŠ è½½Embeddingæ¨¡å‹...")
embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')
print("   âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")

# 3. åŠ è½½LLM
print("\n3ï¸âƒ£ åŠ è½½LLM...")
llm_path = "/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf"

if not os.path.exists(llm_path):
    print("âŒ é”™è¯¯ï¼šLLMæ¨¡å‹ä¸å­˜åœ¨ï¼")
    print(f"   è¯·æ£€æŸ¥è·¯å¾„: {llm_path}")
    exit(1)

llm = Llama(
    model_path=llm_path,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,  # CPUæ¨¡å¼
    verbose=False
)
print("   âœ… LLMåŠ è½½å®Œæˆ")

# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šå®ç°RAGæ ¸å¿ƒå‡½æ•°
# ============================================================

print("\nã€ç¬¬äºŒéƒ¨åˆ†ï¼šRAGæ ¸å¿ƒå‡½æ•°ã€‘")
print("-" * 60)

def retrieve_documents(question, top_k=3):
    """
    æ£€ç´¢ç›¸å…³æ–‡æ¡£
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        top_k: è¿”å›Top-Kä¸ªæ–‡æ¡£
    
    Returns:
        æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
    """
    # 1. å‘é‡åŒ–é—®é¢˜
    question_vector = embedding_model.encode([question], show_progress_bar=False)
    
    # 2. æ£€ç´¢
    results = collection.query(
        query_embeddings=question_vector.tolist(),
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )
    
    # 3. æ ¼å¼åŒ–ç»“æœ
    retrieved_docs = []
    for i in range(len(results['ids'][0])):
        retrieved_docs.append({
            'id': results['ids'][0][i],
            'content': results['documents'][0][i],
            'chapter': results['metadatas'][0][i]['chapter'],
            'similarity': 1 - results['distances'][0][i]
        })
    
    return retrieved_docs


def generate_answer(question, context):
    """
    åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    
    Returns:
        LLMç”Ÿæˆçš„ç­”æ¡ˆ
    """
    # æ„å»ºPrompt
    prompt = f"""æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘
"""
    
    # LLMç”Ÿæˆ
    output = llm(
        prompt,
        max_tokens=256,
        temperature=0.3,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®š
        stop=["ã€", "\n\n"],
        echo=False,
        stream=False
    )
    
    answer = output['choices'][0]['text'].strip()
    return answer


def rag_query(question, top_k=3, show_retrieval=True):
    """
    å®Œæ•´çš„RAGæŸ¥è¯¢æµç¨‹
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
        show_retrieval: æ˜¯å¦æ˜¾ç¤ºæ£€ç´¢ç»“æœ
    
    Returns:
        ç­”æ¡ˆ
    """
    print(f"\n{'='*60}")
    print(f"â“ é—®é¢˜: {question}")
    print(f"{'='*60}")
    
    # Step 1: æ£€ç´¢
    print(f"\nğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆTop-{top_k}ï¼‰...")
    retrieved_docs = retrieve_documents(question, top_k)
    
    if show_retrieval:
        print(f"\nğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£:")
        for i, doc in enumerate(retrieved_docs, 1):
            print(f"\n[æ–‡æ¡£{i}] ç›¸ä¼¼åº¦: {doc['similarity']*100:.1f}% | {doc['chapter']}")
            preview = doc['content'][:100] + "..." if len(doc['content']) > 100 else doc['content']
            print(f"   {preview}")
    
    # Step 2: æ„å»ºä¸Šä¸‹æ–‡
    context = "\n\n".join([doc['content'] for doc in retrieved_docs])
    
    # Step 3: ç”Ÿæˆç­”æ¡ˆ
    print(f"\nğŸ¤– ç”Ÿæˆç­”æ¡ˆ...")
    answer = generate_answer(question, context)
    
    print(f"\nğŸ’¡ ç­”æ¡ˆ:")
    print(f"{'='*60}")
    print(answer)
    print(f"{'='*60}")
    
    return answer

print("âœ… RAGå‡½æ•°å®šä¹‰å®Œæˆ")

# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæµ‹è¯•RAGç³»ç»Ÿ
# ============================================================

print("\nã€ç¬¬ä¸‰éƒ¨åˆ†ï¼šæµ‹è¯•RAGç³»ç»Ÿã€‘")
print("-" * 60)

# æµ‹è¯•é—®é¢˜åˆ—è¡¨
test_questions = [
    "é…’é©¾ä¼šå—åˆ°ä»€ä¹ˆå¤„ç½šï¼Ÿ",
    "é—¯çº¢ç¯è¦æ‰£å¤šå°‘åˆ†ï¼Ÿ",
    "æ–°æ‰‹å¸æœºå®ä¹ æœŸæœ‰ä»€ä¹ˆè§„å®šï¼Ÿ",
    "äº¤é€šäº‹æ•…ååº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿ",
    "è¶…é€Ÿè¡Œé©¶å¦‚ä½•å¤„ç½šï¼Ÿ"
]

print(f"\nå°†æµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜...")
print("\n" + "ğŸ”” æç¤ºï¼šé¦–æ¬¡ç”Ÿæˆå¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…")

# åªæµ‹è¯•å‰2ä¸ªé—®é¢˜ï¼ˆå®Œæ•´æµ‹è¯•å¤ªæ…¢ï¼‰
for idx, question in enumerate(test_questions[:2], 1):
    print(f"\n\n{'#'*60}")
    print(f"# æµ‹è¯• {idx}/{len(test_questions[:2])}")
    print(f"{'#'*60}")
    
    answer = rag_query(question, top_k=3, show_retrieval=True)

# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¯¹æ¯”æœ‰æ— RAGçš„æ•ˆæœ
# ============================================================

print("\n\n" + "=" * 60)
print("ã€ç¬¬å››éƒ¨åˆ†ï¼šå¯¹æ¯”æœ‰æ— RAGçš„æ•ˆæœã€‘")
print("=" * 60)

comparison_question = "é†‰é…’é©¾é©¶ä¼šè¢«åˆ¤åˆ‘å—ï¼Ÿ"

# æ— RAGï¼šç›´æ¥é—®LLM
print(f"\nâ“ é—®é¢˜: {comparison_question}")
print(f"\n{'='*60}")
print("æ–¹å¼1ï¼šä¸ä½¿ç”¨RAGï¼ˆLLMç›´æ¥å›ç­”ï¼‰")
print(f"{'='*60}")

prompt_no_rag = f"{comparison_question}\nå›ç­”ï¼š"

output_no_rag = llm(
    prompt_no_rag,
    max_tokens=128,
    temperature=0.7,
    stop=["\n\n"],
    echo=False,
    stream=False
)

answer_no_rag = output_no_rag['choices'][0]['text'].strip()
print(f"\nğŸ’¬ {answer_no_rag}")

# ä½¿ç”¨RAG
print(f"\n{'='*60}")
print("æ–¹å¼2ï¼šä½¿ç”¨RAGï¼ˆåŸºäºäº¤é€šæ³•æ–‡æ¡£ï¼‰")
print(f"{'='*60}")

retrieved_docs = retrieve_documents(comparison_question, top_k=2)
context = "\n\n".join([doc['content'] for doc in retrieved_docs])
answer_with_rag = generate_answer(comparison_question, context)

print(f"\nğŸ’¡ {answer_with_rag}")

# å¯¹æ¯”åˆ†æ
print(f"\n{'='*60}")
print("ğŸ“Š å¯¹æ¯”åˆ†æ")
print(f"{'='*60}")
print("\næ— RAG:")
print("  â€¢ å¯èƒ½åŸºäºæ¨¡å‹è®°å¿†å›ç­”")
print("  â€¢ ä¿¡æ¯å¯èƒ½è¿‡æ—¶æˆ–ä¸å‡†ç¡®")
print("  â€¢ ç¼ºä¹ä¾æ®")

print("\nä½¿ç”¨RAG:")
print("  â€¢ åŸºäºæœ€æ–°çš„äº¤é€šæ³•æ–‡æ¡£")
print("  â€¢ ç­”æ¡ˆæœ‰æ®å¯æŸ¥")
print("  â€¢ æ›´å‡†ç¡®ã€æ›´å¯ä¿¡")

# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šåˆ†æRAGæµç¨‹
# ============================================================

print("\n" + "=" * 60)
print("ã€ç¬¬äº”éƒ¨åˆ†ï¼šRAGæµç¨‹åˆ†æã€‘")
print("=" * 60)

test_q = "é©¾é©¶è¯æ‰£æ»¡12åˆ†æ€ä¹ˆåŠï¼Ÿ"

print(f"\nğŸ“‹ è¯¦ç»†æµç¨‹æ¼”ç¤º")
print(f"é—®é¢˜: {test_q}")
print("-" * 60)

# Step 1
print("\n[Step 1] å‘é‡åŒ–é—®é¢˜")
q_vec = embedding_model.encode([test_q], show_progress_bar=False)
print(f"   â€¢ é—®é¢˜: {test_q}")
print(f"   â€¢ å‘é‡ç»´åº¦: {q_vec.shape[1]}")
print(f"   â€¢ å‘é‡ç¤ºä¾‹: [{q_vec[0][:5].tolist()}...]")

# Step 2
print("\n[Step 2] æ£€ç´¢ç›¸å…³æ–‡æ¡£")
docs = retrieve_documents(test_q, top_k=2)
print(f"   â€¢ æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
for i, doc in enumerate(docs, 1):
    print(f"   â€¢ æ–‡æ¡£{i}: ç›¸ä¼¼åº¦={doc['similarity']:.2%}, ç« èŠ‚={doc['chapter']}")

# Step 3
print("\n[Step 3] æ„å»ºPrompt")
context = "\n\n".join([doc['content'] for doc in docs])
prompt = f"""æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š

ã€å‚è€ƒèµ„æ–™ã€‘
{context[:200]}...ï¼ˆçœç•¥ï¼‰

ã€é—®é¢˜ã€‘
{test_q}

ã€å›ç­”ã€‘
"""
print(f"   â€¢ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
print(f"   â€¢ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")

# Step 4
print("\n[Step 4] LLMç”Ÿæˆç­”æ¡ˆ")
answer = generate_answer(test_q, context)
print(f"   â€¢ ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
print(f"   â€¢ ç­”æ¡ˆ: {answer[:100]}...")

# ============================================================
# æ€»ç»“
# ============================================================

print("\n" + "=" * 60)
print("ğŸ‰ åŸºç¡€RAGç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
print("=" * 60)

print("\nâœ… ä½ å·²ç»å­¦ä¼š:")
print("   1. åŠ è½½å‘é‡æ•°æ®åº“å’ŒLLM")
print("   2. å®ç°æ£€ç´¢å‡½æ•°")
print("   3. å®ç°ç”Ÿæˆå‡½æ•°")
print("   4. æ•´åˆå®Œæ•´RAGæµç¨‹")
print("   5. å¯¹æ¯”æœ‰æ— RAGçš„æ•ˆæœ")

print("\nğŸ’¡ RAGæ ¸å¿ƒæµç¨‹:")
print("   é—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ æ„å»ºPrompt â†’ LLMç”Ÿæˆ â†’ ç­”æ¡ˆ")

print("\nğŸ“Š è§‚å¯Ÿç»“æœ:")
print("   â€¢ RAGèƒ½æä¾›å‡†ç¡®çš„ã€æœ‰ä¾æ®çš„ç­”æ¡ˆ")
print("   â€¢ æ£€ç´¢è´¨é‡ç›´æ¥å½±å“ç­”æ¡ˆè´¨é‡")
print("   â€¢ Promptè®¾è®¡å¾ˆé‡è¦")

print("\nğŸ”§ å¯ä¼˜åŒ–çš„åœ°æ–¹:")
print("   â€¢ Top-Ké€‰æ‹©ï¼ˆ3 vs 5 vs 10ï¼‰")
print("   â€¢ ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤")
print("   â€¢ Promptæ¨¡æ¿ä¼˜åŒ–")
print("   â€¢ Temperatureè°ƒæ•´")

print("\nğŸ¯ ä¸‹ä¸€æ­¥:")
print("   è¿è¡Œ: python 02_improve_retrieval.py")
print("   å­¦ä¹ å¦‚ä½•ä¼˜åŒ–æ£€ç´¢æ•ˆæœ")

print("\n" + "=" * 60)

