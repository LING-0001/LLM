#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 5.4: å®Œæ•´çš„RAGç³»ç»Ÿ
å­¦ä¹ ç›®æ ‡ï¼šæ•´åˆæ‰€æœ‰ä¼˜åŒ–ï¼Œæ„å»ºç”Ÿäº§çº§RAGç±»
"""

import chromadb
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
import os
import time

print("=" * 60)
print("ğŸ—ï¸  å®Œæ•´RAGç³»ç»Ÿ")
print("=" * 60)

# ============================================================
# RAGç³»ç»Ÿç±»å®šä¹‰
# ============================================================

class TrafficLawRAG:
    """
    äº¤é€šæ³•RAGé—®ç­”ç³»ç»Ÿ
    
    æ•´åˆäº†æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥ï¼š
    - ä¼˜åŒ–çš„æ£€ç´¢ï¼ˆTop-K + é˜ˆå€¼ + å»é‡ï¼‰
    - ä¼˜åŒ–çš„Promptï¼ˆé˜²å¹»è§‰ + æ ¼å¼åŒ–ï¼‰
    - å¼‚å¸¸å¤„ç†
    - æ—¥å¿—è®°å½•
    """
    
    def __init__(
        self,
        db_path,
        embedding_model_name,
        llm_path,
        collection_name="traffic_law"
    ):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            embedding_model_name: Embeddingæ¨¡å‹åç§°
            llm_path: LLMæ¨¡å‹è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        print("\nğŸš€ åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        print("   [1/3] åŠ è½½å‘é‡æ•°æ®åº“...")
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_collection(name=collection_name)
        print(f"         âœ… æ•°æ®åº“åŒ…å« {self.collection.count()} ä¸ªæ–‡æ¡£")
        
        # åŠ è½½Embeddingæ¨¡å‹
        print("   [2/3] åŠ è½½Embeddingæ¨¡å‹...")
        self.embedding_model = SentenceTransformer(embedding_model_name)
        print("         âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åŠ è½½LLM
        print("   [3/3] åŠ è½½LLM...")
        self.llm = Llama(
            model_path=llm_path,
            n_ctx=2048,
            n_threads=4,
            n_gpu_layers=0,
            verbose=False
        )
        print("         âœ… LLMåŠ è½½å®Œæˆ")
        
        print("\nâœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼\n")
    
    def retrieve(
        self,
        question,
        top_k=10,
        threshold=0.7,
        max_results=3
    ):
        """
        ä¼˜åŒ–çš„æ£€ç´¢å‡½æ•°
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: åˆå§‹æ£€ç´¢æ•°é‡
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            max_results: æœ€ç»ˆè¿”å›æ•°é‡
        
        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # å‘é‡åŒ–é—®é¢˜
            question_vector = self.embedding_model.encode(
                [question],
                show_progress_bar=False
            )
            
            # æ£€ç´¢
            results = self.collection.query(
                query_embeddings=question_vector.tolist(),
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )
            
            # è¿‡æ»¤å’Œæ ¼å¼åŒ–
            retrieved_docs = []
            for i in range(len(results['ids'][0])):
                similarity = 1 - results['distances'][0][i]
                
                if similarity >= threshold:
                    retrieved_docs.append({
                        'id': results['ids'][0][i],
                        'content': results['documents'][0][i],
                        'chapter': results['metadatas'][0][i]['chapter'],
                        'similarity': similarity
                    })
            
            # è¿”å›Top-N
            return retrieved_docs[:max_results]
        
        except Exception as e:
            print(f"âš ï¸  æ£€ç´¢é”™è¯¯: {e}")
            return []
    
    def generate(self, question, context):
        """
        åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # æ„å»ºPrompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº¤é€šæ³•è§„åŠ©æ‰‹ï¼Œä¸“é—¨è§£ç­”ä¸­å›½é“è·¯äº¤é€šå®‰å…¨æ³•ç›¸å…³é—®é¢˜ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€å›ç­”è§„åˆ™ã€‘
1. ä¸¥æ ¼ä¾æ®å‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸ç¼–é€ ä¿¡æ¯
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œæ˜ç¡®å›ç­”ã€Œå‚è€ƒèµ„æ–™ä¸­æœªæåŠæ­¤å†…å®¹ã€
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€åˆ†ç‚¹åˆ—å‡º
4. ä¿æŒå®¢è§‚ä¸­ç«‹çš„è¯­æ°”

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€ä½ çš„å›ç­”ã€‘
"""
        
        try:
            # ç”Ÿæˆç­”æ¡ˆ
            output = self.llm(
                prompt,
                max_tokens=300,
                temperature=0.2,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®š
                stop=["ã€", "\n\n\n"],
                echo=False,
                stream=False
            )
            
            answer = output['choices'][0]['text'].strip()
            return answer
        
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆé”™è¯¯: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™äº†ã€‚"
    
    def query(
        self,
        question,
        top_k=10,
        threshold=0.7,
        max_results=3,
        show_details=False
    ):
        """
        å®Œæ•´çš„RAGæŸ¥è¯¢æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢æ•°é‡
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            max_results: æœ€ç»ˆæ–‡æ¡£æ•°
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            dict: åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•°æ®
        """
        start_time = time.time()
        
        # Step 1: æ£€ç´¢
        retrieved_docs = self.retrieve(
            question,
            top_k=top_k,
            threshold=threshold,
            max_results=max_results
        )
        
        retrieve_time = time.time() - start_time
        
        # Step 2: å¤„ç†æ£€ç´¢ç»“æœ
        if len(retrieved_docs) == 0:
            return {
                'question': question,
                'answer': "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨ï¼š\n1. å°è¯•ç”¨ä¸åŒæ–¹å¼è¡¨è¾¾é—®é¢˜\n2. æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨äº¤é€šæ³•è§„èŒƒå›´å†…",
                'sources': [],
                'retrieve_time': retrieve_time,
                'generate_time': 0,
                'total_time': retrieve_time
            }
        
        # Step 3: æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc['content'] for doc in retrieved_docs])
        
        # Step 4: ç”Ÿæˆç­”æ¡ˆ
        generate_start = time.time()
        answer = self.generate(question, context)
        generate_time = time.time() - generate_start
        
        total_time = time.time() - start_time
        
        # è¿”å›ç»“æœ
        result = {
            'question': question,
            'answer': answer,
            'sources': retrieved_docs,
            'retrieve_time': retrieve_time,
            'generate_time': generate_time,
            'total_time': total_time
        }
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if show_details:
            self._print_result(result)
        
        return result
    
    def _print_result(self, result):
        """æ‰“å°æŸ¥è¯¢ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"â“ é—®é¢˜: {result['question']}")
        print(f"{'='*60}")
        
        # æ£€ç´¢çš„æ–‡æ¡£
        print(f"\nğŸ“š æ£€ç´¢åˆ° {len(result['sources'])} ä¸ªç›¸å…³æ–‡æ¡£:")
        for i, doc in enumerate(result['sources'], 1):
            print(f"\n[{i}] ç›¸ä¼¼åº¦: {doc['similarity']*100:.1f}% | {doc['chapter']}")
            preview = doc['content'][:80] + "..." if len(doc['content']) > 80 else doc['content']
            print(f"    {preview}")
        
        # ç­”æ¡ˆ
        print(f"\nğŸ’¡ ç­”æ¡ˆ:")
        print(f"{'='*60}")
        print(result['answer'])
        print(f"{'='*60}")
        
        # æ€§èƒ½ç»Ÿè®¡
        print(f"\nâ±ï¸  æ€§èƒ½:")
        print(f"   â€¢ æ£€ç´¢æ—¶é—´: {result['retrieve_time']*1000:.0f}ms")
        print(f"   â€¢ ç”Ÿæˆæ—¶é—´: {result['generate_time']*1000:.0f}ms")
        print(f"   â€¢ æ€»æ—¶é—´: {result['total_time']:.2f}s")


# ============================================================
# æµ‹è¯•RAGç³»ç»Ÿ
# ============================================================

print("ã€æµ‹è¯•å®Œæ•´RAGç³»ç»Ÿã€‘")
print("=" * 60)

# åˆå§‹åŒ–ç³»ç»Ÿ
rag_system = TrafficLawRAG(
    db_path="../step4_vectorstore/data/chroma_traffic_law",
    embedding_model_name="shibing624/text2vec-base-chinese",
    llm_path="/Users/a58/llama.cpp/models/qwen2.5-3b-instruct-q4_k_m.gguf"
)

# æµ‹è¯•é—®é¢˜
test_questions = [
    "é…’é©¾çš„å¤„ç½šæ˜¯ä»€ä¹ˆï¼Ÿ",
    "äº¤é€šäº‹æ•…ååº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿ",
    "é©¾é©¶è¯æ‰£æ»¡12åˆ†æ€ä¹ˆåŠï¼Ÿ",
]

print(f"\næµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜:")

for idx, question in enumerate(test_questions, 1):
    print(f"\n\n{'#'*60}")
    print(f"# æµ‹è¯• {idx}/{len(test_questions)}")
    print(f"{'#'*60}")
    
    result = rag_system.query(
        question,
        top_k=10,
        threshold=0.7,
        max_results=3,
        show_details=True
    )

# ============================================================
# æ€»ç»“
# ============================================================

print("\n\n" + "=" * 60)
print("ğŸ‰ å®Œæ•´RAGç³»ç»Ÿæ„å»ºå®Œæˆï¼")
print("=" * 60)

print("\nâœ… ç³»ç»Ÿç‰¹æ€§:")
print("   1. é¢å‘å¯¹è±¡è®¾è®¡ï¼Œæ˜“äºå¤ç”¨")
print("   2. æ•´åˆæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥")
print("   3. å®Œå–„çš„å¼‚å¸¸å¤„ç†")
print("   4. æ€§èƒ½ç»Ÿè®¡")
print("   5. çµæ´»çš„å‚æ•°é…ç½®")

print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
print("""
# åˆå§‹åŒ–
rag = TrafficLawRAG(db_path, model_name, llm_path)

# æŸ¥è¯¢
result = rag.query(
    "ä½ çš„é—®é¢˜",
    top_k=10,
    threshold=0.7,
    max_results=3,
    show_details=True
)

# è·å–ç­”æ¡ˆ
answer = result['answer']
sources = result['sources']
""")

print("\nğŸ¯ ä¸‹ä¸€æ­¥:")
print("   è¿è¡Œ: python 05_interactive_qa.py")
print("   ä½“éªŒäº¤äº’å¼é—®ç­”ç•Œé¢")

print("\n" + "=" * 60)

