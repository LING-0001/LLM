#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 4.2: å¯¼å…¥äº¤é€šæ³•æ•°æ®åˆ°ChromaDB
å­¦ä¹ ç›®æ ‡ï¼šå°†çœŸå®æ•°æ®å¯¼å…¥å‘é‡æ•°æ®åº“
"""

import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np
import json
import os

print("=" * 60)
print("ğŸš— å¯¼å…¥äº¤é€šæ³•æ•°æ®åˆ°ChromaDB")
print("=" * 60)

# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŠ è½½é¢„å¤„ç†çš„æ•°æ®
# ============================================================

print("\nã€ç¬¬ä¸€éƒ¨åˆ†ï¼šåŠ è½½æ•°æ®ã€‘")
print("-" * 60)

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
data_dir = "../../data"
vectors_path = os.path.join(data_dir, "traffic_law_vectors.npy")
json_path = os.path.join(data_dir, "traffic_law_data.json")

if not os.path.exists(vectors_path):
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°å‘é‡æ–‡ä»¶ï¼")
    print(f"   è¯·å…ˆè¿è¡Œ: python ../../prepare_traffic_law_data.py")
    exit(1)

if not os.path.exists(json_path):
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼")
    print(f"   è¯·å…ˆè¿è¡Œ: python ../../prepare_traffic_law_data.py")
    exit(1)

# åŠ è½½å‘é‡
print(f"\nğŸ“¦ åŠ è½½å‘é‡æ•°æ®...")
vectors = np.load(vectors_path)
print(f"   âœ… åŠ è½½æˆåŠŸ: {vectors.shape[0]} ä¸ªå‘é‡ï¼Œç»´åº¦: {vectors.shape[1]}")

# åŠ è½½JSONæ•°æ®
print(f"\nğŸ“¦ åŠ è½½æ–‡æœ¬æ•°æ®...")
with open(json_path, 'r', encoding='utf-8') as f:
    data = json.load(f)

chunks = data['chunks']

print(f"   âœ… åŠ è½½æˆåŠŸ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
print(f"   â€¢ æ–‡æ¡£æ¥æº: {data['source_file']}")
print(f"   â€¢ æ¨¡å‹åç§°: {data['model_name']}")
print(f"   â€¢ åˆ†å—æ•°é‡: {data['num_chunks']} ä¸ª")
print(f"   â€¢ åˆ†å—å¤§å°: {data['chunk_size']} å­—ç¬¦")
print(f"   â€¢ é‡å å¤§å°: {data['chunk_overlap']} å­—ç¬¦")
print(f"   â€¢ å‘é‡ç»´åº¦: {data['vector_dim']}")

# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ›å»ºChromaDBæ•°æ®åº“
# ============================================================

print("\nã€ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“ã€‘")
print("-" * 60)

# åˆ›å»ºæŒä¹…åŒ–æ•°æ®åº“
db_path = "./data/chroma_traffic_law"
os.makedirs(db_path, exist_ok=True)

client = chromadb.PersistentClient(path=db_path)
print(f"âœ… æ•°æ®åº“åˆ›å»ºæˆåŠŸ")
print(f"ğŸ“ å­˜å‚¨è·¯å¾„: {db_path}")

# åˆ é™¤æ—§é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    client.delete_collection(name="traffic_law")
    print("ğŸ—‘ï¸  åˆ é™¤æ—§é›†åˆ: traffic_law")
except:
    pass

# åˆ›å»ºæ–°é›†åˆ
collection = client.create_collection(
    name="traffic_law",
    metadata={
        "description": "ä¸­å›½äº¤é€šæ³•è§„çŸ¥è¯†åº“",
        "hnsw:space": "cosine",  # ä½™å¼¦ç›¸ä¼¼åº¦
        "source": data['source_file'],
        "model": data['model_name']
    }
)
print("âœ… åˆ›å»ºæ–°é›†åˆ: traffic_law")

# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰¹é‡å¯¼å…¥æ•°æ®
# ============================================================

print("\nã€ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰¹é‡å¯¼å…¥æ•°æ®ã€‘")
print("-" * 60)

# å‡†å¤‡æ•°æ®
documents = [chunk['content'] for chunk in chunks]
ids = [chunk['id'] for chunk in chunks]
metadatas = [{
    'chapter': chunk['chapter'],
    'length': chunk['length'],
    'chunk_id': chunk['index']
} for chunk in chunks]

print(f"\nğŸ“ å‡†å¤‡å¯¼å…¥ {len(documents)} ä¸ªæ–‡æœ¬å—...")

# æ‰¹é‡æ·»åŠ 
batch_size = 50  # æ¯æ¬¡æ·»åŠ 50ä¸ª
total = len(documents)

for i in range(0, total, batch_size):
    end_idx = min(i + batch_size, total)
    batch_vectors = vectors[i:end_idx].tolist()
    batch_documents = documents[i:end_idx]
    batch_ids = ids[i:end_idx]
    batch_metadatas = metadatas[i:end_idx]
    
    collection.add(
        embeddings=batch_vectors,
        documents=batch_documents,
        ids=batch_ids,
        metadatas=batch_metadatas
    )
    
    print(f"   âœ… æ‰¹æ¬¡ {i//batch_size + 1}: å·²å¯¼å…¥ {end_idx}/{total} ä¸ªæ–‡æœ¬å—")

print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®å¯¼å…¥å®Œæˆï¼")

# éªŒè¯å¯¼å…¥
count = collection.count()
print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
print(f"   â€¢ é›†åˆä¸­çš„æ–‡æ¡£æ•°: {count}")
print(f"   â€¢ é¢„æœŸæ–‡æ¡£æ•°: {total}")
print(f"   â€¢ å¯¼å…¥çŠ¶æ€: {'âœ… æˆåŠŸ' if count == total else 'âŒ å¤±è´¥'}")

# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæµ‹è¯•æ£€ç´¢æ•ˆæœ
# ============================================================

print("\nã€ç¬¬å››éƒ¨åˆ†ï¼šæµ‹è¯•æ£€ç´¢æ•ˆæœã€‘")
print("-" * 60)

# åŠ è½½embeddingæ¨¡å‹
print(f"\nğŸ“¦ åŠ è½½embeddingæ¨¡å‹...")
model = SentenceTransformer('shibing624/text2vec-base-chinese')
print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# æµ‹è¯•é—®é¢˜åˆ—è¡¨
test_questions = [
    "é…’é©¾ä¼šå—åˆ°ä»€ä¹ˆå¤„ç½šï¼Ÿ",
    "é—¯çº¢ç¯è¦æ‰£å‡ åˆ†ï¼Ÿ",
    "æ–°æ‰‹å¸æœºå®ä¹ æœŸæœ‰ä»€ä¹ˆè§„å®šï¼Ÿ",
    "è¶…é€Ÿè¡Œé©¶ä¼šè¢«æ€ä¹ˆå¤„ç†ï¼Ÿ",
    "äº¤é€šäº‹æ•…ååº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
]

print(f"\nğŸ§ª æµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜...")
print("=" * 60)

for idx, question in enumerate(test_questions, 1):
    print(f"\nã€é—®é¢˜ {idx}ã€‘{question}")
    print("-" * 60)
    
    # ç”Ÿæˆé—®é¢˜å‘é‡
    query_embedding = model.encode([question], show_progress_bar=False)
    
    # æ£€ç´¢Top-3
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=3,
        include=["documents", "metadatas", "distances"]
    )
    
    # æ˜¾ç¤ºç»“æœ
    for i in range(len(results['ids'][0])):
        doc_id = results['ids'][0][i]
        document = results['documents'][0][i]
        distance = results['distances'][0][i]
        similarity = 1 - distance
        chunk_id = results['metadatas'][0][i]['chunk_id']
        length = results['metadatas'][0][i]['length']
        
        print(f"\n[Top-{i+1}] ç›¸ä¼¼åº¦: {similarity*100:.1f}%")
        print(f"   å—ID: {chunk_id} | é•¿åº¦: {length}å­—ç¬¦")
        
        # æ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
        preview = document[:100] + "..." if len(document) > 100 else document
        print(f"   å†…å®¹: {preview}")

# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®ç»Ÿè®¡åˆ†æ
# ============================================================

print("\n" + "=" * 60)
print("ã€ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®ç»Ÿè®¡åˆ†æã€‘")
print("-" * 60)

# è·å–æ‰€æœ‰æ•°æ®
all_data = collection.get(include=["metadatas"])

# ç»Ÿè®¡é•¿åº¦åˆ†å¸ƒ
lengths = [meta['length'] for meta in all_data['metadatas']]
avg_length = np.mean(lengths)
min_length = np.min(lengths)
max_length = np.max(lengths)

print(f"\nğŸ“Š æ–‡æœ¬å—é•¿åº¦ç»Ÿè®¡:")
print(f"   â€¢ å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
print(f"   â€¢ æœ€çŸ­: {min_length} å­—ç¬¦")
print(f"   â€¢ æœ€é•¿: {max_length} å­—ç¬¦")

# ç»Ÿè®¡ç« èŠ‚åˆ†å¸ƒ
chapters = [meta['chapter'] for meta in all_data['metadatas']]
chapter_counts = {}
for chapter in chapters:
    chapter_counts[chapter] = chapter_counts.get(chapter, 0) + 1

print(f"\nğŸ“– ç« èŠ‚åˆ†å¸ƒ:")
for chapter, count in sorted(chapter_counts.items()):
    bar = "â–ˆ" * (count // 2)
    print(f"   {chapter}: {count:2d} ä¸ª {bar}")

# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šå…ƒæ•°æ®è¿‡æ»¤æµ‹è¯•
# ============================================================

print("\nã€ç¬¬å…­éƒ¨åˆ†ï¼šå…ƒæ•°æ®è¿‡æ»¤æµ‹è¯•ã€‘")
print("-" * 60)

# åªæœç´¢ç‰¹å®šç« èŠ‚
question = "é©¾é©¶è¯æ‰£åˆ†æœ‰ä»€ä¹ˆè§„å®šï¼Ÿ"
print(f"\nâ“ é—®é¢˜: {question}")
print("ğŸ”§ è¿‡æ»¤: åªåœ¨ã€Œç¬¬ä¸‰ç« ã€ä¸­æœç´¢")

query_embedding = model.encode([question], show_progress_bar=False)

results_filtered = collection.query(
    query_embeddings=query_embedding.tolist(),
    n_results=2,
    where={"chapter": "ç¬¬ä¸‰ç« "},
    include=["documents", "metadatas", "distances"]
)

print(f"\nğŸ” è¿‡æ»¤åçš„ç»“æœ:")
for i in range(len(results_filtered['ids'][0])):
    document = results_filtered['documents'][0][i]
    distance = results_filtered['distances'][0][i]
    similarity = 1 - distance
    chapter = results_filtered['metadatas'][0][i]['chapter']
    
    print(f"\n[{i+1}] {chapter} | ç›¸ä¼¼åº¦: {similarity*100:.1f}%")
    preview = document[:80] + "..." if len(document) > 80 else document
    print(f"    {preview}")

# ============================================================
# æ€»ç»“
# ============================================================

print("\n" + "=" * 60)
print("ğŸ‰ äº¤é€šæ³•æ•°æ®å¯¼å…¥å®Œæˆï¼")
print("=" * 60)

print("\nâœ… å®Œæˆçš„å·¥ä½œ:")
print("   1. åŠ è½½é¢„å¤„ç†çš„å‘é‡æ•°æ®")
print("   2. åˆ›å»ºChromaDBæŒä¹…åŒ–æ•°æ®åº“")
print("   3. æ‰¹é‡å¯¼å…¥æ–‡æœ¬å’Œå‘é‡")
print("   4. æµ‹è¯•è¯­ä¹‰æ£€ç´¢æ•ˆæœ")
print("   5. æ•°æ®ç»Ÿè®¡åˆ†æ")
print("   6. å…ƒæ•°æ®è¿‡æ»¤æµ‹è¯•")

print(f"\nğŸ“ æ•°æ®åº“ä½ç½®:")
print(f"   {os.path.abspath(db_path)}")

print(f"\nğŸ“Š æ•°æ®åº“è§„æ¨¡:")
print(f"   â€¢ æ–‡æ¡£æ•°: {count}")
print(f"   â€¢ æ€»å­—ç¬¦æ•°: {sum(chunk['length'] for chunk in chunks)}")
print(f"   â€¢ å‘é‡ç»´åº¦: {vectors.shape[1]}")

print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
print("   1. å…¶ä»–è„šæœ¬å¯ä»¥ç›´æ¥è¿æ¥è¿™ä¸ªæ•°æ®åº“")
print("   2. è·¯å¾„: ./data/chroma_traffic_law")
print("   3. é›†åˆå: traffic_law")

print("\nğŸ¯ ä¸‹ä¸€æ­¥:")
print("   è¿è¡Œ: python 03_advanced_query.py")
print("   å­¦ä¹ é«˜çº§æ£€ç´¢æŠ€å·§")

print("\n" + "=" * 60)

