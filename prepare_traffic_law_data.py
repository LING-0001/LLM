"""
å‡†å¤‡äº¤é€šæ³•æ–‡æ¡£æ•°æ®
1. è¯»å–äº¤é€šæ³•æ–‡æ¡£
2. æ™ºèƒ½åˆ†å—
3. å‘é‡åŒ–
4. ä¿å­˜ç»“æœä¾›åç»­æ­¥éª¤ä½¿ç”¨
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import numpy as np
import json

print("="*70)
print(" "*15 + "äº¤é€šæ³•æ–‡æ¡£æ•°æ®å‡†å¤‡")
print("="*70)
print()

# 1. è¯»å–æ–‡æ¡£
print("æ­¥éª¤1ï¼šè¯»å–æ–‡æ¡£")
print("â”€"*70)

with open('traffic_law_document.md', 'r', encoding='utf-8') as f:
    document = f.read()

print(f"âœ… æ–‡æ¡£è¯»å–æˆåŠŸ")
print(f"   æ–‡ä»¶ï¼štraffic_law_document.md")
print(f"   é•¿åº¦ï¼š{len(document)} å­—ç¬¦")
print(f"   çº¦ï¼š{len(document)//500} ä¸ªæ®µè½")
print()

# 2. æ™ºèƒ½åˆ†å—
print("æ­¥éª¤2ï¼šæ™ºèƒ½åˆ†å—")
print("â”€"*70)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=60,
    separators=["\n## ", "\n### ", "\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""]
)

chunks = text_splitter.split_text(document)

print(f"âœ… åˆ†å—å®Œæˆ")
print(f"   å—æ•°ï¼š{len(chunks)} å—")
print(f"   å¹³å‡å¤§å°ï¼š{sum(len(c) for c in chunks) / len(chunks):.0f} å­—ç¬¦")
print()

print("å‰3ä¸ªå—é¢„è§ˆï¼š")
for i, chunk in enumerate(chunks[:3], 1):
    preview = chunk[:80].replace('\n', ' ')
    print(f"   å—{i}: {preview}...")
print()

# 3. å‘é‡åŒ–
print("æ­¥éª¤3ï¼šå‘é‡åŒ–ï¼ˆEmbeddingï¼‰")
print("â”€"*70)

print("æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹...")
model = SentenceTransformer('shibing624/text2vec-base-chinese')
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
print()

print("æ­£åœ¨æ‰¹é‡ç”Ÿæˆå‘é‡...")
vectors = model.encode(chunks, batch_size=32, show_progress_bar=True)
print(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆ")
print(f"   å‘é‡æ•°é‡ï¼š{len(vectors)}")
print(f"   å‘é‡ç»´åº¦ï¼š{vectors.shape[1]}")
print()

# 4. å‡†å¤‡å…ƒæ•°æ®
print("æ­¥éª¤4ï¼šå‡†å¤‡å…ƒæ•°æ®")
print("â”€"*70)

chunks_with_metadata = []
for i, chunk in enumerate(chunks):
    # ç¡®å®šæ¥æºç« èŠ‚
    if "ç¬¬ä¸€ç« " in chunk or "é€šè¡Œè§„åˆ™" in chunk:
        chapter = "ç¬¬ä¸€ç« ï¼šåŸºæœ¬é€šè¡Œè§„åˆ™"
    elif "ç¬¬äºŒç« " in chunk or "äº¤é€šä¿¡å·" in chunk:
        chapter = "ç¬¬äºŒç« ï¼šäº¤é€šä¿¡å·å’Œæ ‡å¿—"
    elif "ç¬¬ä¸‰ç« " in chunk or "é©¾é©¶è¯" in chunk:
        chapter = "ç¬¬ä¸‰ç« ï¼šæœºåŠ¨è½¦é©¾é©¶è¯ç®¡ç†"
    elif "ç¬¬å››ç« " in chunk or "äº¤é€šäº‹æ•…" in chunk:
        chapter = "ç¬¬å››ç« ï¼šäº¤é€šäº‹æ•…å¤„ç†"
    elif "ç¬¬äº”ç« " in chunk or "æ³•å¾‹è´£ä»»" in chunk:
        chapter = "ç¬¬äº”ç« ï¼šæ³•å¾‹è´£ä»»ä¸å¤„ç½š"
    else:
        chapter = "æœªåˆ†ç±»"
    
    chunks_with_metadata.append({
        "id": f"chunk_{i:03d}",
        "content": chunk,
        "chapter": chapter,
        "length": len(chunk),
        "index": i
    })

print(f"âœ… å…ƒæ•°æ®å‡†å¤‡å®Œæˆ")
print()

# 5. ä¿å­˜ç»“æœ
print("æ­¥éª¤5ï¼šä¿å­˜ç»“æœ")
print("â”€"*70)

# ä¿å­˜å‘é‡
vectors_file = "data/traffic_law_vectors.npy"
np.save(vectors_file, vectors)
print(f"âœ… å‘é‡å·²ä¿å­˜ï¼š{vectors_file}")

# ä¿å­˜å®Œæ•´æ•°æ®åŒ…
data_package = {
    "source_file": "traffic_law_document.md",
    "model_name": "shibing624/text2vec-base-chinese",
    "chunk_size": 400,
    "chunk_overlap": 60,
    "num_chunks": len(chunks),
    "vector_dim": int(vectors.shape[1]),
    "chunks": chunks_with_metadata
}

package_file = "data/traffic_law_data.json"
with open(package_file, 'w', encoding='utf-8') as f:
    json.dump(data_package, f, ensure_ascii=False, indent=2)
print(f"âœ… æ•°æ®åŒ…å·²ä¿å­˜ï¼š{package_file}")
print()

# 6. æ•°æ®ç»Ÿè®¡
print("æ­¥éª¤6ï¼šæ•°æ®ç»Ÿè®¡")
print("â”€"*70)

from collections import Counter
chapter_counts = Counter(item["chapter"] for item in chunks_with_metadata)

print("å„ç« èŠ‚å—æ•°åˆ†å¸ƒï¼š")
for chapter, count in sorted(chapter_counts.items()):
    bar = "â–ˆ" * (count * 2)
    print(f"   {chapter:25} {bar} ({count}å—)")
print()

# å—å¤§å°åˆ†å¸ƒ
sizes = [item["length"] for item in chunks_with_metadata]
print("å—å¤§å°ç»Ÿè®¡ï¼š")
print(f"   æœ€å°ï¼š{min(sizes)} å­—ç¬¦")
print(f"   æœ€å¤§ï¼š{max(sizes)} å­—ç¬¦")
print(f"   å¹³å‡ï¼š{np.mean(sizes):.0f} å­—ç¬¦")
print(f"   ä¸­ä½æ•°ï¼š{np.median(sizes):.0f} å­—ç¬¦")
print()

# å‘é‡å†…å­˜å ç”¨
vector_memory = vectors.nbytes / 1024 / 1024
print(f"å‘é‡å†…å­˜å ç”¨ï¼š{vector_memory:.2f} MB")
print()

# 7. ç®€å•æµ‹è¯•
print("æ­¥éª¤7ï¼šå¿«é€Ÿæµ‹è¯•æ£€ç´¢")
print("â”€"*70)

from sklearn.metrics.pairwise import cosine_similarity

test_query = "é…’é©¾æ€ä¹ˆå¤„ç½š"
print(f"æµ‹è¯•é—®é¢˜ï¼š{test_query}")
print()

query_vector = model.encode(test_query)
similarities = cosine_similarity([query_vector], vectors)[0]

top_3_indices = similarities.argsort()[-3:][::-1]

print("æœ€ç›¸å…³çš„3ä¸ªå—ï¼š")
for rank, idx in enumerate(top_3_indices, 1):
    chunk_info = chunks_with_metadata[idx]
    score = similarities[idx]
    preview = chunk_info["content"][:60].replace('\n', ' ')
    
    print(f"{rank}. [{score:.3f}] {chunk_info['chapter']}")
    print(f"   {preview}...")
    print()

print("="*70)
print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
print()
print("ğŸ“Š æ€»ç»“ï¼š")
print(f"   â€¢ åŸå§‹æ–‡æ¡£ï¼š1ç¯‡ (çº¦1600å­—)")
print(f"   â€¢ åˆ†å—æ•°é‡ï¼š{len(chunks)} å—")
print(f"   â€¢ å‘é‡ç»´åº¦ï¼š{vectors.shape[1]}")
print(f"   â€¢ æ•°æ®æ–‡ä»¶ï¼š")
print(f"     - {vectors_file}")
print(f"     - {package_file}")
print()
print("ğŸ“ è¿™äº›æ•°æ®å°†åœ¨Step 4å’ŒStep 5ä¸­ä½¿ç”¨")
print("="*70)

